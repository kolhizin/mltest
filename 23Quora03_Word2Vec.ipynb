{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import modutils\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "w2v_src_file = '../DataSets/Quora/w2v_src_180115.pickle'\n",
    "w2v_res_file = '../DataSets/Quora/w2v_res_180119.pickle'\n",
    "w2v_size = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(w2v_src_file, 'rb') as f:\n",
    "    (full_dict, full_sentences) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recode_max_dict(sentences, full_dict, dict_size):\n",
    "    last_ind = dict_size - 1\n",
    "    new_dict = full_dict[:last_ind]\n",
    "    new_num = sum([x[1] for x in full_dict[last_ind:]])\n",
    "    new_freq = sum([x[2] for x in full_dict[last_ind:]])\n",
    "    new_dict.append(('<UNK>', new_num, new_freq, 1))\n",
    "    \n",
    "    new_sentences = [[min(last_ind, z) for z in x] for x in sentences]\n",
    "    return (new_sentences, new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(w2v_src, w2v_dict) = recode_max_dict(full_sentences, full_dict, dict_size=w2v_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load state    \n",
    "mapper = {x[0]:i for (i,x) in enumerate(w2v_dict)}\n",
    "\n",
    "def word2idx(w):\n",
    "    if w in mapper:\n",
    "        return mapper[w]\n",
    "    else:\n",
    "        return mapper['<UNK>']\n",
    "    \n",
    "def idx2word(i):\n",
    "    if i >= len(w2v_dict):\n",
    "        return '<ERR>'\n",
    "    return w2v_dict[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_batch(data, ids):\n",
    "    tmp = np.array([[data[r[0]][r[1]], data[r[0]][r[2]]] for r in ids])\n",
    "    return tmp[:,0], tmp[:,1]\n",
    "\n",
    "def yield_batch(data, batch_size, p_word = 1, p_context = [(-1, 0.8), (1, 0.8)], num_batches=-1, verbose=True):\n",
    "    batch_id = 0\n",
    "    data_len = len(data)\n",
    "    while True:\n",
    "        batch_id += 1\n",
    "        if num_batches > 0:\n",
    "            if batch_id > num_batches:\n",
    "                print('Completed yielding batches {}\\t\\t'.format(num_batches))\n",
    "                break\n",
    "            if not verbose:\n",
    "                print('Yielding batch {} out of {}'.format(batch_id, num_batches), end='\\r')\n",
    "        ids = []\n",
    "        while len(ids) < batch_size:\n",
    "            id0 = np.random.randint(data_len)\n",
    "            if len(data[id0]) == 0:\n",
    "                continue\n",
    "            idi = np.random.randint(len(data[id0]))\n",
    "            idx = data[id0][idi]\n",
    "            if type(p_word) in (list, np.ndarray):\n",
    "                if np.random.uniform() > p_word[idx]:\n",
    "                    continue\n",
    "            for (rj, prob) in p_context:\n",
    "                j = idi + rj\n",
    "                if j < 0 or j >= len(data[id0]):\n",
    "                    continue\n",
    "                if np.random.uniform() > prob:\n",
    "                    continue\n",
    "                ids.append((id0, idi, j))\n",
    "        \n",
    "        yield form_batch(data, ids[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete.\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = len(w2v_dict)\n",
    "EMBED_SIZE = 200\n",
    "NCE_NUM_SAMPLED = 100\n",
    "\n",
    "init_embeding = np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE)/np.sqrt(EMBED_SIZE)\n",
    "init_beta = np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE)/np.sqrt(EMBED_SIZE)\n",
    "init_intercept = np.zeros((DICT_SIZE,))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    tf_in_word = tf.placeholder(tf.int32, shape=(None, ), name='in_word')\n",
    "    tf_in_context = tf.placeholder(tf.int32, shape=(None, 1), name='in_context')\n",
    "    tf_in_regularization = tf.placeholder_with_default(0.1, shape=(), name='in_regularization')\n",
    "    \n",
    "with tf.name_scope('Embedding'):\n",
    "    tf_embedding = tf.Variable(init_embeding, dtype=tf.float32)\n",
    "    tf_embedded_word = tf.nn.embedding_lookup(tf_embedding, tf_in_word, name='out_embedding')\n",
    "    \n",
    "with tf.name_scope('Training'):\n",
    "    tf_nce_beta = tf.Variable(init_beta, dtype=tf.float32)\n",
    "    tf_nce_intercept = tf.Variable(init_intercept, dtype=tf.float32)\n",
    "    tf_nce_loss = tf.reduce_mean(\n",
    "                    tf.nn.nce_loss(weights=tf_nce_beta, biases=tf_nce_intercept,\n",
    "                                   labels=tf_in_context, inputs=tf_embedded_word,\n",
    "                                   num_sampled=NCE_NUM_SAMPLED, num_classes=DICT_SIZE))\n",
    "    #tf_reg_loss = tf.sqrt(tf.reduce_mean(tf.square(tf_embedding))) #bad loss\n",
    "    tf_reg_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.reduce_mean(tf_embedding, axis=0)))) #center of embedding is 0\n",
    "    tf_full_loss = tf_nce_loss + tf_in_regularization * tf_reg_loss\n",
    "    tf_train = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(tf_full_loss)\n",
    "    \n",
    "with tf.name_scope('Validation'):\n",
    "    tf_valid_dictionary = tf.constant(np.array(range(DICT_SIZE)))\n",
    "    tf_valid_embedding = tf.nn.embedding_lookup(tf_embedding, tf_valid_dictionary)\n",
    "    tf_valid_in_norm = tf_embedded_word / tf.sqrt(tf.reduce_sum(tf.square(tf_embedded_word), 1, keep_dims=True))\n",
    "    tf_valid_dic_norm = tf_valid_embedding / tf.sqrt(tf.reduce_sum(tf.square(tf_valid_embedding), 1, keep_dims=True))\n",
    "    tf_valid_similarity = tf.matmul(tf_valid_in_norm, tf_valid_dic_norm, transpose_b=True)\n",
    "    \n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_W2V', tf.get_default_graph())\n",
    "tffw.close()\n",
    "print('Graph creation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed yielding batches 32\t\t\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set = [x for x in yield_batch(w2v_src, batch_size=32768, num_batches=32)]\n",
    "(valid_x, valid_y) = (np.hstack(x) for x in list(zip(*valid_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "simvalid_x = np.array([word2idx('two'), word2idx('this'), word2idx('are'), word2idx('bad'),\n",
    "                       word2idx('price'), word2idx('number'), word2idx('xbox'), word2idx('math'),\n",
    "                      word2idx('book'), word2idx('trump'), word2idx('college')])\n",
    "simvalid_dict = {tf_in_word: simvalid_x}\n",
    "valid_dict = {tf_in_word: valid_x, tf_in_context: valid_y.reshape(-1, 1)}\n",
    "\n",
    "hp_w2v_num0 = 50\n",
    "hp_w2v_alpha = -0.5\n",
    "\n",
    "p_w2v_wordnum = np.array([x[1] for x in w2v_dict])\n",
    "p_w2v_word = 1 #np.power(np.maximum(1, p_w2v_wordnum / hp_w2v_num0), hp_w2v_alpha) \n",
    "p_w2v_context = [(-1, 0.99), (1, 0.99)]\n",
    "#p_w2v_context = [(-2, 0.3), (-1, 0.8), (1, 0.8), (2, 0.5), (3, 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss=315.089 (0.001 reg-loss)\n",
      "['two', 'marking', 'teenagers', 'torrent', 'version', 'fc', 'maintain', 'luther', '\"black', 'srcc']\n",
      "['this', 'cool', 'crisis', 'documents', 'must', 'volt', 'origin', 'chris', 'humans', 'akhilesh']\n",
      "['are', 'gaining', 'brisbane', 'discovered', 'immediate', 'historically', 'avoid', 'end', 'crying', 'microscope']\n",
      "['bad', 'fourth', 'back', 'walls', 'bc', 'dies', 'playing', 'worn', 'intel', 'custom']\n",
      "['price', 'atheist', 'manipal', 'drugs', 'writing', 'k3', 'atrocities', 'beach', 'platform', 'us']\n",
      "['number', 'meanings', 'esteem', 'virus', 'vegetables', 'protest', 'stones', 'said', 'bullets', 'replaced']\n",
      "['xbox', 'wet', 'hour', 'someones', 'sem', 'lovers', 'celebrated', 'fort', 'requested', 'republicans']\n",
      "['math', 'smith', 'caught', \"(i'm\", 'adam', 'ontario', 'skip', 'raghuram', 'gases', 'converting']\n",
      "['book', 'satan', 'skills', 'survive', 'blank', 'annoying', 'polo', 'thriller', 'incorrect', 'itinerary']\n",
      "['trump', 'story', 'vertical', 'hbo', 'cognizant', 'panels', 'revision', 'couples', 'whom', 'accounting']\n",
      "['college', 'picture', 'paid', 'intermediate', 'pixel', 'thus', 'induction', 'thighs', 'suit', 'sister']\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 982.26 sec, loss=8.556 (0.289 reg-loss)\n",
      "['two', 'three', '30', 'players', 'absolute', 'core', 'both', 'articles', '4', 'classical']\n",
      "['this', 'isis', 'masturbation', 'god', 'america', 'jesus', 'smoking', 'water', 'power', 'everyone']\n",
      "['are', 'were', 'invented', 'do', 'cool', 'great', 'was', 'common', 'makes', 'basic']\n",
      "['bad', 'hard', 'strong', 'called', 'healthy', 'useful', 'fresher', 'normal', 'special', 'hot']\n",
      "['price', 'function', 'peace', 'advantage', 'range', 'properties', 'population', 'unit', 'speed', 'rest']\n",
      "['number', 'cell', 'law', 'future', 'name', 'results', 'university', '2015', 'applications', 'state']\n",
      "['xbox', 'pakistani', 'half', 'knowing', 'onto', 'hour', 'atomic', 'hr', 'broken', 'driving']\n",
      "['math', 'content', 'maths', 'hands', 'periods', 'coding', 'resource', 'php', 'particles', 'abroad']\n",
      "['book', 'smartphone', 'gaming', 'websites', 'site', 'resources', 'place', 'camera', 'strategy', 'treatment']\n",
      "['trump', \"trump's\", 'china', 'god', 'us', 'america', 'india', 'man', 'pakistan', 'president']\n",
      "['college', 'colleges', 'university', 'company', 'architecture', 'team', 'gaming', 'property', 'bike', 'building']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2V/model-00.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 1002.60 sec, loss=4.975 (0.513 reg-loss)\n",
      "['two', 'three', 'four', '200', '1.5', '3-4', 'several', 'southern', '23', 'meeting']\n",
      "['this', 'allen', 'spacex', 'david', 'humanity', 'diffusion', 'diversity', 'soil', 'bomb', 'spontaneous']\n",
      "['are', 'were', \"aren't\", 'myths', 'suggested', 'uses', 'notable', 'areas', 'documents', 'motivates']\n",
      "['bad', 'rare', 'reasonable', 'helpful', 'religious', 'rude', 'molecule', 'smaller', 'harmful', 'normal']\n",
      "['price', 'range', 'values', 'concept', 'understanding', 'salaries', 'beliefs', 'implementation', 'aim', 'shape']\n",
      "['number', '3g', 'numbers', 'battery', 'volte', 'location', 'camera', 'cell', 'lost', 'budget']\n",
      "['xbox', 'oneplus', 'nikon', 'punch', 'ps4', 'airtel', 'canon', 'cpu', 'fifa', 'minute']\n",
      "['math', 'maths', 'calculus', 'robotics', 'mathematics', 'drawing', 'coding', 'physics', 'mckinsey', 'weekend']\n",
      "['book', 'topic', 'sport', 'poem', 'subject', 'brand', 'meal', 'novel', 'novels', 'dj']\n",
      "['trump', \"trump's\", 'trump’s', 'china', 'bjp', 'india', 'woman', 'god', 'pakistan', 'russia']\n",
      "['college', 'university', 'colleges', 'phd', 'team', 'masters', 'branch', 'btech', 'professor', 'diploma']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2V/model-01.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 43802.50 sec, loss=4.508 (0.651 reg-loss)\n",
      "['two', 'four', 'three', '3-4', '2-3', '1.5', 'several', 'eight', '2.5', '28']\n",
      "['this', '2012', 'spacex', 'digestion', '2014', 'diffusion', 'soldier', 'genocide', 'reincarnation', 'walmart']\n",
      "['are', 'were', \"aren't\", \"weren't\", 'writes', 'characteristics', 'areas', 'notable', 'is/was', 'suggested']\n",
      "['bad', 'reasonable', 'special', 'frequent', 'terrible', 'horrible', 'toxic', 'crazy', 'inferior', 'normal']\n",
      "['price', 'range', 'understanding', 'mileage', 'salaries', 'values', 'implementation', 'expression', 'adaptations', 'valuation']\n",
      "['number', '3g', 'numbers', 'hotspot', 'volte', 'stolen', 'bomb', 'scams', 'tower', 'location']\n",
      "['xbox', 'nikon', 'ps4', 'oneplus', 'canon', 'honda', 'tick', 'plural', 'evening', 'cpa']\n",
      "['math', 'maths', 'mathematics', 'physics', 'coding', 'calculus', 'unity', 'geometry', 'astronomy', 'robotics']\n",
      "['book', 'novel', 'topic', 'poem', 'books', 'meal', 'poems', 'advertisement', 'novels', 'brand']\n",
      "['trump', 'trump’s', \"trump's\", 'bjp', 'hinduism', 'clinton', 'china', 'israel', 'elect', 'dylan']\n",
      "['college', 'university', 'phd', 'colleges', 'branch', 'diploma', 'pg', 'cs', 'btech', 'professor']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2V/model-02.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 995.28 sec, loss=4.116 (0.717 reg-loss)\n",
      "['two', 'three', 'four', '3-4', 'eight', 'several', '2-3', '45', '2.5', 'smaller']\n",
      "['this', '2012', 'darkness', 'digestion', 'joy', 'denmark', 'me\"', 'boom', 'louis', 'zomato']\n",
      "['are', 'were', \"aren't\", \"weren't\", 'do', 'is/was', 'characteristics', 'instances', 'is/are', 'ngos']\n",
      "['bad', 'weird', 'frequent', 'normal', 'reasonable', 'terrible', 'sociopath', 'special', 'superior', 'immoral']\n",
      "['price', 'understanding', 'mileage', 'salaries', 'lifespan', 'weights', 'directions', 'fate', 'implementation', 'depth']\n",
      "['number', 'numbers', 'scams', 'stolen', 'hotspot', '3g', 'otg', 'salaries', 'icon', 'tower']\n",
      "['xbox', 'nikon', 'ps4', 'oneplus', 'tick', 'cpa', 'hyundai', 'toyota', 'tor', 'alexa']\n",
      "['math', 'maths', 'quant', 'mathematics', 'coding', 'geometry', 'flute', 'time\"', 'javascript', 'niche']\n",
      "['book', 'books', 'novel', 'podcasts', 'topic', 'poem', 'advertisement', 'hobby', 'meal', 'poems']\n",
      "['trump', 'trump’s', \"trump's\", 'clinton', 'bjp', 'defeated', 'elect', 'israel', 'hinduism', 'congress']\n",
      "['college', 'colleges', 'phd', 'university', 'votes', 'diploma', 'branch', 'pg', 'cs', 'btech']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2V/model-03.ckpt\n",
      "Yielding batch 572 out of 10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ef8f25828a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                               batch_size=512, num_batches=10000, verbose=False):\n\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_in_word\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_context\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_valid_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimvalid_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "    [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "    print('Starting loss={:.3f} ({:.3f} reg-loss)'.format(nce_loss, reg_loss))\n",
    "    for q in range(len(sim)):\n",
    "        print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        for (train_x, train_y) in yield_batch(w2v_src, p_word=p_w2v_word, p_context=p_w2v_context,\n",
    "                                              batch_size=512, num_batches=10000, verbose=False):\n",
    "            train_dict = {tf_in_word: train_x, tf_in_context: train_y.reshape(-1, 1)}\n",
    "            tf_train.run(feed_dict=train_dict)\n",
    "\n",
    "        sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "        [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "        dic_embed = tf_valid_dic_norm.eval()\n",
    "        t1 = time.perf_counter()\n",
    "        print('Step complete in {0:.2f} sec, loss={1:.3f} ({2:.3f} reg-loss)'.format(t1-t0, nce_loss, reg_loss))\n",
    "        for q in range(len(sim)):\n",
    "            print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        p = tfsSaver.save(tfs, 'D:/Jupyter/Models/Models-23Quora03-W2VS1/model-{0:02d}.ckpt'.format(i))\n",
    "        print('Model saved at checkpoint: {0}'.format(p))\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "simvalid_x = np.array([word2idx('two'), word2idx('this'), word2idx('are'), word2idx('bad'),\n",
    "                       word2idx('price'), word2idx('number'), word2idx('xbox'), word2idx('math'),\n",
    "                      word2idx('book'), word2idx('trump'), word2idx('college')])\n",
    "simvalid_dict = {tf_in_word: simvalid_x}\n",
    "valid_dict = {tf_in_word: valid_x, tf_in_context: valid_y.reshape(-1, 1)}\n",
    "\n",
    "hp_w2v_num0 = 500\n",
    "hp_w2v_alpha = -0.75\n",
    "\n",
    "p_w2v_wordnum = np.array([x[1] for x in w2v_dict])\n",
    "p_w2v_word = np.power(np.maximum(1, p_w2v_wordnum / hp_w2v_num0), hp_w2v_alpha) \n",
    "#p_w2v_context = [(-1, 0.99), (1, 0.99)]\n",
    "p_w2v_context = [(-3, 0.2), (-2, 0.4), (-1, 0.6), (1, 0.7), (2, 0.5), (3, 0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/Jupyter/Models/Models-23Quora03-W2VS1/model-03.ckpt\n",
      "Starting loss=4.113 (0.717 reg-loss)\n",
      "['two', 'three', 'four', '3-4', 'eight', 'several', '2-3', '45', '2.5', 'smaller']\n",
      "['this', '2012', 'darkness', 'digestion', 'joy', 'denmark', 'me\"', 'boom', 'louis', 'zomato']\n",
      "['are', 'were', \"aren't\", \"weren't\", 'do', 'is/was', 'characteristics', 'instances', 'is/are', 'ngos']\n",
      "['bad', 'weird', 'frequent', 'normal', 'reasonable', 'terrible', 'sociopath', 'special', 'superior', 'immoral']\n",
      "['price', 'understanding', 'mileage', 'salaries', 'lifespan', 'weights', 'directions', 'fate', 'implementation', 'depth']\n",
      "['number', 'numbers', 'scams', 'stolen', 'hotspot', '3g', 'otg', 'salaries', 'icon', 'tower']\n",
      "['xbox', 'nikon', 'ps4', 'oneplus', 'tick', 'cpa', 'hyundai', 'toyota', 'tor', 'alexa']\n",
      "['math', 'maths', 'quant', 'mathematics', 'coding', 'geometry', 'flute', 'time\"', 'javascript', 'niche']\n",
      "['book', 'books', 'novel', 'podcasts', 'topic', 'poem', 'advertisement', 'hobby', 'meal', 'poems']\n",
      "['trump', 'trump’s', \"trump's\", 'clinton', 'bjp', 'defeated', 'elect', 'israel', 'hinduism', 'congress']\n",
      "['college', 'colleges', 'phd', 'university', 'votes', 'diploma', 'branch', 'pg', 'cs', 'btech']\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 2714.76 sec, loss=4.245 (0.770 reg-loss)\n",
      "['two', 'three', 'eight', 'four', 'several', '3-4', '2.5', 'five', 'smaller', '2-3']\n",
      "['this', 'up\"', 'insomnia', 'darkness', 'incorrect', 'hurricane', '2012', 'hammer', 'mill', 'correctly']\n",
      "['are', 'were', \"aren't\", 'some', 'characteristics', 'contemporary', 'elements', 'notable', 'errors', 'distinct']\n",
      "['bad', 'frequent', 'weird', 'terrible', 'crazy', 'guilt', 'joy', 'strange', 'harmful', 'nickname']\n",
      "['price', 'mileage', 'target', 'lifespan', 'diameter', 'valuation', 'weekly', 'dealership', 'depth', 'range']\n",
      "['number', 'numbers', 'registered', 'stolen', '(india)', 'cellphone', 'pf', 'info', \"wife's\", 'hotspot']\n",
      "['xbox', 'ps4', 'oneplus', 'hyundai', 'nikon', 'ebay', 'pirated', 'audi', 'playstation', 'icons']\n",
      "['math', 'maths', 'mathematics', 'quant', 'physics', 'geometry', 'coding', 'analytical', 'scala', 'calculus']\n",
      "['book', 'books', 'novel', 'poetry', 'introductory', 'poem', 'magazine', 'ide', 'documentary', 'forensic']\n",
      "['trump', 'trump’s', 'clinton', 'donald', 'hillary', 'elect', 'democrats', 'democrat', 'roger', 'congress']\n",
      "['college', 'votes', 'phd', 'school', 'university', 'colleges', 'profession', 'pg', 'bba', 'iits']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS2/model-00.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 694.64 sec, loss=4.049 (0.787 reg-loss)\n",
      "['two', 'three', 'eight', 'four', 'several', '2', 'five', 'nine', '2.5', '3-4']\n",
      "['this', 'up\"', 'creation', 'darkness', 'metrics', 'detail', '\"it', 'incorrect', '2012', 'insomnia']\n",
      "['are', 'were', 'some', 'notable', \"aren't\", 'distinct', 'five', 'the', 'characteristics', 'various']\n",
      "['bad', 'terrible', 'frequent', 'guilt', 'nickname', 'strange', 'crazy', 'joy', 'weird', 'rude']\n",
      "['price', 'target', 'valuation', 'shares', 'mileage', 'diameter', 'duration', 'approximate', 'weekly', 'outlet']\n",
      "['number', 'numbers', 'info', 'registered', '(india)', 'suspension', 'stolen', 'device', 'payroll', 'cellphone']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'oneplus', 'icons', 'pirated', 'audi', 'controller', 'ebay']\n",
      "['math', 'maths', 'physics', 'mathematics', 'quant', 'geometry', 'analytical', 'biology', 'coding', 'calculus']\n",
      "['book', 'books', 'novel', 'introductory', 'poem', 'poetry', 'newspaper', 'magazine', 'ide', 'biography']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'trump’s', 'elect', 'democrats', 'gop', 'nomination', 'roger']\n",
      "['college', 'school', 'iits', 'undergrad', 'colleges', 'phd', 'bba', 'votes', 'postgraduate', 'university']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS2/model-01.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 681.56 sec, loss=4.203 (0.793 reg-loss)\n",
      "['two', 'three', '2', 'four', 'eight', 'five', '5', 'several', '3', '2-3']\n",
      "['this', 'up\"', 'it\"', 'creation', 'cord', 'silence', '\"it', 'magical', 'insomnia', 'metrics']\n",
      "['are', 'some', 'were', \"aren't\", 'the', 'is', 'five', 'all', 'notable', 'distinct']\n",
      "['bad', 'terrible', 'frequent', 'weird', 'crazy', 'strange', 'harmful', 'regularly', 'nickname', 'joy']\n",
      "['price', 'valuation', 'prices', 'shares', 'purchase', 'rpm', 'target', 'promotion', 'wavelength', 'roof']\n",
      "['number', 'info', 'numbers', 'payroll', 'prepaid', 'suspension', 'stolen', 'trace', 'operator', 'billing']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'oneplus', 'pirated', 'icons', 'controller', 'yamaha', 'ebay']\n",
      "['math', 'maths', 'physics', 'mathematics', 'quant', 'biology', 'homework', 'geometry', 'calculus', 'coding']\n",
      "['book', 'books', 'novel', 'textbook', 'introductory', 'poetry', 'poem', 'magazine', 'newspaper', 'podcast']\n",
      "['trump', 'hillary', 'donald', 'clinton', 'trump’s', 'nomination', 'joe', 'gop', 'democrat', 'elect']\n",
      "['college', 'school', 'colleges', 'university', 'undergrad', 'iits', 'dtu', 'bba', 'postgraduate', 'bca']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS2/model-02.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 691.34 sec, loss=4.060 (0.797 reg-loss)\n",
      "['two', 'three', '2', 'four', 'eight', '3', '5', '4', 'five', 'only']\n",
      "['this', 'it\"', 'it', 'up\"', 'cord', 'magical', 'complexity', 'reflection', 'silence', 'detail']\n",
      "['are', 'were', 'some', 'is', 'the', \"aren't\", 'all', 'be', 'five', 'have']\n",
      "['bad', 'terrible', 'weird', 'good', 'frequent', 'harmful', 'strange', 'normal', 'worst', 'stupid']\n",
      "['price', 'shares', 'prices', 'target', 'purchasing', 'valuation', 'roof', 'promotion', 'cord', 'purchase']\n",
      "['number', 'numbers', 'info', 'payroll', 'trace', 'phone', 'consent', \"user's\", 'efforts', 'verify']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'pirated', 'oneplus', 'ps3', 'icons', 'yamaha', 'htc']\n",
      "['math', 'maths', 'physics', 'mathematics', 'biology', 'linguistics', 'homework', 'calculus', 'geometry', 'economics']\n",
      "['book', 'books', 'textbook', 'novel', 'magazine', 'poem', 'poetry', 'introductory', 'assignment', 'suited']\n",
      "['trump', 'hillary', 'donald', 'clinton', 'trump’s', 'gop', 'usa', 'nomination', 'joe', 'potus']\n",
      "['college', 'school', 'colleges', 'university', 'iits', 'graduate', 'bba', 'phd', 'btech', 'organisation']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS2/model-03.ckpt\n",
      "Yielding batch 56 out of 10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-7f623776044b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                               batch_size=512, num_batches=10000, verbose=False):\n\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_in_word\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_context\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_regularization\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_valid_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimvalid_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfsSaver.restore(tfs, 'D:/Jupyter/Models/Models-23Quora03-W2VS1/model-03.ckpt')\n",
    "    \n",
    "    sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "    [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "    print('Starting loss={:.3f} ({:.3f} reg-loss)'.format(nce_loss, reg_loss))\n",
    "    for q in range(len(sim)):\n",
    "        print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        for (train_x, train_y) in yield_batch(w2v_src, p_word=p_w2v_word, p_context=p_w2v_context,\n",
    "                                              batch_size=512, num_batches=10000, verbose=False):\n",
    "            train_dict = {tf_in_word: train_x, tf_in_context: train_y.reshape(-1, 1), tf_in_regularization: 0.01}\n",
    "            tf_train.run(feed_dict=train_dict)\n",
    "\n",
    "        sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "        [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "        dic_embed = tf_valid_dic_norm.eval()\n",
    "        t1 = time.perf_counter()\n",
    "        print('Step complete in {0:.2f} sec, loss={1:.3f} ({2:.3f} reg-loss)'.format(t1-t0, nce_loss, reg_loss))\n",
    "        for q in range(len(sim)):\n",
    "            print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        p = tfsSaver.save(tfs, 'D:/Jupyter/Models/Models-23Quora03-W2VS2/model-{0:02d}.ckpt'.format(i))\n",
    "        print('Model saved at checkpoint: {0}'.format(p))\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "simvalid_x = np.array([word2idx('two'), word2idx('this'), word2idx('are'), word2idx('bad'),\n",
    "                       word2idx('price'), word2idx('phone'), word2idx('xbox'), word2idx('math'),\n",
    "                      word2idx('book'), word2idx('trump'), word2idx('college'), word2idx('how'),\n",
    "                      word2idx('km'), word2idx('step'), word2idx('guide')])\n",
    "simvalid_dict = {tf_in_word: simvalid_x}\n",
    "valid_dict = {tf_in_word: valid_x, tf_in_context: valid_y.reshape(-1, 1)}\n",
    "\n",
    "hp_w2v_num0 = 500\n",
    "hp_w2v_alpha = -0.85\n",
    "\n",
    "p_w2v_wordnum = np.array([x[1] for x in w2v_dict])\n",
    "p_w2v_word = np.power(np.maximum(1, p_w2v_wordnum / hp_w2v_num0), hp_w2v_alpha) \n",
    "#p_w2v_context = [(-1, 0.99), (1, 0.99)]\n",
    "p_w2v_context = [(-4, 0.1), (-3, 0.3), (-2, 0.5), (-1, 0.7), (1, 0.8), (2, 0.6), (3, 0.4), (4, 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/Jupyter/Models/Models-23Quora03-W2VS3/model-00.ckpt\n",
      "Starting loss=4.247 (0.802 reg-loss)\n",
      "['two', '2', 'three', 'only', '3', '5', '4', 'four', '6', 'both']\n",
      "['this', 'it', '<UNK>', 'me', 'reflection', 'name', 'one', 'every', 'it\"', 'that']\n",
      "['are', 'were', 'some', 'is', 'the', '<UNK>', 'be', 'that', 'have', 'all']\n",
      "['bad', 'good', 'weird', 'normal', 'worst', 'terrible', 'like', 'work', 'harmful', '(on']\n",
      "['price', 'prices', 'shares', 'buying', 'purchasing', 'market', 'buy', 'target', 'tablet', 'purchase']\n",
      "['phone', 'phones', 'smartphone', 'device', 'cellphone', 'mobile', 'laptop', \"phone's\", 'tablet', 'iphone']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'oneplus', 'ps3', 'pirated', 'canon', 'otherwise', 'htc']\n",
      "['math', 'physics', 'maths', 'mathematics', 'biology', 'cs', 'economics', 'linguistics', 'calculus', 'coding']\n",
      "['book', 'books', 'textbook', 'novel', 'read', 'movie', 'course', 'poem', 'introductory', 'series']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'usa', 'he', \"trump's\", 'president', 'wins', 'trump’s']\n",
      "['college', 'school', 'university', 'colleges', 'graduate', 'phd', 'harvard', 'btech', 'cs', 'studying']\n",
      "['how', 'where', 'why', 'what', 'that', 'much', 'can', 'so', 'take', 'when']\n",
      "['km', 'mile', 'seconds', 'miles', 'crore', 'kgs', 'kg', 'minutes', 'lacs', 'continuously']\n",
      "['step', 'steps', 'usmle', 'move', 'switch', 'decade', 'sit', '\"it', 'upon', 'ccna']\n",
      "['guide', 'suggest', 'plz', 'tutorial', 'reference', 'references', 'documentation', 'certifications', 'listing', 'viewer']\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 1079.14 sec, loss=4.471 (0.805 reg-loss)\n",
      "['two', '2', 'three', '5', 'only', '3', '4', '6', 'all', 'both']\n",
      "['this', 'it', '<UNK>', 'every', 'not', 'that', 'me', 'reflection', 'my', 'barely']\n",
      "['are', 'were', 'some', 'is', 'the', '<UNK>', 'be', 'that', 'have', 'all']\n",
      "['bad', 'good', 'normal', 'like', '(on', 'not', 'weird', 'worst', 'trump', 'me']\n",
      "['price', 'prices', 'buying', 'buy', '80%', 'purchase', 'market', 'shares', '6', 'iphone']\n",
      "['phone', 'smartphone', 'phones', 'mobile', 'device', 'laptop', 'cellphone', \"phone's\", 'smartphones', 'tablet']\n",
      "['xbox', 'playstation', 'ps4', 'console', 'ps3', 'oneplus', 'pc', 'nikon', 'yamaha', 'canon']\n",
      "['math', 'physics', 'mathematics', 'maths', 'cs', 'biology', 'economics', 'calculus', 'mathematical', 'linguistics']\n",
      "['book', 'books', 'novel', 'movie', 'read', 'textbook', 'series', 'course', 'poem', 'institute']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'he', 'president', 'usa', 'us', 'she', \"trump's\"]\n",
      "['college', 'school', 'university', 'colleges', 'graduate', 'engineering', 'phd', 'cs', 'studying', 'mumbai']\n",
      "['how', 'where', 'why', 'what', 'that', 'can', 'who', 'when', 'use', 'really']\n",
      "['km', 'mile', 'miles', 'seconds', 'kgs', 'crore', 'lakh', 'kg', 'minutes', '80']\n",
      "['step', 'steps', 'move', 'procedures', 'ccna', 'usmle', 'sit', 'batch', '\"', 'switch']\n",
      "['guide', 'suggest', 'tutorial', 'reference', 'documentation', 'certifications', 'c#', 'viewer', 'plz', 'map']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS3/model-01.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 34393.31 sec, loss=4.332 (0.809 reg-loss)\n",
      "['two', '2', 'three', 'only', '3', '5', '4', 'all', 'one', '6']\n",
      "['this', '<UNK>', 'it', 'that', 'not', 'every', 'me', 'a', 'my', 'no']\n",
      "['are', 'some', 'were', 'is', 'the', 'be', '<UNK>', 'that', 'have', 'all']\n",
      "['bad', 'good', 'like', 'not', 'normal', 'worst', 'work', '<UNK>', 'better', 'me']\n",
      "['price', 'prices', 'buy', 'buying', '80%', 'shares', 'market', 'volume', 'pizza', 'go']\n",
      "['phone', 'smartphone', 'mobile', 'phones', 'device', 'laptop', 'iphone', 'cellphone', \"phone's\", 'account']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'ps3', 'oneplus', 'nikon', 'htc', 'pc', 'llc']\n",
      "['math', 'physics', 'maths', 'mathematics', 'biology', 'cs', 'economics', 'mathematical', 'chemistry', 'calculus']\n",
      "['book', 'books', 'read', 'movie', 'novel', 'series', 'textbook', 'course', 'poem', 'just']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'president', 'he', 'usa', 'she', 'us', \"trump's\"]\n",
      "['college', 'school', 'university', 'colleges', 'graduate', 'engineering', 'phd', 'mumbai', 'cs', 'make']\n",
      "['how', 'where', 'why', 'what', 'that', 'when', 'who', 'can', 'more', 'much']\n",
      "['km', 'mile', 'seconds', 'miles', 'minutes', 'lakh', 'continuously', 'crore', 'kgs', 'min']\n",
      "['step', 'steps', 'move', 'shut', 'sit', '\"', 'procedures', 'switch', 'ccna', 'decade']\n",
      "['guide', 'tutorial', 'suggest', 'documentation', 'certifications', 'reference', 'viewer', 'c#', 'phantom', '25']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS3/model-02.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 2602.89 sec, loss=4.104 (0.813 reg-loss)\n",
      "['two', '2', 'three', 'only', '3', '5', '4', 'all', 'other', '6']\n",
      "['this', '<UNK>', 'it', 'that', 'not', 'my', 'me', 'a', 'the', 'no']\n",
      "['are', 'were', 'some', 'is', 'the', 'of', '<UNK>', 'be', 'was', 'have']\n",
      "['bad', 'good', 'not', 'like', 'normal', 'worst', '<UNK>', 'work', 'used', 'better']\n",
      "['price', 'buy', 'prices', 'rate', 'buying', 'them', 'value', 'mumbai', 'pay', 'india']\n",
      "['phone', 'smartphone', 'phones', 'device', 'mobile', 'laptop', 'iphone', 'cellphone', 'account', 'number']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'oneplus', 'ps3', 'pc', 'nikon', 'htc', '360']\n",
      "['math', 'physics', 'maths', 'mathematics', 'cs', 'biology', 'economics', 'calculus', 'person', 'statistics']\n",
      "['book', 'books', 'read', 'movie', 'course', 'series', 'novel', 'just', 'study', 'learning']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'usa', 'president', 'he', 'us', 'she', \"trump's\"]\n",
      "['college', 'school', 'university', 'colleges', 'engineering', 'make', 'graduate', 'phd', 'course', 'studying']\n",
      "['how', 'where', 'why', 'what', 'that', 'who', 'when', 'more', 'can', 'if']\n",
      "['km', 'mile', 'seconds', 'lakh', 'miles', 'minutes', 'crore', 'kgs', 'continuously', 'jan']\n",
      "['step', 'steps', 'move', 'switch', '\"', 'sit', 'them', 'upon', 'procedures', 'break']\n",
      "['guide', 'suggest', 'viewer', 'certifications', 'documentation', '25', 'tutorial', 'phantom', 'discipline', 'wants']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS3/model-03.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 1296.61 sec, loss=4.096 (0.817 reg-loss)\n",
      "['two', '2', 'three', '5', '3', 'only', '4', 'all', 'other', 'use']\n",
      "['this', '<UNK>', 'it', 'not', 'that', 'my', 'me', 'a', 'the', 'only']\n",
      "['are', 'were', 'some', 'is', 'the', '<UNK>', 'of', 'be', 'have', 'that']\n",
      "['bad', 'good', 'not', 'like', 'work', 'normal', 'them', 'different', '<UNK>', 'one']\n",
      "['price', 'buy', 'buying', 'value', 'rate', 'india', 'prices', 'them', 'iphone', 'market']\n",
      "['phone', 'smartphone', 'mobile', 'phones', 'device', 'laptop', 'iphone', 'account', 'number', 'them']\n",
      "['xbox', 'ps4', 'playstation', 'console', 'ps3', 'oneplus', 'pc', 'htc', 'nikon', 'canon']\n",
      "['math', 'physics', 'maths', 'mathematics', 'cs', 'economics', 'biology', 'calculus', 'engineering', 'computer']\n",
      "['book', 'books', 'movie', 'read', 'course', 'series', 'study', 'just', 'learning', 'novel']\n",
      "['trump', 'donald', 'hillary', 'clinton', 'president', 'he', 'us', 'usa', 'she', \"trump's\"]\n",
      "['college', 'school', 'university', 'colleges', 'engineering', 'make', 'physics', 'course', 'studying', 'country']\n",
      "['how', 'where', 'why', 'that', 'what', 'when', 'really', 'who', 'so', 'if']\n",
      "['km', 'mile', 'seconds', 'minutes', 'lakh', 'miles', 'kgs', 'continuously', 'lakhs', 'kg']\n",
      "['step', 'steps', 'move', 'switch', 'them', 'taken', 'just', '\"', 'wealth', 'said']\n",
      "['guide', 'suggest', 'tutorial', 'certifications', '25', 'viewer', 'reference', 'let', 'charles', 'answer']\n",
      "Model saved at checkpoint: D:/Jupyter/Models/Models-23Quora03-W2VS3/model-04.ckpt\n",
      "Yielding batch 6 out of 10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-983b4ec2f72d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                                               batch_size=512, num_batches=10000, verbose=False):\n\u001b[0;32m     17\u001b[0m             \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_in_word\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_context\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_regularization\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_valid_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimvalid_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "i = 0\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfsSaver.restore(tfs, 'D:/Jupyter/Models/Models-23Quora03-W2VS3/model-{0:02d}.ckpt'.format(i))\n",
    "    \n",
    "    sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "    [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "    print('Starting loss={:.3f} ({:.3f} reg-loss)'.format(nce_loss, reg_loss))\n",
    "    for q in range(len(sim)):\n",
    "        print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        \n",
    "    while i < num_epochs:\n",
    "        t0 = time.perf_counter()\n",
    "        for (train_x, train_y) in yield_batch(w2v_src, p_word=p_w2v_word, p_context=p_w2v_context,\n",
    "                                              batch_size=512, num_batches=10000, verbose=False):\n",
    "            train_dict = {tf_in_word: train_x, tf_in_context: train_y.reshape(-1, 1), tf_in_regularization: 0.001}\n",
    "            tf_train.run(feed_dict=train_dict)\n",
    "\n",
    "        sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "        [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "        dic_embed = tf_valid_dic_norm.eval()\n",
    "        t1 = time.perf_counter()\n",
    "        print('Step complete in {0:.2f} sec, loss={1:.3f} ({2:.3f} reg-loss)'.format(t1-t0, nce_loss, reg_loss))\n",
    "        for q in range(len(sim)):\n",
    "            print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        \n",
    "        i += 1\n",
    "        p = tfsSaver.save(tfs, 'D:/Jupyter/Models/Models-23Quora03-W2VS3/model-{0:02d}.ckpt'.format(i))\n",
    "        print('Model saved at checkpoint: {0}'.format(p))\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(full_dict, full_sentences)\n",
    "full_w2v = dic_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = (full_dict, full_sentences, full_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(w2v_res_file, 'wb') as f:\n",
    "    pickle.dump(full_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
