{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import modutils\n",
    "import pickle\n",
    "import time\n",
    "import sklearn, sklearn.metrics\n",
    "\n",
    "src_file = '../DataSets/Quora/tfidf_src_180124.pickle'\n",
    "label_file = '../DataSets/Quora/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(src_file, 'rb') as f:\n",
    "    (src_data, src_vocab_size) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242506\n",
      "Wall time: 891 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_full = pd.read_csv(label_file)\n",
    "print(len(src_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_target = src_full.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_data, train_target), (valid_data, valid_target) = modutils.splitSample((src_data, src_target), pcts=[0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dense(values, size):\n",
    "    res = np.zeros(shape=(size,))\n",
    "    for x in values:\n",
    "        if x[0] < size:\n",
    "            res[x[0]] = x[1]\n",
    "    return res\n",
    "\n",
    "def make_features(src, vocab_size=100):\n",
    "    p1 = np.array([make_dense(x[0], size=vocab_size) for x in src])\n",
    "    p2 = np.array([make_dense(x[1], size=vocab_size) for x in src])\n",
    "    f1 = p1 * p2\n",
    "    f2 = np.square(p1-p2)\n",
    "    f3 = 0.5 * (p1 + p2)\n",
    "    f4 = np.array([x[2] for x in src])\n",
    "    return np.hstack([f1,f2,f3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_vocab_size = 1000 #override vocab size\n",
    "valid_set = make_features(valid_data, vocab_size=src_vocab_size)\n",
    "valid_trg = np.array(valid_target).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf_in_x = tf.placeholder(tf.float32, shape=(None, valid_set.shape[1]))\n",
    "tf_in_y = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "tf_hidden = tf.layers.dense(tf.layers.dropout(tf_in_x), 10, activation=tf.nn.elu)\n",
    "tf_logit = tf.layers.dense(tf_hidden, 2)\n",
    "tf_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_in_y, logits=tf_logit))\n",
    "tf_train = tf.train.AdamOptimizer(1e-2).minimize(tf_loss)\n",
    "\n",
    "tf_prob = tf.nn.softmax(tf_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.411 -> 0.355\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-00.ckpt\n",
      "Epoch 0: 0.694 -> 0.482 in 37.99 sec, gini=0.667, accur=0.757\n",
      "0.591 -> 0.522\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-01.ckpt\n",
      "Epoch 1: 0.482 -> 0.469 in 40.08 sec, gini=0.685, accur=0.764\n",
      "0.451 -> 0.387\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-02.ckpt\n",
      "Epoch 2: 0.469 -> 0.466 in 40.08 sec, gini=0.693, accur=0.766\n",
      "0.282 -> 0.244\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-03.ckpt\n",
      "Epoch 3: 0.466 -> 0.457 in 39.49 sec, gini=0.704, accur=0.772\n",
      "0.478 -> 0.395\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-04.ckpt\n",
      "Epoch 4: 0.457 -> 0.456 in 39.31 sec, gini=0.709, accur=0.774\n",
      "0.449 -> 0.369\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-05.ckpt\n",
      "Epoch 5: 0.456 -> 0.457 in 39.53 sec, gini=0.715, accur=0.772\n",
      "0.398 -> 0.328\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-06.ckpt\n",
      "Epoch 6: 0.457 -> 0.453 in 39.82 sec, gini=0.719, accur=0.779\n",
      "0.427 -> 0.329\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-07.ckpt\n",
      "Epoch 7: 0.453 -> 0.455 in 39.94 sec, gini=0.720, accur=0.780\n",
      "0.592 -> 0.445\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-08.ckpt\n",
      "Epoch 8: 0.455 -> 0.458 in 39.43 sec, gini=0.718, accur=0.780\n",
      "0.338 -> 0.276\n",
      "Model saved at checkpoint: ../Models/23Quora08TFIDF_v1/model-09.ckpt\n",
      "Epoch 9: 0.458 -> 0.466 in 36.86 sec, gini=0.719, accur=0.776\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_steps  = 2\n",
    "batch_size = 1024\n",
    "valid_dict = {tf_in_x: valid_set, tf_in_y: valid_trg}\n",
    "\n",
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    for n in range(num_epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        l0 = tf_loss.eval(feed_dict=valid_dict)\n",
    "        for bx, by in modutils.shuffleBatches((train_data, train_target), batchSize=batch_size):\n",
    "            train_x = make_features(bx, vocab_size=src_vocab_size)\n",
    "            train_y = np.array(by).reshape(-1)\n",
    "            train_dict = {tf_in_x: train_x, tf_in_y: train_y}\n",
    "            tl0 = tf_loss.eval(feed_dict=train_dict)\n",
    "            for i in range(num_steps):\n",
    "                tf_train.run(feed_dict=train_dict)\n",
    "            tl1 = tf_loss.eval(feed_dict=train_dict)\n",
    "            print('{0:.3f} -> {1:.3f}'.format(tl0, tl1), end='\\r')\n",
    "\n",
    "        valid_p = tf_prob.eval(feed_dict=valid_dict)\n",
    "        gini = sklearn.metrics.roc_auc_score(valid_trg, valid_p[:,1])*2-1\n",
    "        accur = sklearn.metrics.accuracy_score(valid_trg, 1*(valid_p[:,1]>0.5))\n",
    "        l1 = tf_loss.eval(feed_dict=valid_dict)\n",
    "        t1 = time.perf_counter()\n",
    "        p = tfsSaver.save(tfs, '../Models/23Quora08TFIDF_v1/model-{:02d}.ckpt'.format(n))\n",
    "        print('\\nModel saved at checkpoint: {0}'.format(p))        \n",
    "        print('Epoch {0}: {1:.3f} -> {2:.3f} in {3:.2f} sec, gini={4:.3f}, accur={5:.3f}'.format(n, l0, l1, t1-t0, gini, accur))\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. tfidf-1000 => 72.1% gini, 78.0% accuracy\n",
    "2. tfidf-300 => 66.4% gini, 75.3% accuracy\n",
    "3. tfidf-100 => 61.9% gini, 73.5% accuracy\n",
    "4. virtual-tfidf-5000 => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf-idf-1000 w word-set-features, no hidden\n",
    "    #=> gini on Epoch 4 is 71.2 (already overfitting), gini on Epoch 9 is 71.2\n",
    "#tf-idf-1000 w/o word-set-features, no hidden\n",
    "    #=> gini on Epoch 4 is 65.5 (already overfitting), gini on Epoch 9 is 65.7\n",
    "#tf-idf-1000 only word-set-features, no hidden\n",
    "    #=> gini on Epoch 4 is 48.9 (no overfitting), gini on Epoch 9 is 49.3\n",
    "\n",
    "#tf-idf-1000 only word-set-features, with hidden layer of 20\n",
    "    #=> gini on Epoch 4 is 50.1, gini on Epoch 9 is 50.2\n",
    "#tf-idf-1000 w/o word-set-features, with hidden layer of 20\n",
    "    #=> gini on Epoch 4 is 71.4 (already overfitting), gini on Epoch 9 is 72.2\n",
    "#tf-idf-1000 w word-set-features, with hidden layer of 20\n",
    "    #=> gini on Epoch 4 is 77.1 (already overfitting), gini on Epoch 9 is 77.2\n",
    "    \n",
    "#tf-idf-1000 full, with hidden layer of 40&do.5 and 10&do.5\n",
    "    #=> gini on Epoch 4 is 77.8? (already overfitting), gini on Epoch 9 is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/23Quora08TFIDF_v1/model-07.ckpt\n",
      "242688/242506   \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data_prob = None\n",
    "data_features = None\n",
    "with tf.Session() as tfs:\n",
    "    tfsSaver.restore(tfs, '../Models/23Quora08TFIDF_v1/model-{:02d}.ckpt'.format(7))\n",
    "    batch_size = 1024\n",
    "    cur_offset = 0\n",
    "    while cur_offset < len(src_data):\n",
    "        batch_dict = {tf_in_x: make_features(src_data[cur_offset:(cur_offset+batch_size)], vocab_size=src_vocab_size)}\n",
    "        cur_offset += batch_size\n",
    "        [tmp_p, tmp_features] = tfs.run([tf_prob, tf_hidden], feed_dict=batch_dict)\n",
    "        if data_prob is None:\n",
    "            data_prob = tmp_p[:, 1]\n",
    "            data_features = tmp_features\n",
    "        else:\n",
    "            data_prob = np.hstack([data_prob, tmp_p[:, 1]])\n",
    "            data_features = np.vstack([data_features, tmp_features])\n",
    "        print('{}/{}   '.format(cur_offset, len(src_data)), end='\\r')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wset_features = np.array([x[2] for x in src_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_full['tfidf_prob'] = data_prob\n",
    "for k in range(data_features.shape[1]):\n",
    "    src_full['tfidf_features_{:02d}'.format(k)] = data_features[:, k]\n",
    "for k in range(wset_features.shape[1]):\n",
    "    src_full['wset_features_{:02d}'.format(k)] = wset_features[:, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242506, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_full.to_csv('../DataSets/Quora/train_tfidf.csv',\n",
    "                 columns=['id', 'tfidf_prob']\n",
    "                + ['tfidf_features_{:02d}'.format(k) for k in range(data_features.shape[1])]\n",
    "                + ['wset_features_{:02d}'.format(k) for k in range(wset_features.shape[1])],\n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242506, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wset_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
