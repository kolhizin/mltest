{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genObs(length):\n",
    "    return np.random.choice(range(3), size=length)\n",
    "\n",
    "#def genTarget(x):\n",
    "#    s = 0\n",
    "#    ms = 0\n",
    "#    for z in x:\n",
    "#        if z==1:\n",
    "#            s+=1\n",
    "#        else:\n",
    "#            if s > ms:\n",
    "#                ms = s\n",
    "#            s = 0\n",
    "#    return ms\n",
    "\n",
    "#saw-like sequence\n",
    "def genTarget(x):\n",
    "    s = 0\n",
    "    ms = 0\n",
    "    for i in range(len(x)-2):\n",
    "        j = i + 1\n",
    "        if (x[j - 1] - x[j]) * (x[j + 1] - x[j]) > 0:\n",
    "            s += 1\n",
    "        else:\n",
    "            if s > ms:\n",
    "                ms = s\n",
    "            s = 0\n",
    "    return ms\n",
    "\n",
    "def genSample(num, length=20):\n",
    "    x = [genObs(length=length) for _ in range(num)]\n",
    "    y = [genTarget(t) for t in x]\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def randomBatch(tensorTuple, batchSize=64):\n",
    "    ids = np.random.choice(range(tensorTuple[0].shape[0]), batchSize)\n",
    "    return (x[ids,] for x in tensorTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 1],\n",
       "        [2, 1, 0, 0, 2, 1, 2, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1],\n",
       "        [1, 1, 2, 1, 2, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 2, 1],\n",
       "        [1, 1, 1, 2, 2, 0, 2, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 1, 0],\n",
       "        [2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 1, 0, 2, 2, 0, 2],\n",
       "        [1, 2, 1, 1, 1, 0, 0, 2, 1, 0, 1, 0, 2, 1, 1, 2, 1, 2, 2, 0],\n",
       "        [0, 2, 0, 0, 2, 1, 0, 1, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2],\n",
       "        [0, 1, 1, 2, 0, 0, 2, 2, 2, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2]]),\n",
       " array([6, 1, 7, 2, 2, 1, 3, 4, 4, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = genSample(10000)\n",
    "valid_x, valid_y = genSample(1000)\n",
    "valid_x[:10], valid_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 20\n",
    "RNN_SIZE = [10]\n",
    "\n",
    "InnerCell = lambda n: tf.nn.rnn_cell.GRUCell(num_units=n, activation=tf.nn.elu)\n",
    "#InnerCell = lambda n: tf.nn.rnn_cell.LSTMCell(num_units=n, activation=tf.nn.relu, use_peepholes=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tfi_x = tf.placeholder(shape=(None, SEQ_LEN), dtype=tf.float32)\n",
    "tfi_y = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "\n",
    "tfX = tf.reshape(tfi_x, shape=(tf.shape(tfi_x)[0], tf.shape(tfi_x)[1], 1))\n",
    "tfY = tf.reshape(tfi_y, shape=(tf.shape(tfi_y)[0], 1))\n",
    "\n",
    "\n",
    "rnnCell = tf.nn.rnn_cell.MultiRNNCell([InnerCell(s) for s in RNN_SIZE])\n",
    "#rnnCell = tf.nn.rnn_cell.GRUCell(RNN_SIZE[0])\n",
    "\n",
    "_, tfO = tf.nn.dynamic_rnn(rnnCell, inputs=tfX, dtype=tf.float32)\n",
    "\n",
    "tfOut = tf.layers.dense(tfO[-1], 1)\n",
    "\n",
    "tfLoss = tf.sqrt(tf.reduce_mean(tf.square(tfY - tfOut)))\n",
    "tfTrain = tf.train.AdamOptimizer(1e-3).minimize(tfLoss)\n",
    "\n",
    "#tfOutP = tf.nn.softmax(tfOut)\n",
    "tfOutR = tf.cast(tf.round(tfOut),dtype=tf.int64)\n",
    "\n",
    "tfAccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tfY, dtype=tf.int64), tf.cast(tf.round(tfOut),dtype=tf.int64)), dtype=tf.float32))\n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_A', tf.get_default_graph())\n",
    "print('Graph creation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (0.597 sec): loss changed from 3.52 to 2.88\t\tVL:2.906\t\tAC:0.116\n",
      "Epoch 10 (0.577 sec): loss changed from 1.7 to 1.69\t\tVL:1.645\t\tAC:0.231\n",
      "Epoch 20 (0.574 sec): loss changed from 1.18 to 1.15\t\tVL:1.186\t\tAC:0.347\n",
      "Epoch 30 (0.574 sec): loss changed from 1.17 to 1.14\t\tVL:1.122\t\tAC:0.394\n",
      "Epoch 40 (0.68 sec): loss changed from 0.978 to 0.951\t\tVL:0.986\t\tAC:0.427\n",
      "Epoch 50 (0.593 sec): loss changed from 0.839 to 0.791\t\tVL:0.887\t\tAC:0.470\n",
      "Epoch 60 (0.622 sec): loss changed from 0.774 to 0.748\t\tVL:0.807\t\tAC:0.523\n",
      "Epoch 70 (0.643 sec): loss changed from 0.703 to 0.687\t\tVL:0.758\t\tAC:0.556\n",
      "Epoch 80 (0.578 sec): loss changed from 0.751 to 0.73\t\tVL:0.726\t\tAC:0.611\n",
      "Epoch 90 (0.583 sec): loss changed from 0.667 to 0.65\t\tVL:0.701\t\tAC:0.628\n",
      "Epoch 100 (0.588 sec): loss changed from 0.621 to 0.597\t\tVL:0.688\t\tAC:0.652\n",
      "Epoch 110 (0.579 sec): loss changed from 0.647 to 0.63\t\tVL:0.678\t\tAC:0.668\n",
      "Epoch 120 (0.582 sec): loss changed from 0.607 to 0.593\t\tVL:0.668\t\tAC:0.690\n",
      "Epoch 130 (0.581 sec): loss changed from 0.673 to 0.656\t\tVL:0.673\t\tAC:0.689\n",
      "Epoch 140 (0.591 sec): loss changed from 0.564 to 0.546\t\tVL:0.650\t\tAC:0.719\n",
      "Epoch 150 (0.583 sec): loss changed from 0.553 to 0.538\t\tVL:0.641\t\tAC:0.719\n",
      "Epoch 160 (0.58 sec): loss changed from 0.573 to 0.552\t\tVL:0.640\t\tAC:0.715\n",
      "Epoch 170 (0.582 sec): loss changed from 0.683 to 0.635\t\tVL:0.627\t\tAC:0.722\n",
      "Epoch 180 (0.577 sec): loss changed from 0.54 to 0.523\t\tVL:0.615\t\tAC:0.767\n",
      "Epoch 190 (0.578 sec): loss changed from 0.553 to 0.527\t\tVL:0.605\t\tAC:0.769\n",
      "Epoch 200 (0.694 sec): loss changed from 0.562 to 0.53\t\tVL:0.576\t\tAC:0.765\n",
      "Epoch 210 (0.578 sec): loss changed from 0.55 to 0.515\t\tVL:0.588\t\tAC:0.781\n",
      "Epoch 220 (0.583 sec): loss changed from 0.555 to 0.535\t\tVL:0.548\t\tAC:0.806\n",
      "Epoch 230 (0.592 sec): loss changed from 0.497 to 0.481\t\tVL:0.539\t\tAC:0.807\n",
      "Epoch 240 (0.579 sec): loss changed from 0.461 to 0.445\t\tVL:0.502\t\tAC:0.811\n",
      "Epoch 250 (0.595 sec): loss changed from 0.503 to 0.471\t\tVL:0.500\t\tAC:0.820\n",
      "Epoch 260 (0.577 sec): loss changed from 0.477 to 0.446\t\tVL:0.479\t\tAC:0.830\n",
      "Epoch 270 (0.584 sec): loss changed from 0.493 to 0.457\t\tVL:0.486\t\tAC:0.838\n",
      "Epoch 280 (0.616 sec): loss changed from 0.431 to 0.404\t\tVL:0.467\t\tAC:0.840\n",
      "Epoch 290 (0.588 sec): loss changed from 0.455 to 0.437\t\tVL:0.468\t\tAC:0.851\n",
      "Epoch 300 (0.589 sec): loss changed from 0.459 to 0.415\t\tVL:0.451\t\tAC:0.846\n",
      "Epoch 310 (0.585 sec): loss changed from 0.49 to 0.44\t\tVL:0.444\t\tAC:0.852\n",
      "Epoch 320 (0.586 sec): loss changed from 0.401 to 0.369\t\tVL:0.477\t\tAC:0.853\n",
      "Epoch 330 (0.586 sec): loss changed from 0.386 to 0.366\t\tVL:0.434\t\tAC:0.859\n",
      "Epoch 340 (0.601 sec): loss changed from 0.377 to 0.354\t\tVL:0.467\t\tAC:0.879\n",
      "Epoch 350 (0.593 sec): loss changed from 0.369 to 0.35\t\tVL:0.423\t\tAC:0.875\n",
      "Epoch 360 (0.607 sec): loss changed from 0.438 to 0.405\t\tVL:0.410\t\tAC:0.876\n",
      "Epoch 370 (0.584 sec): loss changed from 0.389 to 0.366\t\tVL:0.401\t\tAC:0.886\n",
      "Epoch 380 (0.587 sec): loss changed from 0.352 to 0.335\t\tVL:0.389\t\tAC:0.887\n",
      "Epoch 390 (0.7 sec): loss changed from 0.368 to 0.353\t\tVL:0.377\t\tAC:0.892\n",
      "Epoch 400 (0.588 sec): loss changed from 0.336 to 0.306\t\tVL:0.384\t\tAC:0.896\n",
      "Epoch 410 (0.584 sec): loss changed from 0.346 to 0.306\t\tVL:0.367\t\tAC:0.906\n",
      "Epoch 420 (0.627 sec): loss changed from 0.387 to 0.342\t\tVL:0.371\t\tAC:0.906\n",
      "Epoch 430 (0.589 sec): loss changed from 0.368 to 0.309\t\tVL:0.348\t\tAC:0.915\n",
      "Epoch 440 (0.597 sec): loss changed from 0.317 to 0.302\t\tVL:0.339\t\tAC:0.920\n",
      "Epoch 450 (0.583 sec): loss changed from 0.3 to 0.281\t\tVL:0.329\t\tAC:0.930\n",
      "Epoch 460 (0.587 sec): loss changed from 0.307 to 0.291\t\tVL:0.326\t\tAC:0.929\n",
      "Epoch 470 (0.59 sec): loss changed from 0.326 to 0.27\t\tVL:0.320\t\tAC:0.944\n",
      "Epoch 480 (0.588 sec): loss changed from 0.276 to 0.256\t\tVL:0.298\t\tAC:0.939\n",
      "Epoch 490 (0.588 sec): loss changed from 0.301 to 0.273\t\tVL:0.289\t\tAC:0.938\n",
      "Epoch 499 (0.585 sec): loss changed from 0.3 to 0.267\t\tVL:0.298\t\tAC:0.942\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "num_steps  = 30\n",
    "num_epochs = 500\n",
    "\n",
    "fmtstr = 'Epoch {0} ({1:1.3} sec): loss changed from {2:1.3} to {3:1.3}\\t\\tVL:{4:1.3f}\\t\\tAC:{5:1.3f}'\n",
    "valid_batch = {tfi_x: valid_x, tfi_y: valid_y}\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        mini_x, mini_y = randomBatch((train_x, train_y), batchSize=batch_size)\n",
    "        train_batch = {tfi_x:mini_x, tfi_y:mini_y}\n",
    "        l0 = tfLoss.eval(feed_dict=train_batch)\n",
    "        t0 = time.perf_counter()\n",
    "        for j in range(num_steps):\n",
    "            tfTrain.run(feed_dict=train_batch)\n",
    "        t1 = time.perf_counter()\n",
    "        l1 = tfLoss.eval(feed_dict=train_batch)\n",
    "        lv = tfLoss.eval(feed_dict=valid_batch)\n",
    "        ac = tfAccuracy.eval(feed_dict=valid_batch)\n",
    "        if i%10 == 0 or i == num_epochs - 1:\n",
    "            print(fmtstr.format(i,t1-t0,l0,l1,lv,ac))\n",
    "    valid_r = tfs.run(tfOutR, feed_dict=valid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6, 1, 7, 2, 2, 1, 3, 4, 4, 2]),\n",
       " array([6, 1, 7, 2, 2, 1, 3, 4, 4, 2], dtype=int64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max saw sequence\n",
    "#basic-rnn, elu, 5     => 0.906 loss and 0.485 accuracy\n",
    "#basic-rnn, elu, 10    => 0.538 loss and 0.804 accuracy\n",
    "#basic-rnn, elu, 10x10 => 0.699 loss and 0.644 accuracy\n",
    "#gru-rnn, elu, 10      => 0.298 loss and 0.942 accuracy\n",
    "\n",
    "valid_y[:10], valid_r[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 1],\n",
       "       [2, 1, 0, 0, 2, 1, 2, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1],\n",
       "       [1, 1, 2, 1, 2, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 2, 1],\n",
       "       [1, 1, 1, 2, 2, 0, 2, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 1, 0],\n",
       "       [2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 1, 0, 2, 2, 0, 2],\n",
       "       [1, 2, 1, 1, 1, 0, 0, 2, 1, 0, 1, 0, 2, 1, 1, 2, 1, 2, 2, 0],\n",
       "       [0, 2, 0, 0, 2, 1, 0, 1, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2],\n",
       "       [0, 1, 1, 2, 0, 0, 2, 2, 2, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#max sequence length of 1\n",
    "#basic-rnn, elu, 5     => 0.34 loss and 0.90 accuracy\n",
    "#basic-rnn, elu, 10    => 0.36 loss and 0.87 accuracy\n",
    "#basic-rnn, elu, 10x10 => 0.168 loss and 0.982 accuracy \n",
    "#gru-rnn, elu, 10      => 0.056 loss and 0.998 accuracy\n",
    "\n",
    "valid_y[:10], valid_r[:10,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
