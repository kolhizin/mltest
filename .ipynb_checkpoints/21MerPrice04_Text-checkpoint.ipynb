{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time\n",
    "import tensorflow as tf\n",
    "import sklearn, sklearn.metrics, sklearn.preprocessing, sklearn.linear_model, sklearn.ensemble, sklearn.model_selection\n",
    "import nltk, nltk.stem\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import modutils\n",
    "\n",
    "data_dir = '../DataSets/MercariPrice/'\n",
    "src_file = data_dir + 'train_title.csv' \n",
    "stat_file = '21MerPrice04_TextStat.csv'\n",
    "w2vsrc_file = '21MerPrice04_W2VSrc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re1 = re.compile('[^a-z0-9\\[\\]&\\+]')\n",
    "re2 = re.compile('[\\[\\]]')\n",
    "re3 = re.compile('[0-9]+')\n",
    "re4 = re.compile('[&\\+]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src['item_text'] = src.item_description.astype(str).map(lambda x: re4.sub(' and ', re3.sub(' <num> ', re2.sub(' ', re1.sub(' ', x.lower()).replace('[rm]','<rm>')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "src['item_proc'] = src.item_text.astype(str).map(lambda x: [stemmer.stem(y.lower()) for y in x.split() if len(y) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_dict = collections.Counter([y for x in src.item_proc for y in x if len(y) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords = list(sorted(list(word_dict.items()), key=lambda x: x[1], reverse=True))\n",
    "len(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9804709952583157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x for (_,x) in allWords[:6800]]) / sum([x for (_,x) in allWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('wildflow', 56), 3.7488962513045823e-06)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 6800\n",
    "allWords[idx], allWords[idx][1] / sum([x for (_,x) in allWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary coverage properties:\n",
    "\n",
    "<table>\n",
    "<th>Dictionary size</th><th>Last word</th><th>Last-word frequency</th>\n",
    "<tr><td>50% / 100 words</td><td>'by'</td><td>23600 / 0.16%</td></tr>\n",
    "<tr><td>75% / 500 words</td><td>'decor'</td><td>4409 / 0.03%</td></tr>\n",
    "<tr><td>90% / 1700 words</td><td>'butteri'</td><td>843 / 5.7e-3%</td></tr>\n",
    "<tr><td>95% / 3200 words</td><td>'swaddl'</td><td>273 / 1.8e-3%</td></tr>\n",
    "<tr><td>97% / 5000 words</td><td>'gameplay'</td><td>109 / 7.3e-4%</td></tr>\n",
    "<tr><td>98% / 6800 words</td><td>'lap'</td><td>57 / 3.8e-4%</td></tr>\n",
    "<tr><td>99% / 11500 words</td><td>'grovia'</td><td>17 / 1.1e-4%</td></tr>\n",
    "<tr><td>99.5% / 20000 words</td><td>'goon'</td><td>5 / 3.4e-5%</td></tr>\n",
    "<tr><td>99.7% / 29000 words</td><td>'cinnamarol'</td><td>2 / 1.3e-5%</td></tr>\n",
    "<tr><td>99.9% / 53000 words</td><td>'volm'</td><td>1 / 6.7e-6%</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topNwords = list(sorted(list(word_dict.items()), key=lambda x: x[1], reverse=True))[:6800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "statNwords = [(z[0],z[1],tuple(src.fcst_diff_simple_title[src.item_proc.map(lambda x: z[0] in x)].agg(['mean', 'std'])))\n",
    "              for z in topNwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<num>', 848931, (0.051297657066670693, 0.58452474698813783)),\n",
       " ('box', 43846, (0.20456952926852379, 0.65494762688800934)),\n",
       " ('<rm>', 88744, (0.12990638636404458, 0.61274165886665655)),\n",
       " ('authent', 26868, (0.23631473579978479, 0.63025086254346996)),\n",
       " ('retail', 20313, (0.23935557697420937, 0.58869535329390232)),\n",
       " ('and', 408742, (0.05121744858037202, 0.58124049340977157)),\n",
       " ('with', 186851, (0.073057848124179856, 0.58620271856202033)),\n",
       " ('the', 260826, (0.057850948562996767, 0.5872065789159443)),\n",
       " ('come', 38352, (0.16481804952543094, 0.66297236020213213)),\n",
       " ('origin', 21377, (0.20382794468309259, 0.64026241892653202))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(statNwords, key=lambda x: abs(x[2][0]/x[2][1])*(x[1]**0.5), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(stat_file, 'wb') as f:\n",
    "    pickle.dump(statNwords, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapper = {x[0]:i for (i,x) in enumerate(statNwords)}\n",
    "\n",
    "def word2idx(w):\n",
    "    if w in mapper:\n",
    "        return mapper[w]+1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def idx2word(i):\n",
    "    if i == 0:\n",
    "        return '<unk>'\n",
    "    if i-1 >= len(statNwords):\n",
    "        return '<err>'\n",
    "    return statNwords[i-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_src = list(src.item_proc.map(lambda x: [word2idx(z) for z in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(w2vsrc_file, 'wb') as f:\n",
    "    pickle.dump(w2v_src, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load state\n",
    "\n",
    "with open(stat_file, 'rb') as f:\n",
    "    statNwords = pickle.load(f)\n",
    "\n",
    "with open(w2vsrc_file, 'rb') as f:\n",
    "    w2v_src = pickle.load(f)\n",
    "    \n",
    "mapper = {x[0]:i for (i,x) in enumerate(statNwords)}\n",
    "\n",
    "def word2idx(w):\n",
    "    if w in mapper:\n",
    "        return mapper[w]+1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def idx2word(i):\n",
    "    if i == 0:\n",
    "        return '<unk>'\n",
    "    if i-1 >= len(statNwords):\n",
    "        return '<err>'\n",
    "    return statNwords[i-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_batch(data, ids):\n",
    "    tmp = np.array([[data[r[0]][r[1]], data[r[0]][r[2]]] for r in ids])\n",
    "    return tmp[:,0], tmp[:,1]\n",
    "\n",
    "def yield_batch(data, batch_size, p_word = 1, p_context = [(-1, 0.8), (1, 0.8)], num_batches=-1, verbose=True):\n",
    "    batch_id = 0\n",
    "    data_len = len(data)\n",
    "    while True:\n",
    "        batch_id += 1\n",
    "        if num_batches > 0:\n",
    "            if batch_id > num_batches:\n",
    "                print('Completed yielding batches {}\\t\\t'.format(num_batches))\n",
    "                break\n",
    "            if not verbose:\n",
    "                print('Yielding batch {} out of {}'.format(batch_id, num_batches), end='\\r')\n",
    "        ids = []\n",
    "        while len(ids) < batch_size:\n",
    "            id0 = np.random.randint(data_len)\n",
    "            if len(data[id0]) == 0:\n",
    "                continue\n",
    "            idi = np.random.randint(len(data[id0]))\n",
    "            idx = data[id0][idi]\n",
    "            if type(p_word) in (list, np.ndarray):\n",
    "                if np.random.uniform() > p_word[idx]:\n",
    "                    continue\n",
    "            for (rj, prob) in p_context:\n",
    "                j = idi + rj\n",
    "                if j < 0 or j >= len(data[id0]):\n",
    "                    continue\n",
    "                if np.random.uniform() > prob:\n",
    "                    continue\n",
    "                ids.append((id0, idi, j))\n",
    "        \n",
    "        yield form_batch(data, ids[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete.\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = len(statNwords) + 1\n",
    "EMBED_SIZE = 25\n",
    "NCE_NUM_SAMPLED = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    tf_in_word = tf.placeholder(tf.int32, shape=(None, ), name='in_word')\n",
    "    tf_in_context = tf.placeholder(tf.int32, shape=(None, 1), name='in_context')\n",
    "    \n",
    "with tf.name_scope('Embedding'):\n",
    "    tf_embedding = tf.Variable(np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE), dtype=tf.float32)\n",
    "    tf_embedded_word = tf.nn.embedding_lookup(tf_embedding, tf_in_word, name='out_embedding')\n",
    "    \n",
    "with tf.name_scope('Training'):\n",
    "    tf_nce_beta = tf.Variable(np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE), dtype=tf.float32)\n",
    "    tf_nce_intercept = tf.Variable(np.random.normal(size=(DICT_SIZE,)), dtype=tf.float32)\n",
    "    tf_nce_loss = tf.reduce_mean(\n",
    "                    tf.nn.nce_loss(weights=tf_nce_beta, biases=tf_nce_intercept,\n",
    "                                   labels=tf_in_context, inputs=tf_embedded_word,\n",
    "                                   num_sampled=NCE_NUM_SAMPLED, num_classes=DICT_SIZE))\n",
    "    tf_reg_loss = tf.reduce_mean(tf.square(tf.sqrt(tf.reduce_sum(tf.square(tf_embedding), axis=1)) - 1))\n",
    "    tf_full_loss = tf_nce_loss# + tf_reg_loss\n",
    "    tf_train = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(tf_full_loss)\n",
    "    \n",
    "with tf.name_scope('Validation'):\n",
    "    tf_valid_dictionary = tf.constant(np.array(range(DICT_SIZE)))\n",
    "    tf_valid_embedding = tf.nn.embedding_lookup(tf_embedding, tf_valid_dictionary)\n",
    "    tf_valid_in_norm = tf_embedded_word / tf.sqrt(tf.reduce_sum(tf.square(tf_embedded_word), 1, keep_dims=True))\n",
    "    tf_valid_dic_norm = tf_valid_embedding / tf.sqrt(tf.reduce_sum(tf.square(tf_valid_embedding), 1, keep_dims=True))\n",
    "    tf_valid_similarity = tf.matmul(tf_valid_in_norm, tf_valid_dic_norm, transpose_b=True)\n",
    "    \n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_B', tf.get_default_graph())\n",
    "    \n",
    "print('Graph creation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed yielding batches 16\t\t\n",
      "Wall time: 7.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set = [x for x in yield_batch(w2v_src, batch_size=32768, num_batches=16)]\n",
    "(valid_x, valid_y) = (np.hstack(x) for x in list(zip(*valid_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "simvalid_x = np.array([word2idx('two'), word2idx('this'), word2idx('awesom'), word2idx('bad'),\n",
    "                       word2idx('price'), word2idx('ring'), word2idx('xbox'), word2idx('call'),\n",
    "                      word2idx('book'), word2idx('shirt'), word2idx('<num>'), word2idx('<rm>')])\n",
    "simvalid_dict = {tf_in_word: simvalid_x}\n",
    "valid_dict = {tf_in_word: valid_x, tf_in_context: valid_y.reshape(-1, 1)}\n",
    "\n",
    "hp_w2v_num0 = 50\n",
    "hp_w2v_alpha = -0.5\n",
    "\n",
    "p_w2v_wordnum = np.array([hp_w2v_num0] + [x[1] for x in statNwords])\n",
    "p_w2v_word = 1 #np.power(np.maximum(1, p_w2v_wordnum / hp_w2v_num0), hp_w2v_alpha) \n",
    "p_w2v_context = [(-2, 0.3), (-1, 0.8), (1, 0.8), (2, 0.5), (3, 0.2)]\n",
    "#p_w2v_context = [(-1, 0.2), (1, 0.8), (2, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss=215.411 (16.001 reg-loss)\n",
      "['two', 'jaw', 'offer', 'choker', 'worm', 'exact', 'th', 'ninja', 'nikki', 'coca']\n",
      "['this', 'earphon', 'steam', 'eastern', 'toofac', 'polyamid', 'exposur', 'exact', 'banner', 'theft']\n",
      "['awesom', 'majest', 'hippi', 'mud', 'dressi', 'licens', 'densiti', 'minim', 'odorless', 'snap']\n",
      "['bad', 'extend', 'dd', 'grape', 'turkey', 'druzi', 'blueberri', 'infal', 'er', 'fring']\n",
      "['price', 'ram', 'winchest', 'januari', 'bailey', 'advis', 'keurig', 'simpson', 'unlimit', 'rope']\n",
      "['ring', 'shore', 'kevin', 'scallop', 'ciat', 'tube', 'refil', 'os', 'tartelett', 'useabl']\n",
      "['xbox', 'research', 'sheep', 'earli', 'teapot', 'marker', 'minus', 'orlando', 'cactus', 'processor']\n",
      "['call', 'superman', 'plain', 'brooch', 'bitti', 'cord', 'apo', 'grillz', 'appl', 'rescu']\n",
      "['book', 'bauer', 'forc', 'vintag', 'eau', 'june', 'randi', 'freshen', 'creativ', 'cam']\n",
      "['shirt', 'foundat', 'marl', 'nivea', 'jessica', 'bronz', 'swag', 'scoobi', 'scoop', 'rosett']\n",
      "['<num>', 'egg', 'espresso', 'reviv', 'bamboo', 'asus', 'stainless', 'furnitur', 'scope', 'major']\n",
      "['<rm>', 'pleas', 'prestig', 'expir', 'walmart', 'thread', 'groot', 'varianc', 'dalmatian', 'critter']\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 119.06 sec, loss=18.030 (17.403 reg-loss)\n",
      "['two', 'this', 'grey', 'my', 'authent', 'dress', 'work', 'on', 'offer', 'is']\n",
      "['this', 'the', 'authent', 'and', '<unk>', 'for', 'all', '<rm>', 'is', 'pleas']\n",
      "['awesom', 'everyth', 'activ', 'os', 'minim', 'wash', 'origin', 'leg', 'so', 'short']\n",
      "['bad', 'dd', 'cardigan', 'sheet', 'line', 'lowest', 'strap', 'canva', 'need', 'kid']\n",
      "['price', 'for', '<unk>', 'and', 'in', 'the', 'with', 'black', '<num>', 'to']\n",
      "['ring', 'os', 'pictur', 'bag', 'more', 'light', 'one', 'navi', 'will', 'boot']\n",
      "['xbox', 'tote', 'pro', 'mm', 'ador', 'unus', 'also', 'swatch', 've', 'scratch']\n",
      "['call', 'cord', 'two', 'superman', 'appl', 'pictur', 'take', 'michael', 'short', 'gift']\n",
      "['book', 'more', 'your', 'pictur', 'from', 'lip', 'check', 'realli', 'bra', 'at']\n",
      "['shirt', 'small', 'is', 'item', 'will', 'price', 'flaw', 'free', 'fit', 'that']\n",
      "['<num>', 'for', '<unk>', 'in', 'and', 'to', 'size', 'black', 'brand', 'with']\n",
      "['<rm>', 'pleas', 'the', '<unk>', 'brand', 'and', 'great', 'size', 'but', 'like']\n",
      "Model saved at checkpoint: D:/Jupyter/Models-21MerPrice04-W2V/model-00.ckpt\n",
      "Completed yielding batches 10000\t\t\n",
      "Step complete in 123.71 sec, loss=6.295 (21.854 reg-loss)\n",
      "['two', 'front', 'this', 'the', 'it', 'back', 'blue', 'soft', 'that', 'color']\n",
      "['this', 'it', 'all', 'the', 'item', 'two', 'to', 'is', 'on', 'color']\n",
      "['awesom', 'activ', 'everyth', 'air', 're', 'heel', 'pictur', 'short', 'ring', 'take']\n",
      "['bad', 'dd', 'cardigan', 'fur', 'canva', 'lowest', 'closur', 'jacket', 'strap', 'time']\n",
      "['price', 'is', 'will', 'on', 'or', 'for', '<rm>', 'to', 'all', 'firm']\n",
      "['ring', 'purpl', 'crop', 'hair', 'leg', 'under', 'navi', 'can', 'air', 'button']\n",
      "['xbox', 'water', 'pro', 'berri', 'tote', 'cross', 'zipper', 've', 'mm', 'money']\n",
      "['call', 'cord', 'organ', 'winter', 'even', 'hand', 'take', 'infant', 'sever', 'appl']\n",
      "['book', 'card', 'vintag', 'leather', 'your', 'bra', 'from', 'onlin', 'appl', 'lip']\n",
      "['shirt', '<unk>', 'girl', 'black', 'authent', 'men', 'fit', 'short', 'for', 'one']\n",
      "['<num>', 'size', 'xl', '<unk>', 'shirt', 'authent', 'women', 'larg', 'men', 'small']\n",
      "['<rm>', 'item', 'is', 'will', 'and', 'if', 'includ', 'the', 'for', 'brown']\n",
      "Model saved at checkpoint: D:/Jupyter/Models-21MerPrice04-W2V/model-01.ckpt\n",
      "Yielding batch 817 out of 10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-918b3076877e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                               batch_size=512, num_batches=10000, verbose=False):\n\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_in_word\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_in_context\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_valid_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimvalid_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "    [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "    print('Starting loss={:.3f} ({:.3f} reg-loss)'.format(nce_loss, reg_loss))\n",
    "    for q in range(len(sim)):\n",
    "        print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        for (train_x, train_y) in yield_batch(w2v_src, p_word=p_w2v_word, p_context=p_w2v_context,\n",
    "                                              batch_size=512, num_batches=10000, verbose=False):\n",
    "            train_dict = {tf_in_word: train_x, tf_in_context: train_y.reshape(-1, 1)}\n",
    "            tf_train.run(feed_dict=train_dict)\n",
    "\n",
    "        sim = tf_valid_similarity.eval(feed_dict=simvalid_dict)\n",
    "        [nce_loss, reg_loss] = tfs.run([tf_nce_loss, tf_reg_loss], feed_dict=valid_dict)\n",
    "        dic_embed = tf_valid_dic_norm.eval()\n",
    "        t1 = time.perf_counter()\n",
    "        print('Step complete in {0:.2f} sec, loss={1:.3f} ({2:.3f} reg-loss)'.format(t1-t0, nce_loss, reg_loss))\n",
    "        for q in range(len(sim)):\n",
    "            print([idx2word(z) for z in list(reversed(sim[q,:].argsort()))[:10]])\n",
    "        p = tfsSaver.save(tfs, 'D:/Jupyter/Models-21MerPrice04-W2V/model-{0:02d}.ckpt'.format(i))\n",
    "        print('Model saved at checkpoint: {0}'.format(p))\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx('ring'), word2idx('shirt'), word2idx('book'), word2idx('xbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12082284, -0.11939144,  0.12321651,  0.12471975,  0.10879699,\n",
       "       -0.12246475,  0.12836537, -0.13117318, -0.13070413,  0.13072409,\n",
       "        0.11504891,  0.12889196, -0.12317298, -0.10389047,  0.12465525,\n",
       "       -0.12415253,  0.10928587,  0.12867165, -0.11030703,  0.12536304,\n",
       "       -0.12166201,  0.10459193,  0.13441706,  0.13206217, -0.13261577], dtype=float32)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dic_embed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15574615,  0.15616973,  0.15664187,  0.15925401,  0.1541833 ,\n",
       "        0.15634127,  0.16082509,  0.16339707,  0.16245647,  0.16218348,\n",
       "        0.15690722,  0.15919511,  0.15955788,  0.15215985,  0.15896773,\n",
       "        0.15876476,  0.152426  ,  0.15973669,  0.15225206,  0.16068704,\n",
       "        0.16040784,  0.15007742,  0.15788537,  0.1597278 ,  0.16191949], dtype=float32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(dic_embed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2173498 , -0.22903879,  0.22286132,  0.13756789,  0.1990371 ,\n",
       "        -0.16451138,  0.19596237, -0.18101329, -0.18045586,  0.16646686,\n",
       "         0.23006666,  0.17033021, -0.19361262, -0.1999778 ,  0.186573  ,\n",
       "        -0.17190142,  0.19666769,  0.21611533, -0.19643372,  0.22396116,\n",
       "        -0.15412666,  0.22039597,  0.23910256,  0.23738678, -0.22321671],\n",
       "       [ 0.23985499, -0.21388969,  0.25334546,  0.11352131,  0.2127897 ,\n",
       "        -0.19762003,  0.16952735, -0.19452524, -0.1572832 ,  0.17038959,\n",
       "         0.21420483,  0.12099274, -0.19096471, -0.19581595,  0.18808624,\n",
       "        -0.14558859,  0.17790687,  0.21009009, -0.22555663,  0.23373504,\n",
       "        -0.15956974,  0.23497406,  0.2331166 ,  0.2403014 , -0.22135891],\n",
       "       [ 0.2165428 , -0.21533097,  0.22044113,  0.1540786 ,  0.19411729,\n",
       "        -0.15430568,  0.18733428, -0.16363351, -0.18572168,  0.17800497,\n",
       "         0.23226233,  0.2057184 , -0.20317858, -0.19071822,  0.18615259,\n",
       "        -0.18618186,  0.21078692,  0.20230702, -0.20701844,  0.21471632,\n",
       "        -0.16185811,  0.2323007 ,  0.2207785 ,  0.22089976, -0.22273169],\n",
       "       [ 0.19922718, -0.21865115,  0.22365546,  0.16461687,  0.19254467,\n",
       "        -0.12192628,  0.2081421 , -0.17706646, -0.20135932,  0.18874767,\n",
       "         0.22769521,  0.21858844, -0.20419315, -0.1802664 ,  0.19548872,\n",
       "        -0.1819081 ,  0.19983825,  0.22472197, -0.17653921,  0.21193852,\n",
       "        -0.16456522,  0.21285836,  0.23212402,  0.22557402, -0.209273  ],\n",
       "       [ 0.23626758, -0.22145589,  0.23489803,  0.13866435,  0.20307194,\n",
       "        -0.14935793,  0.1816438 , -0.16519842, -0.19646204,  0.17080565,\n",
       "         0.23332183,  0.20205705, -0.18821174, -0.18993744,  0.19302124,\n",
       "        -0.17053077,  0.21939081,  0.18984158, -0.1916101 ,  0.21859619,\n",
       "        -0.14739522,  0.20917723,  0.25136057,  0.22714126, -0.21701758],\n",
       "       [ 0.23132108, -0.18587936,  0.22373253,  0.10566302,  0.18404911,\n",
       "        -0.11271669,  0.16576347, -0.1497521 , -0.1646881 ,  0.11735271,\n",
       "         0.26598111,  0.20391642, -0.22276998, -0.20969228,  0.20145841,\n",
       "        -0.17009024,  0.26037711,  0.22761916, -0.21575668,  0.21944845,\n",
       "        -0.15022825,  0.22820647,  0.2242592 ,  0.22840028, -0.2155924 ],\n",
       "       [ 0.29461417, -0.20998548,  0.21604942,  0.0720836 ,  0.25953346,\n",
       "        -0.02664856,  0.16516905, -0.1793084 , -0.21599571,  0.12626895,\n",
       "         0.30878028,  0.19121015, -0.23786069, -0.18084048,  0.15721373,\n",
       "        -0.17033112,  0.16235095,  0.21922813, -0.1874845 ,  0.20325842,\n",
       "        -0.15791473,  0.24641353,  0.20041674,  0.19026521, -0.19827543],\n",
       "       [ 0.25570977, -0.21206872,  0.25060362,  0.10094626,  0.1967669 ,\n",
       "        -0.17650437,  0.1896262 , -0.1542664 , -0.14841607,  0.14418522,\n",
       "         0.22352222,  0.11897515, -0.20698009, -0.19090831,  0.20096219,\n",
       "        -0.14767361,  0.18938667,  0.21420559, -0.2076757 ,  0.25173855,\n",
       "        -0.12848029,  0.2681345 ,  0.23493496,  0.24855299, -0.21207692],\n",
       "       [ 0.23080967, -0.2365094 ,  0.23301727,  0.14021431,  0.20290919,\n",
       "        -0.11686443,  0.19166458, -0.18526179, -0.19918486,  0.1785589 ,\n",
       "         0.22873148,  0.20728567, -0.20482504, -0.18102869,  0.19718067,\n",
       "        -0.18158892,  0.20773688,  0.21284528, -0.18120009,  0.21837255,\n",
       "        -0.1308686 ,  0.21006367,  0.24084571,  0.20154421, -0.22060311],\n",
       "       [ 0.21696189, -0.19225188,  0.2273535 ,  0.11537449,  0.20038237,\n",
       "        -0.12375861,  0.17932186, -0.15972675, -0.14823771,  0.14367299,\n",
       "         0.28013861,  0.21382518, -0.226088  , -0.19078892,  0.18121067,\n",
       "        -0.19088627,  0.21283805,  0.2270561 , -0.20304585,  0.2088636 ,\n",
       "        -0.17094618,  0.22029029,  0.22169691,  0.24477708, -0.213774  ]], dtype=float32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_embed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_var = src.copy()\n",
    "for (x,_,_) in tmpDict:\n",
    "    src_var['f_{0}'.format(x)] = 1*src_var.item_proc.map(lambda z: x in z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 5s\n",
      "Parser   : 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = src_var[['f_{}'.format(x) for (x,_,_) in tmpDict]].values\n",
    "Y = src_var.fcst_diff_simple_title.values\n",
    "(Xtrain, Ytrain), (Xvalid, Yvalid), (Xtest, Ytest) = modutils.splitSample((X,Y), [0.3,0.2,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#mgb0 = sklearn.ensemble.GradientBoostingRegressor(min_samples_leaf=100).fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlr0 = sklearn.linear_model.LinearRegression().fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsqr=0.1150 (train), 0.0945 (test)\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Ptrain = mlr0.predict(Xtrain)\n",
    "Ptest = mlr0.predict(Xtest)\n",
    "print('Rsqr={:.4f} (train), {:.4f} (test)'.format(sklearn.metrics.r2_score(Ytrain, Ptrain),\n",
    "                                                  sklearn.metrics.r2_score(Ytest, Ptest))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dists = [[np.sqrt(np.dot(dic_embed[i,:]-dic_embed[j,:], dic_embed[i,:]-dic_embed[j,:])) for j in range(len(dic_embed))]\n",
    "         for i in range(len(dic_embed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndists = np.array(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allids_i = np.array([[i for j in range(len(dic_embed))] for i in range(len(dic_embed))])\n",
    "allids_j = np.array([[j for j in range(len(dic_embed))] for i in range(len(dic_embed))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = (ndists>0.7)&(ndists>0)\n",
    "res = [(idx2word(i),idx2word(j),ndists[i,j], i, j) for (i,j) in zip(list(allids_i[mask]), list(allids_j[mask])) if i < j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = [statNwords[i] for i in np.random.randint(0, len(statNwords), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bike', 300, 0.28529497656828423),\n",
       " ('singl', 1636, 0.087022327248350337),\n",
       " ('abov', 2700, 0.061280791801226107),\n",
       " ('philip', 62, 0.86021066043918659),\n",
       " ('mouthwash', 76, 0.74594960520796938),\n",
       " ('dolc', 677, 0.16138640034844332),\n",
       " ('screwdriv', 109, 0.5795360474360336),\n",
       " ('aso', 506, 0.19786712395596842),\n",
       " ('bacteri', 78, 0.73250868985133333),\n",
       " ('team', 1083, 0.11615531406126813)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = -0.7\n",
    "num0 = 50\n",
    "[(x[0], x[1], np.power(max(num0,x[1]), alpha)/np.power(num0,alpha)) for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array([1,2])) in (list, np.ndarray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bike', 300, 1.7689360204744258),\n",
       " ('singl', 1636, 2.0959375092312422),\n",
       " ('abov', 2700, 2.2036183309053405),\n",
       " ('philip', 62, 1.5109119946911631),\n",
       " ('mouthwash', 76, 1.5419892968416637),\n",
       " ('dolc', 677, 1.9189288261197288),\n",
       " ('screwdriv', 109, 1.5986104580964025),\n",
       " ('aso', 506, 1.8638675687448392),\n",
       " ('bacteri', 78, 1.5459998956507686),\n",
       " ('team', 1083, 2.0112351276167502)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
