{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Client generation functions\n",
    "#0 - ok, 1 - 1 bucket, 2 - 2, 3 - 3, 4 - 4+, C - closed\n",
    "#rr[i,j] - p ith to jth, j=5 => Early Repayment\n",
    "\n",
    "def states2string(states):\n",
    "    mp = {0:'0',1:'1',2:'2',3:'3',4:'4',5:'C', 6:'L'}\n",
    "    if type(states) is list:\n",
    "        return ''.join([mp[x] for x in states])\n",
    "    return mp[states]\n",
    "\n",
    "def augmentSymbol(s, pm):\n",
    "    if s in ('C', 'N'):\n",
    "        return s\n",
    "    return np.random.choice(['-',s],p=[pm,1-pm])\n",
    "\n",
    "def augmentMissing(pp, pm):\n",
    "    return ''.join([augmentSymbol(x, pm) for x in pp])\n",
    "\n",
    "def string2states(s):\n",
    "    mp = {'0':0,'1':1,'2':2,'3':3,'4':4,'C':5, 'L':6}\n",
    "    return [mp[x] for x in s]\n",
    "\n",
    "def genPP(age, term, rr, s0, pMissing=0.05):\n",
    "    if age <= 0:\n",
    "        return \"\"\n",
    "    pp = [np.random.choice(range(6), p=s0)]\n",
    "    if age <= 1:\n",
    "        return states2string(pp)\n",
    "    for i in range(age-1):\n",
    "        prev = pp[-1]\n",
    "        nxt = 5\n",
    "        if prev < 5 and (i < term or prev > 0):\n",
    "            nxt = np.random.choice(range(6), p=rr[prev,:])\n",
    "            if i >= term and nxt==0:\n",
    "                nxt = 5\n",
    "        pp.append(nxt)\n",
    "    return augmentMissing(states2string(list(reversed(pp))), pm=pMissing)\n",
    "\n",
    "def genCreditRR(rr, lamAge=20, lamTerm=10, emu=np.log(1e5), esigma=3, pMissing=0.1):\n",
    "    s0 = np.zeros(6)\n",
    "    s0[0] = rr[0,0] / (rr[0,0] + rr[0,5])\n",
    "    s0[5] = rr[0,5] / (rr[0,0] + rr[0,5])\n",
    "    \n",
    "    age = np.random.poisson(lam=lamAge)\n",
    "    term = np.random.poisson(lam=lamTerm)\n",
    "    limit = np.ceil(np.exp(np.random.normal(loc=emu, scale=esigma)) / 1e3) * 1e3\n",
    "    pp = genPP(age, term, rr, s0, pMissing=pMissing)\n",
    "    return (limit, term, pp)    \n",
    "\n",
    "def genCreditSimple(pBad=0.1, pEarlyRepayment=0.1, lamAge=20, pMissing=0.1):\n",
    "    pGood = (1 - pBad) \n",
    "    r0 = [(pGood - pEarlyRepayment), pBad, 0, 0, 0, pEarlyRepayment]\n",
    "    r1x = np.array([pGood * 0.33 / 0.9, 0.33, pBad * 0.33 / 0.1, 0, 0, pEarlyRepayment * pGood * 0.2])\n",
    "    r1s = np.sum(r1x)\n",
    "    r1 = [x/r1s for x in r1x]\n",
    "    r2p = [[0.10, 0.20, 0.10, 0.60, 0.0, 0.0],\n",
    "      [0.05, 0.05, 0.05, 0.05, 0.8, 0.0],\n",
    "      [0.03, 0.03, 0.02, 0.02, 0.9, 0.0]]\n",
    "    rr = np.array([r0] + [r1] + r2p)\n",
    "    return genCreditRR(rr, lamAge=lamAge, pMissing=pMissing)\n",
    "\n",
    "def getClientTarget(data):\n",
    "    num0 = 0\n",
    "    num1 = 0\n",
    "    num2p = 0\n",
    "    for r in data:\n",
    "        num0 += r[2].count('0') + r[2].count('L') + min(1, r[2].count('C'))\n",
    "        num1 += r[2].count('1')\n",
    "        num2p += 2 * r[2].count('2') + 3 * r[2].count('3') + 4 * r[2].count('4')\n",
    "    pGood = 0.5\n",
    "    if num0 + num1 + num2p > 0:\n",
    "        pGood = num0 / (0.1 + num0 + num1 + num2p)\n",
    "    pBad = 1 - pGood\n",
    "    return (np.random.binomial(1, pBad), pBad)\n",
    "\n",
    "def genClient(muBad=0.1, sigmaBad=0.1, pEarlyRepayment=0.05, muAge=20, sigmaAge=5, pMissing=0.1):\n",
    "    numCredits = 1\n",
    "    data = [genCreditSimple(pBad=min(0.5,np.random.lognormal(mean=np.log(muBad), sigma=sigmaBad)),\n",
    "                            pEarlyRepayment=pEarlyRepayment, pMissing=pMissing,\n",
    "                           lamAge=np.random.lognormal(mean=np.log(muAge), sigma=np.log(sigmaAge)))\n",
    "            for i in range(numCredits)]\n",
    "    target, prob = getClientTarget(data)\n",
    "    return (data, target, prob)\n",
    "\n",
    "#Generate sample (as in RRs)\n",
    "def genSample(numObs=1000, genObs=genClient):\n",
    "    res = []\n",
    "    for i in range(numObs):\n",
    "        (obs, trgt, prob) = genObs()\n",
    "        row = [i, trgt, prob] + list(obs[0])\n",
    "        res.append(row)\n",
    "    return pd.DataFrame(np.array(res),\n",
    "                        columns=['accnt_id', 'trgt', 'prob', 'limit0', 'term0', 'pp0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnt_id</th>\n",
       "      <th>trgt</th>\n",
       "      <th>prob</th>\n",
       "      <th>limit0</th>\n",
       "      <th>term0</th>\n",
       "      <th>pp0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04761904761904767</td>\n",
       "      <td>978734000.0</td>\n",
       "      <td>10</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9402220324508966</td>\n",
       "      <td>29000.0</td>\n",
       "      <td>17</td>\n",
       "      <td>CCCCCC13444444444444433224324444444321100000-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953198127925117</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>C4444--44444-4444--32111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09090909090909094</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6208530805687205</td>\n",
       "      <td>178000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024390243902438935</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032258064516129115</td>\n",
       "      <td>541000.0</td>\n",
       "      <td>16</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007633587786259555</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>CCCCCCCCCCCCC000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833729216152019</td>\n",
       "      <td>4468000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>444-32212102132211000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032258064516129115</td>\n",
       "      <td>3420000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>CCCCC00-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  accnt_id trgt                  prob       limit0 term0  \\\n",
       "0        0    0   0.04761904761904767  978734000.0    10   \n",
       "1        1    1    0.9402220324508966      29000.0    17   \n",
       "2        2    1     0.953198127925117       2000.0     9   \n",
       "3        3    0   0.09090909090909094      14000.0     8   \n",
       "4        4    0    0.6208530805687205     178000.0    11   \n",
       "5        5    0  0.024390243902438935      66000.0    11   \n",
       "6        6    0  0.032258064516129115     541000.0    16   \n",
       "7        7    0  0.007633587786259555      30000.0    12   \n",
       "8        8    1     0.833729216152019    4468000.0    19   \n",
       "9        9    0  0.032258064516129115    3420000.0    12   \n",
       "\n",
       "                                                 pp0  \n",
       "0  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...  \n",
       "1     CCCCCC13444444444444433224324444444321100000-0  \n",
       "2                        C4444--44444-4444--32111100  \n",
       "3                                                 0-  \n",
       "4  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...  \n",
       "5                                               0000  \n",
       "6  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...  \n",
       "7                          CCCCCCCCCCCCC000000000000  \n",
       "8                           444-32212102132211000000  \n",
       "9                                           CCCCC00-  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = genSample(10000)\n",
    "valid_sample = genSample(2000)\n",
    "train_sample[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transformation functions\n",
    "def transformPP(term, pp):\n",
    "    if pp is None:\n",
    "        return pp\n",
    "    pplen = len(pp)\n",
    "    lst = max(0, pplen - term)\n",
    "    return pp[:lst] + pp[lst:].replace('C', 'L')\n",
    "\n",
    "def truncPP(term, pp, trlen=60):\n",
    "    if pp is None:\n",
    "        if term is None:\n",
    "            return 'X'*trlen\n",
    "        else:\n",
    "            return 'N'*trlen\n",
    "    pplen = len(pp)\n",
    "    if pplen >= trlen:\n",
    "        return pp[:trlen]\n",
    "    return pp + 'N'*(trlen - pplen)\n",
    "\n",
    "def transformDF(df, name='pp{0}t', trlen=60):\n",
    "    num = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    res = df.copy()\n",
    "    for i in range(num):\n",
    "        cols = ['pp{0}'.format(i), 'term{0}'.format(i)]\n",
    "        res[name.format(i)] = [truncPP(int(t), transformPP(int(t), p), trlen) for _,(p,t) in df[cols].iterrows()]\n",
    "    return res\n",
    "\n",
    "\n",
    "def transformToTensor(df, pp='pp{0}t', useX=False, numCredits=None):\n",
    "    #check dimensions\n",
    "    #num credits\n",
    "    num_credits = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    if numCredits is not None:\n",
    "        if num_credits < numCredits:\n",
    "            raise \"Provided <numCredits> is greater than number of fields in DF\"\n",
    "        num_credits = numCredits\n",
    "    num_mobs = None\n",
    "    for i in range(num_credits):\n",
    "        lens = list(set(len(x) for x in df[pp.format(i)] if x is not None))\n",
    "        numx = np.sum(['X' in x for x in df[pp.format(i)] if x is not None])\n",
    "        if numx > 0 and not useX:\n",
    "            raise \"Not supposed to use X, but X is found in observations!\"\n",
    "        if len(lens) != 1:\n",
    "            raise \"Expected same length in all observations!\"\n",
    "        if num_mobs is None:\n",
    "            num_mobs = lens[0]\n",
    "        if num_mobs != lens[0]:\n",
    "            raise \"Expected same length in all observations!\"\n",
    "    mapping = {'0':0,'1':1,'2':2,'3':3,'4':4,'-':5,'L':6,'C':7,'N':8}\n",
    "    if useX:\n",
    "        mapping['X'] = 9\n",
    "    \n",
    "    res = []\n",
    "    res_meta = []\n",
    "    res_trgt = []\n",
    "    for _, r in df.iterrows():\n",
    "        cred = []\n",
    "        cred_meta = []\n",
    "        res_trgt.append(r.trgt)\n",
    "        for i in range(num_credits):\n",
    "            cred.append([mapping[x] for x in reversed(r[pp.format(i)])])\n",
    "            cred_meta.append([-1 if r[f.format(i)] is None else r[f.format(i)] for f in ['limit{0}','term{0}']])\n",
    "        res.append(cred)\n",
    "        res_meta.append(cred_meta)\n",
    "    return np.array(res, dtype=np.int32), np.array(res_meta, dtype=np.float32), np.array(res_trgt, dtype=np.int32)\n",
    "\n",
    "def randomBatch(tensorTuple, batchSize=64):\n",
    "    ids = np.random.choice(range(tensorTuple[0].shape[0]), batchSize)\n",
    "    return (x[ids,] for x in tensorTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_logit(x, clampX=1e-3):\n",
    "    p0 = np.maximum(clampX, x)\n",
    "    p1 = np.maximum(clampX, 1-x)\n",
    "    return np.log(p0 / p1)\n",
    "    \n",
    "def extractFeatures(df, pp='pp{0}'):\n",
    "    num_credits = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    cnt_symbols = ['0', '1', '2', '3', '4', '5', 'L', 'C', '00', '01', '0L', '0C', '10', '11', '12', '1L', '1C']\n",
    "    \n",
    "    rr_names = ['f_rr{0}_01{1}', 'f_rr{0}_00{1}', 'f_rr{0}_10{1}', 'f_rr{0}_11{1}', 'f_rr{0}_12{1}']\n",
    "    rr_types = ['', 'f0', 'f1', 'lgt']\n",
    "    rr_final = [x.format(i) for x in ['f_rr{0}_0d', 'f_rr{0}_1d'] for i in range(num_credits)] + [x.format(i, y) for x in rr_names for y in rr_types for i in range(num_credits)]\n",
    "    \n",
    "    new_features = ['f_num{0}_{1}'.format(i, x) for i in range(num_credits) for x in cnt_symbols] + rr_final\n",
    "    res = pd.concat([df, pd.DataFrame(columns=new_features)])\n",
    "    \n",
    "    for _, r in res.iterrows():\n",
    "        for i in range(num_credits):\n",
    "            paypat = r[pp.format(i)]\n",
    "            cnts = {x:paypat.count(x) for x in cnt_symbols}\n",
    "            for x,v in cnts.items():\n",
    "                r['f_num{0}_{1}'.format(i, x)] = v\n",
    "            r0 = cnts['00'] + cnts['01'] + cnts['0L'] + cnts['0C']\n",
    "            r1 = cnts['10'] + cnts['11'] + cnts['12'] + cnts['1L'] + cnts['1C']\n",
    "            r['f_rr{0}_0d'.format(i)] = r0\n",
    "            r['f_rr{0}_1d'.format(i)] = r0\n",
    "            r['f_rr{0}_01'.format(i)] = (cnts['01'] / r0 if r0 > 0 else 0) \n",
    "            r['f_rr{0}_00'.format(i)] = ((cnts['00'] + cnts['0L'] + cnts['0C']) / r0 if r0 > 0 else 1)\n",
    "            r['f_rr{0}_12'.format(i)] = (cnts['12'] / r1 if r1 > 0 else 0)\n",
    "            r['f_rr{0}_11'.format(i)] = (cnts['11'] / r1 if r1 > 0 else 0)\n",
    "            r['f_rr{0}_10'.format(i)] = ((cnts['10'] + cnts['1L'] + cnts['1C']) / r1 if r1 > 0 else 0)\n",
    "            for f in ['f_rr{0}_01{1}', 'f_rr{0}_00{1}', 'f_rr{0}_10{1}', 'f_rr{0}_11{1}', 'f_rr{0}_12{1}']:\n",
    "                r[f.format(i, 'f0')] = (1 if r[f.format(i,'')]==0 else 0)\n",
    "                r[f.format(i, 'f1')] = (1 if r[f.format(i,'')]==1 else 0)\n",
    "                r[f.format(i, 'lgt')] = safe_logit(r[f.format(i,'')])   \n",
    "    return res\n",
    "\n",
    "def featuresToTensor(df):\n",
    "    features = [x for x in df.columns if x.find('f_') == 0]\n",
    "    return np.array(df[features]), np.array(df.trgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wf = extractFeatures(train_sample)\n",
    "valid_wf = extractFeatures(valid_sample)\n",
    "\n",
    "train_sample = transformDF(train_sample)\n",
    "valid_sample = transformDF(valid_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = featuresToTensor(train_wf)\n",
    "valid_x, valid_y = featuresToTensor(valid_wf)\n",
    "\n",
    "logreg0 = LogisticRegression().fit(train_x, train_y)\n",
    "train_p = logreg0.predict_proba(train_x)[:,1]\n",
    "valid_p = logreg0.predict_proba(valid_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47899599,  0.08283167,  0.1556964 ])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete\n"
     ]
    }
   ],
   "source": [
    "param_RNN_size = 30\n",
    "param_OUT_H1   = 30\n",
    "param_LR       = 1e-3\n",
    "\n",
    "size_pp_dictionary =  9\n",
    "size_meta_vars = 2\n",
    "\n",
    "size_mob_vars = size_pp_dictionary + size_meta_vars\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tfIn_Train = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "tfIn_Trgt = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "tfIn_PP = tf.placeholder(shape=(None, 1, None), dtype=tf.int32)\n",
    "\n",
    "with tf.name_scope(name='DATA-TRANSFORMATION'):\n",
    "    tfY  = tf.one_hot(tfIn_Trgt, 2)\n",
    "    tfX  = tf.reduce_sum(tf.one_hot(tfIn_PP, size_pp_dictionary), axis=1)\n",
    "    tfXn = tfX[:,1:,:]\n",
    "\n",
    "#3 define RNN on these inputs\n",
    "with tf.name_scope(name='RNN'):\n",
    "    #rnnCell = tf.nn.rnn_cell.GRUCell(num_units=param_RNN_size, activation=tf.nn.tanh)\n",
    "    rnnCell = tf.nn.rnn_cell.LSTMCell(num_units=param_RNN_size)\n",
    "    #rnnCell = tf.nn.rnn_cell.BasicRNNCell(num_units=param_RNN_size, activation=tf.nn.relu)\n",
    "    #rnnCell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(num_units=param_RNN_size, activation=tf.nn.relu) for _ in range(3)])\n",
    "    _, tfMO = tf.nn.dynamic_rnn(rnnCell, inputs=tfX, dtype=tf.float32)\n",
    "\n",
    "#4 define output layer\n",
    "with tf.name_scope(name='OUTPUT-FFNN'):\n",
    "    tfOH1 = tf.layers.dense(tf.layers.dropout(tfMO, training=tfIn_Train), param_OUT_H1, activation=tf.nn.relu, name='OUT-H1')\n",
    "    tfOO = tf.layers.dense(tf.layers.dropout(tfOH1, training=tfIn_Train), 2, name='OUT-OUT')\n",
    "    tft = tf.reduce_mean(tfOO)\n",
    "\n",
    "#5a define loss functions\n",
    "with tf.name_scope(name='LOSS-OPTIMIZER'):\n",
    "    tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfY,logits=tfOO))\n",
    "    tfTrain = tf.train.AdamOptimizer(param_LR).minimize(tfLoss)\n",
    "\n",
    "tfLossSummary = tf.summary.scalar('Cross-Entropy-Loss', tfLoss)\n",
    "#5b prediction\n",
    "tfOutProb = tf.nn.softmax(tfOO)[:,1]\n",
    "\n",
    "print('Graph creation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pp, train_meta, train_trgt = transformToTensor(train_sample, useX=False, numCredits=1)\n",
    "valid_pp, valid_meta, valid_trgt = transformToTensor(valid_sample, useX=False, numCredits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[4000,2] labels_size=[2000,2]\n\t [[Node: LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](LOSS-OPTIMIZER/Reshape, LOSS-OPTIMIZER/Reshape_1)]]\n\nCaused by op 'LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-150-2701e23cf335>\", line 37, in <module>\n    tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfY,logits=tfOO))\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1594, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2380, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[4000,2] labels_size=[2000,2]\n\t [[Node: LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](LOSS-OPTIMIZER/Reshape, LOSS-OPTIMIZER/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[4000,2] labels_size=[2000,2]\n\t [[Node: LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](LOSS-OPTIMIZER/Reshape, LOSS-OPTIMIZER/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-4790e685d1bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtime0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtfTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3926\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3927\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3928\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[4000,2] labels_size=[2000,2]\n\t [[Node: LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](LOSS-OPTIMIZER/Reshape, LOSS-OPTIMIZER/Reshape_1)]]\n\nCaused by op 'LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-150-2701e23cf335>\", line 37, in <module>\n    tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfY,logits=tfOO))\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1594, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2380, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[4000,2] labels_size=[2000,2]\n\t [[Node: LOSS-OPTIMIZER/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](LOSS-OPTIMIZER/Reshape, LOSS-OPTIMIZER/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "num_step  = 20\n",
    "batch_size = 2000\n",
    "\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_BurPat_SRNN-{0}'.format(dt_now), tf.get_default_graph())\n",
    "train_batch = {tfIn_PP: train_pp, tfIn_Trgt: train_trgt, tfIn_Train:True}\n",
    "valid_batch = {tfIn_PP: valid_pp, tfIn_Trgt: valid_trgt, tfIn_Train:False}\n",
    "with tf.Session() as tfs:    \n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epoch):\n",
    "        mini_pp, mini_meta, mini_trgt = randomBatch((train_pp, train_meta, train_trgt), batch_size)\n",
    "        mini_batch = {tfIn_PP: mini_pp, tfIn_Trgt: mini_trgt, tfIn_Train:True}\n",
    "        \n",
    "        time0 = time.perf_counter()\n",
    "        loss0 = tfLoss.eval(feed_dict=mini_batch)\n",
    "        for j in range(num_step):\n",
    "            tfTrain.run(feed_dict=mini_batch)\n",
    "        loss1 = tfLoss.eval(feed_dict=mini_batch)\n",
    "        time1 = time.perf_counter()\n",
    "        \n",
    "        valid_loss_str = tfLossSummary.eval(feed_dict=valid_batch)\n",
    "        tffw.add_summary(valid_loss_str, i)\n",
    "        print('Epoch {0} ({3:1.2} sec): loss changed from {1:1.3} to {2:1.3}'.format(i, loss0, loss1, time1-time0))\n",
    "    train_prob = tfOutProb.eval(feed_dict=train_batch)\n",
    "    valid_prob = tfOutProb.eval(feed_dict=valid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vsmpl = valid_sample.copy()\n",
    "vsmpl['nnp'] = valid_prob\n",
    "vsmpl['lrp'] = valid_p\n",
    "vsmpl = vsmpl[['accnt_id','trgt','prob','nnp','lrp','pp0','pp0t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8407087210992128"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.prob, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74813644419875613"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.nnp, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087475993184059"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.lrp, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {a:a+'-' for a in 'xyz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 39)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
