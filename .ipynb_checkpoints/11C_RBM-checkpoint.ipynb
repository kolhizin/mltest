{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Client generation functions\n",
    "#0 - ok, 1 - 1 bucket, 2 - 2, 3 - 3, 4 - 4+, C - closed\n",
    "#rr[i,j] - p ith to jth, j=5 => Early Repayment\n",
    "\n",
    "def states2string(states):\n",
    "    mp = {0:'0',1:'1',2:'2',3:'3',4:'4',5:'C', 6:'L'}\n",
    "    if type(states) is list:\n",
    "        return ''.join([mp[x] for x in states])\n",
    "    return mp[states]\n",
    "\n",
    "def augmentSymbol(s, pm):\n",
    "    if s in ('C', 'N'):\n",
    "        return s\n",
    "    return np.random.choice(['-',s],p=[pm,1-pm])\n",
    "\n",
    "def augmentMissing(pp, pm):\n",
    "    return ''.join([augmentSymbol(x, pm) for x in pp])\n",
    "\n",
    "def string2states(s):\n",
    "    mp = {'0':0,'1':1,'2':2,'3':3,'4':4,'C':5, 'L':6}\n",
    "    return [mp[x] for x in s]\n",
    "\n",
    "def genPP(age, term, rr, s0, pMissing=0.05):\n",
    "    if age <= 0:\n",
    "        return \"\"\n",
    "    pp = [np.random.choice(range(6), p=s0)]\n",
    "    if age <= 1:\n",
    "        return states2string(pp)\n",
    "    for i in range(age-1):\n",
    "        prev = pp[-1]\n",
    "        nxt = 5\n",
    "        if prev < 5 and (i < term or prev > 0):\n",
    "            nxt = np.random.choice(range(6), p=rr[prev,:])\n",
    "            if i >= term and nxt==0:\n",
    "                nxt = 5\n",
    "        pp.append(nxt)\n",
    "    return augmentMissing(states2string(list(reversed(pp))), pm=pMissing)\n",
    "\n",
    "def genCreditRR(rr, lamAge=20, lamTerm=10, emu=np.log(1e5), esigma=3, pMissing=0.1):\n",
    "    s0 = np.zeros(6)\n",
    "    s0[0] = rr[0,0] / (rr[0,0] + rr[0,5])\n",
    "    s0[5] = rr[0,5] / (rr[0,0] + rr[0,5])\n",
    "    \n",
    "    age = np.random.poisson(lam=lamAge)\n",
    "    term = np.random.poisson(lam=lamTerm)\n",
    "    limit = np.ceil(np.exp(np.random.normal(loc=emu, scale=esigma)) / 1e3) * 1e3\n",
    "    pp = genPP(age, term, rr, s0, pMissing=pMissing)\n",
    "    return (limit, term, pp)    \n",
    "\n",
    "def genCreditSimple(pBad=0.1, pEarlyRepayment=0.1, lamAge=20, pMissing=0.1):\n",
    "    pGood = (1 - pBad) \n",
    "    r0 = [(pGood - pEarlyRepayment), pBad, 0, 0, 0, pEarlyRepayment]\n",
    "    r1x = np.array([pGood * 0.33 / 0.9, 0.33, pBad * 0.33 / 0.1, 0, 0, pEarlyRepayment * pGood * 0.2])\n",
    "    r1s = np.sum(r1x)\n",
    "    r1 = [x/r1s for x in r1x]\n",
    "    r2p = [[0.10, 0.20, 0.10, 0.60, 0.0, 0.0],\n",
    "      [0.05, 0.05, 0.05, 0.05, 0.8, 0.0],\n",
    "      [0.03, 0.03, 0.02, 0.02, 0.9, 0.0]]\n",
    "    rr = np.array([r0] + [r1] + r2p)\n",
    "    return genCreditRR(rr, lamAge=lamAge, pMissing=pMissing)\n",
    "\n",
    "def getClientTarget(data):\n",
    "    num0 = 0\n",
    "    num1 = 0\n",
    "    num2p = 0\n",
    "    for r in data:\n",
    "        num0 += r[2].count('0') + r[2].count('L') + min(1, r[2].count('C'))\n",
    "        num1 += r[2].count('1')\n",
    "        num2p += 2 * r[2].count('2') + 3 * r[2].count('3') + 4 * r[2].count('4')\n",
    "    pGood = 0.5\n",
    "    if num0 + num1 + num2p > 0:\n",
    "        pGood = num0 / (0.1 + num0 + num1 + num2p)\n",
    "    pBad = 1 - pGood\n",
    "    return (np.random.binomial(1, pBad), pBad)\n",
    "\n",
    "def genClient(muBad=0.1, sigmaBad=0.1, pEarlyRepayment=0.05, muAge=20, sigmaAge=5, pMissing=0.1):\n",
    "    numCredits = 1\n",
    "    data = [genCreditSimple(pBad=min(0.5,np.random.lognormal(mean=np.log(muBad), sigma=sigmaBad)),\n",
    "                            pEarlyRepayment=pEarlyRepayment, pMissing=pMissing,\n",
    "                           lamAge=np.random.lognormal(mean=np.log(muAge), sigma=np.log(sigmaAge)))\n",
    "            for i in range(numCredits)]\n",
    "    target, prob = getClientTarget(data)\n",
    "    return (data, target, prob)\n",
    "\n",
    "#Generate sample (as in RRs)\n",
    "def genSample(numObs=1000, genObs=genClient):\n",
    "    res = []\n",
    "    for i in range(numObs):\n",
    "        (obs, trgt, prob) = genObs()\n",
    "        row = [i, trgt, prob] + list(obs[0])\n",
    "        res.append(row)\n",
    "    return pd.DataFrame(np.array(res),\n",
    "                        columns=['accnt_id', 'trgt', 'prob', 'limit0', 'term0', 'pp0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnt_id</th>\n",
       "      <th>trgt</th>\n",
       "      <th>prob</th>\n",
       "      <th>limit0</th>\n",
       "      <th>term0</th>\n",
       "      <th>pp0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9750623441396509</td>\n",
       "      <td>3976000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4444444432110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032258064516129115</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>CCC00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23664122137404575</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>15</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09909909909909909</td>\n",
       "      <td>4519000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44099378881987583</td>\n",
       "      <td>234000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCCC032100001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09090909090909094</td>\n",
       "      <td>1288000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09090909090909094</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014084507042253502</td>\n",
       "      <td>29000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCCCCC0000-00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3548387096774194</td>\n",
       "      <td>167000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCCCC-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.36936936936936937</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCCCCCC000000111-1-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  accnt_id trgt                  prob     limit0 term0  \\\n",
       "0        0    1    0.9750623441396509  3976000.0    13   \n",
       "1        1    0  0.032258064516129115    69000.0    12   \n",
       "2        2    0   0.23664122137404575     1000.0    15   \n",
       "3        3    0   0.09909909909909909  4519000.0     9   \n",
       "4        4    1   0.44099378881987583   234000.0    11   \n",
       "5        5    0   0.09090909090909094  1288000.0     6   \n",
       "6        6    0   0.09090909090909094     4000.0    14   \n",
       "7        7    0  0.014084507042253502    29000.0    11   \n",
       "8        8    0    0.3548387096774194   167000.0    11   \n",
       "9        9    0   0.36936936936936937    10000.0    11   \n",
       "\n",
       "                                                 pp0  \n",
       "0                                      4444444432110  \n",
       "1                                              CCC00  \n",
       "2  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...  \n",
       "3  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...  \n",
       "4                                  CCCCC032100001000  \n",
       "5                                                  0  \n",
       "6                                                  0  \n",
       "7                                     CCCCCCC0000-00  \n",
       "8                                          CCCCCC-10  \n",
       "9                               CCCCCCCC000000111-1-  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30 sec (20k and 2k)\n",
    "train_sample_src = genSample(20000)\n",
    "valid_sample_src = genSample(2000)\n",
    "train_sample_src[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transformation functions\n",
    "def transformPP(term, pp):\n",
    "    if pp is None:\n",
    "        return pp\n",
    "    pplen = len(pp)\n",
    "    lst = max(0, pplen - term)\n",
    "    return pp[:lst] + pp[lst:].replace('C', 'L')\n",
    "\n",
    "def truncPP(term, pp, trlen=60):\n",
    "    if pp is None:\n",
    "        if term is None:\n",
    "            return 'X'*trlen\n",
    "        else:\n",
    "            return 'N'*trlen\n",
    "    pplen = len(pp)\n",
    "    if pplen >= trlen:\n",
    "        return pp[:trlen]\n",
    "    return pp + 'N'*(trlen - pplen)\n",
    "\n",
    "def transformDF(df, name='pp{0}t', trlen=60):\n",
    "    num = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    res = df.copy()\n",
    "    for i in range(num):\n",
    "        cols = ['pp{0}'.format(i), 'term{0}'.format(i)]\n",
    "        res[name.format(i)] = [truncPP(int(t), transformPP(int(t), p), trlen) for _,(p,t) in df[cols].iterrows()]\n",
    "    return res\n",
    "\n",
    "\n",
    "def transformToTensor(df, pp='pp{0}t', useX=False, numCredits=None):\n",
    "    #check dimensions\n",
    "    #num credits\n",
    "    num_credits = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    if numCredits is not None:\n",
    "        if num_credits < numCredits:\n",
    "            raise \"Provided <numCredits> is greater than number of fields in DF\"\n",
    "        num_credits = numCredits\n",
    "    num_mobs = None\n",
    "    for i in range(num_credits):\n",
    "        lens = list(set(len(x) for x in df[pp.format(i)] if x is not None))\n",
    "        numx = np.sum(['X' in x for x in df[pp.format(i)] if x is not None])\n",
    "        if numx > 0 and not useX:\n",
    "            raise \"Not supposed to use X, but X is found in observations!\"\n",
    "        if len(lens) != 1:\n",
    "            raise \"Expected same length in all observations!\"\n",
    "        if num_mobs is None:\n",
    "            num_mobs = lens[0]\n",
    "        if num_mobs != lens[0]:\n",
    "            raise \"Expected same length in all observations!\"\n",
    "    mapping = {'0':0,'1':1,'2':2,'3':3,'4':4,'-':5,'L':6,'C':7,'N':8}\n",
    "    if useX:\n",
    "        mapping['X'] = 9\n",
    "    \n",
    "    res = []\n",
    "    res_meta = []\n",
    "    res_trgt = []\n",
    "    for _, r in df.iterrows():\n",
    "        cred = []\n",
    "        cred_meta = []\n",
    "        res_trgt.append(r.trgt)\n",
    "        for i in range(num_credits):\n",
    "            cred.append([mapping[x] for x in reversed(r[pp.format(i)])])\n",
    "            cred_meta.append([-1 if r[f.format(i)] is None else r[f.format(i)] for f in ['limit{0}','term{0}']])\n",
    "        res.append(cred)\n",
    "        res_meta.append(cred_meta)\n",
    "    return np.array(res, dtype=np.int32), np.array(res_meta, dtype=np.float32), np.array(res_trgt, dtype=np.int32)\n",
    "\n",
    "def randomBatch(tensorTuple, batchSize=64):\n",
    "    ids = np.random.choice(range(tensorTuple[0].shape[0]), batchSize)\n",
    "    return (x[ids,] for x in tensorTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_logit(x, clampX=1e-3):\n",
    "    p0 = np.maximum(clampX, x)\n",
    "    p1 = np.maximum(clampX, 1-x)\n",
    "    return np.log(p0 / p1)\n",
    "    \n",
    "def extractFeatures(df, pp='pp{0}'):\n",
    "    num_credits = np.sum([x.replace('pp','').isnumeric() for x in df.columns])\n",
    "    cnt_symbols = ['0', '1', '2', '3', '4', '5', 'L', 'C', '00', '01', '0L', '0C', '10', '11', '12', '1L', '1C']\n",
    "    \n",
    "    rr_names = ['f_rr{0}_01{1}', 'f_rr{0}_00{1}', 'f_rr{0}_10{1}', 'f_rr{0}_11{1}', 'f_rr{0}_12{1}']\n",
    "    rr_types = ['', 'f0', 'f1', 'lgt']\n",
    "    rr_final = [x.format(i) for x in ['f_rr{0}_0d', 'f_rr{0}_1d'] for i in range(num_credits)] + [x.format(i, y) for x in rr_names for y in rr_types for i in range(num_credits)]\n",
    "    \n",
    "    new_features = ['f_num{0}_{1}'.format(i, x) for i in range(num_credits) for x in cnt_symbols] + rr_final\n",
    "    res = pd.concat([df, pd.DataFrame(columns=new_features)])\n",
    "    \n",
    "    for _, r in res.iterrows():\n",
    "        for i in range(num_credits):\n",
    "            paypat = r[pp.format(i)]\n",
    "            cnts = {x:paypat.count(x) for x in cnt_symbols}\n",
    "            for x,v in cnts.items():\n",
    "                r['f_num{0}_{1}'.format(i, x)] = v\n",
    "            r0 = cnts['00'] + cnts['01'] + cnts['0L'] + cnts['0C']\n",
    "            r1 = cnts['10'] + cnts['11'] + cnts['12'] + cnts['1L'] + cnts['1C']\n",
    "            r['f_rr{0}_0d'.format(i)] = r0\n",
    "            r['f_rr{0}_1d'.format(i)] = r0\n",
    "            r['f_rr{0}_01'.format(i)] = (cnts['01'] / r0 if r0 > 0 else 0) \n",
    "            r['f_rr{0}_00'.format(i)] = ((cnts['00'] + cnts['0L'] + cnts['0C']) / r0 if r0 > 0 else 1)\n",
    "            r['f_rr{0}_12'.format(i)] = (cnts['12'] / r1 if r1 > 0 else 0)\n",
    "            r['f_rr{0}_11'.format(i)] = (cnts['11'] / r1 if r1 > 0 else 0)\n",
    "            r['f_rr{0}_10'.format(i)] = ((cnts['10'] + cnts['1L'] + cnts['1C']) / r1 if r1 > 0 else 0)\n",
    "            for f in ['f_rr{0}_01{1}', 'f_rr{0}_00{1}', 'f_rr{0}_10{1}', 'f_rr{0}_11{1}', 'f_rr{0}_12{1}']:\n",
    "                r[f.format(i, 'f0')] = (1 if r[f.format(i,'')]==0 else 0)\n",
    "                r[f.format(i, 'f1')] = (1 if r[f.format(i,'')]==1 else 0)\n",
    "                r[f.format(i, 'lgt')] = safe_logit(r[f.format(i,'')])   \n",
    "    return res\n",
    "\n",
    "def featuresToTensor(df):\n",
    "    features = [x for x in df.columns if x.find('f_') == 0]\n",
    "    return np.array(df[features]), np.array(df.trgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#20 sec (20k and 2k)\n",
    "train_sample = transformDF(train_sample_src)\n",
    "valid_sample = transformDF(valid_sample_src)\n",
    "train_wf = extractFeatures(train_sample, pp='pp{0}t')\n",
    "valid_wf = extractFeatures(valid_sample, pp='pp{0}t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = featuresToTensor(train_wf)\n",
    "valid_x, valid_y = featuresToTensor(valid_wf)\n",
    "\n",
    "logreg0 = LogisticRegression().fit(train_x, train_y)\n",
    "train_p = logreg0.predict_proba(train_x)[:,1]\n",
    "valid_p = logreg0.predict_proba(valid_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete\n"
     ]
    }
   ],
   "source": [
    "param_H_size   = [300, 200, 100]\n",
    "param_RBM_hid  = 60\n",
    "param_LR       = 1e-3\n",
    "\n",
    "size_pp_dictionary = 9\n",
    "size_pp_time = 60\n",
    "size_input = size_pp_time * size_pp_dictionary\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tfIn_PP = tf.placeholder(shape=(None, 1, None), dtype=tf.int32)\n",
    "tfIn_hrand = tf.placeholder(shape=(None, param_RBM_hid), dtype=tf.float32)\n",
    "#tfIn_vrand = tf.placeholder(shape=(None, size_input), dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(name='DATA-TRANSFORMATION'):\n",
    "    tfX  = tf.reshape(tf.reduce_sum(tf.one_hot(tfIn_PP, size_pp_dictionary), axis=1), shape=(-1, size_input))\n",
    "    \n",
    "with tf.name_scope(name='RBM-STATE'):\n",
    "    tfW  = tf.Variable(tf.random_normal(mean=0.0, stddev=0.1, shape=(size_input, param_RBM_hid)), name='RBM-W')\n",
    "    tfBH = tf.Variable(tf.zeros([param_RBM_hid]), name='RBM-BIAS-H')\n",
    "    tfBV = tf.Variable(tf.zeros([size_input]), name='RBM-BIAS-V')\n",
    "    \n",
    "#1-step gibbs sampling:\n",
    "with tf.name_scope(name='CD-k'):\n",
    "    tf_hp0 = tf.nn.sigmoid(tf.matmul(tfX, tfW) + tfBH)\n",
    "    tf_hs0 = tf.nn.relu(tf.sign(tf_hp0 - tfIn_hrand))\n",
    "    tf_positive = tf.matmul(tf.transpose(tfX), tf_hs0, name='POSITIVE')\n",
    "    tf_vp = tf.nn.sigmoid(tf.matmul(tf_hp0, tf.transpose(tfW)) + tfBV)\n",
    "    tf_hp1 = tf.nn.sigmoid(tf.matmul(tf_vp, tfW) + tfBH)\n",
    "    tf_hs1 = tf.nn.relu(tf.sign(tf_hp1 - tfIn_hrand))\n",
    "    tf_negative = tf.matmul(tf.transpose(tf_vp), tf_hp1)\n",
    "    \n",
    "tfFeatures = tf_hp0\n",
    "\n",
    "with tf.name_scope(name='TRAINING'):\n",
    "    tf_train_w = tfW.assign_add(param_LR * (tf_positive - tf_negative))\n",
    "    tf_train_bh = tfBH.assign_add(param_LR * tf.reduce_mean(tf_hp0 - tf_hp1, axis=0))\n",
    "    tf_train_bv = tfBV.assign_add(param_LR * tf.reduce_mean(tfX - tf_vp, axis=0))\n",
    "    tfTrain = tf.group(tf_train_w, tf_train_bh, tf_train_bv)\n",
    "    tfLoss = tf.sqrt(tf.reduce_mean(tf.square(tfX - tf_vp)))\n",
    "\n",
    "#tfCostSummary = tf.summary.scalar('RBM-Cost', tfCost)\n",
    "\n",
    "print('Graph creation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pp, train_meta, train_trgt = transformToTensor(train_sample, useX=False, numCredits=1)\n",
    "valid_pp, valid_meta, valid_trgt = transformToTensor(valid_sample, useX=False, numCredits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (0.16 sec): loss changed from 0.504 to 0.143\t\t0.142\n",
      "Epoch 1 (0.15 sec): loss changed from 0.151 to 0.137\t\t0.129\n",
      "Epoch 2 (0.20 sec): loss changed from 0.127 to 0.119\t\t0.123\n",
      "Epoch 3 (0.16 sec): loss changed from 0.122 to 0.117\t\t0.119\n",
      "Epoch 4 (0.17 sec): loss changed from 0.119 to 0.115\t\t0.116\n",
      "Epoch 5 (0.15 sec): loss changed from 0.125 to 0.121\t\t0.114\n",
      "Epoch 6 (0.16 sec): loss changed from 0.119 to 0.116\t\t0.113\n",
      "Epoch 7 (0.15 sec): loss changed from 0.107 to 0.104\t\t0.111\n",
      "Epoch 8 (0.17 sec): loss changed from 0.115 to 0.112\t\t0.110\n",
      "Epoch 9 (0.21 sec): loss changed from 0.115 to 0.112\t\t0.109\n",
      "Epoch 10 (0.23 sec): loss changed from 0.104 to 0.102\t\t0.108\n",
      "Epoch 11 (0.16 sec): loss changed from 0.110 to 0.107\t\t0.107\n",
      "Epoch 12 (0.16 sec): loss changed from 0.110 to 0.108\t\t0.106\n",
      "Epoch 13 (0.16 sec): loss changed from 0.104 to 0.101\t\t0.105\n",
      "Epoch 14 (0.20 sec): loss changed from 0.105 to 0.103\t\t0.105\n",
      "Epoch 15 (0.20 sec): loss changed from 0.109 to 0.107\t\t0.103\n",
      "Epoch 16 (0.27 sec): loss changed from 0.106 to 0.104\t\t0.103\n",
      "Epoch 17 (0.23 sec): loss changed from 0.106 to 0.103\t\t0.102\n",
      "Epoch 18 (0.19 sec): loss changed from 0.104 to 0.101\t\t0.101\n",
      "Epoch 19 (0.19 sec): loss changed from 0.106 to 0.104\t\t0.100\n",
      "Epoch 20 (0.20 sec): loss changed from 0.098 to 0.096\t\t0.100\n",
      "Epoch 21 (0.25 sec): loss changed from 0.099 to 0.097\t\t0.099\n",
      "Epoch 22 (0.20 sec): loss changed from 0.100 to 0.098\t\t0.098\n",
      "Epoch 23 (0.15 sec): loss changed from 0.099 to 0.097\t\t0.098\n",
      "Epoch 24 (0.16 sec): loss changed from 0.104 to 0.102\t\t0.097\n",
      "Epoch 25 (0.15 sec): loss changed from 0.099 to 0.097\t\t0.097\n",
      "Epoch 26 (0.16 sec): loss changed from 0.096 to 0.095\t\t0.096\n",
      "Epoch 27 (0.18 sec): loss changed from 0.095 to 0.092\t\t0.096\n",
      "Epoch 28 (0.27 sec): loss changed from 0.096 to 0.094\t\t0.095\n",
      "Epoch 29 (0.16 sec): loss changed from 0.091 to 0.089\t\t0.095\n",
      "Epoch 30 (0.16 sec): loss changed from 0.100 to 0.098\t\t0.095\n",
      "Epoch 31 (0.15 sec): loss changed from 0.095 to 0.092\t\t0.094\n",
      "Epoch 32 (0.16 sec): loss changed from 0.094 to 0.092\t\t0.094\n",
      "Epoch 33 (0.15 sec): loss changed from 0.089 to 0.087\t\t0.094\n",
      "Epoch 34 (0.16 sec): loss changed from 0.095 to 0.093\t\t0.093\n",
      "Epoch 35 (0.17 sec): loss changed from 0.094 to 0.093\t\t0.093\n",
      "Epoch 36 (0.22 sec): loss changed from 0.091 to 0.089\t\t0.093\n",
      "Epoch 37 (0.18 sec): loss changed from 0.092 to 0.090\t\t0.092\n",
      "Epoch 38 (0.16 sec): loss changed from 0.088 to 0.086\t\t0.092\n",
      "Epoch 39 (0.16 sec): loss changed from 0.094 to 0.093\t\t0.092\n",
      "Epoch 40 (0.15 sec): loss changed from 0.089 to 0.087\t\t0.092\n",
      "Epoch 41 (0.16 sec): loss changed from 0.091 to 0.089\t\t0.091\n",
      "Epoch 42 (0.24 sec): loss changed from 0.087 to 0.085\t\t0.091\n",
      "Epoch 43 (0.20 sec): loss changed from 0.088 to 0.086\t\t0.091\n",
      "Epoch 44 (0.16 sec): loss changed from 0.095 to 0.093\t\t0.091\n",
      "Epoch 45 (0.17 sec): loss changed from 0.086 to 0.084\t\t0.090\n",
      "Epoch 46 (0.26 sec): loss changed from 0.093 to 0.092\t\t0.090\n",
      "Epoch 47 (0.21 sec): loss changed from 0.092 to 0.090\t\t0.090\n",
      "Epoch 48 (0.15 sec): loss changed from 0.085 to 0.083\t\t0.090\n",
      "Epoch 49 (0.16 sec): loss changed from 0.089 to 0.087\t\t0.090\n",
      "Epoch 50 (0.15 sec): loss changed from 0.086 to 0.085\t\t0.089\n",
      "Epoch 51 (0.16 sec): loss changed from 0.093 to 0.091\t\t0.089\n",
      "Epoch 52 (0.16 sec): loss changed from 0.091 to 0.089\t\t0.089\n",
      "Epoch 53 (0.17 sec): loss changed from 0.093 to 0.091\t\t0.089\n",
      "Epoch 54 (0.21 sec): loss changed from 0.087 to 0.085\t\t0.089\n",
      "Epoch 55 (0.19 sec): loss changed from 0.090 to 0.088\t\t0.089\n",
      "Epoch 56 (0.17 sec): loss changed from 0.092 to 0.091\t\t0.088\n",
      "Epoch 57 (0.17 sec): loss changed from 0.092 to 0.090\t\t0.088\n",
      "Epoch 58 (0.15 sec): loss changed from 0.092 to 0.089\t\t0.088\n",
      "Epoch 59 (0.16 sec): loss changed from 0.088 to 0.086\t\t0.088\n",
      "Epoch 60 (0.15 sec): loss changed from 0.086 to 0.085\t\t0.087\n",
      "Epoch 61 (0.17 sec): loss changed from 0.085 to 0.083\t\t0.087\n",
      "Epoch 62 (0.15 sec): loss changed from 0.091 to 0.090\t\t0.087\n",
      "Epoch 63 (0.17 sec): loss changed from 0.085 to 0.083\t\t0.087\n",
      "Epoch 64 (0.15 sec): loss changed from 0.086 to 0.084\t\t0.087\n",
      "Epoch 65 (0.16 sec): loss changed from 0.088 to 0.086\t\t0.087\n",
      "Epoch 66 (0.15 sec): loss changed from 0.086 to 0.085\t\t0.087\n",
      "Epoch 67 (0.17 sec): loss changed from 0.082 to 0.080\t\t0.086\n",
      "Epoch 68 (0.24 sec): loss changed from 0.083 to 0.082\t\t0.086\n",
      "Epoch 69 (0.20 sec): loss changed from 0.089 to 0.087\t\t0.086\n",
      "Epoch 70 (0.16 sec): loss changed from 0.092 to 0.089\t\t0.086\n",
      "Epoch 71 (0.16 sec): loss changed from 0.085 to 0.083\t\t0.086\n",
      "Epoch 72 (0.15 sec): loss changed from 0.086 to 0.085\t\t0.086\n",
      "Epoch 73 (0.16 sec): loss changed from 0.095 to 0.093\t\t0.085\n",
      "Epoch 74 (0.21 sec): loss changed from 0.082 to 0.081\t\t0.085\n",
      "Epoch 75 (0.23 sec): loss changed from 0.086 to 0.084\t\t0.085\n",
      "Epoch 76 (0.15 sec): loss changed from 0.083 to 0.081\t\t0.085\n",
      "Epoch 77 (0.17 sec): loss changed from 0.081 to 0.080\t\t0.085\n",
      "Epoch 78 (0.16 sec): loss changed from 0.079 to 0.077\t\t0.085\n",
      "Epoch 79 (0.18 sec): loss changed from 0.091 to 0.089\t\t0.085\n",
      "Epoch 80 (0.24 sec): loss changed from 0.086 to 0.084\t\t0.085\n",
      "Epoch 81 (0.20 sec): loss changed from 0.085 to 0.083\t\t0.085\n",
      "Epoch 82 (0.16 sec): loss changed from 0.083 to 0.082\t\t0.085\n",
      "Epoch 83 (0.16 sec): loss changed from 0.080 to 0.078\t\t0.084\n",
      "Epoch 84 (0.15 sec): loss changed from 0.082 to 0.080\t\t0.084\n",
      "Epoch 85 (0.17 sec): loss changed from 0.086 to 0.084\t\t0.084\n",
      "Epoch 86 (0.15 sec): loss changed from 0.083 to 0.081\t\t0.084\n",
      "Epoch 87 (0.16 sec): loss changed from 0.087 to 0.086\t\t0.084\n",
      "Epoch 88 (0.15 sec): loss changed from 0.081 to 0.079\t\t0.084\n",
      "Epoch 89 (0.21 sec): loss changed from 0.081 to 0.079\t\t0.084\n",
      "Epoch 90 (0.22 sec): loss changed from 0.081 to 0.079\t\t0.084\n",
      "Epoch 91 (0.17 sec): loss changed from 0.084 to 0.083\t\t0.084\n",
      "Epoch 92 (0.16 sec): loss changed from 0.081 to 0.079\t\t0.083\n",
      "Epoch 93 (0.15 sec): loss changed from 0.092 to 0.091\t\t0.083\n",
      "Epoch 94 (0.23 sec): loss changed from 0.080 to 0.078\t\t0.083\n",
      "Epoch 95 (0.22 sec): loss changed from 0.084 to 0.082\t\t0.083\n",
      "Epoch 96 (0.15 sec): loss changed from 0.082 to 0.080\t\t0.083\n",
      "Epoch 97 (0.17 sec): loss changed from 0.080 to 0.079\t\t0.083\n",
      "Epoch 98 (0.15 sec): loss changed from 0.081 to 0.079\t\t0.083\n",
      "Epoch 99 (0.16 sec): loss changed from 0.085 to 0.083\t\t0.082\n",
      "Epoch 100 (0.15 sec): loss changed from 0.085 to 0.083\t\t0.083\n",
      "Epoch 101 (0.24 sec): loss changed from 0.084 to 0.082\t\t0.082\n",
      "Epoch 102 (0.21 sec): loss changed from 0.080 to 0.078\t\t0.082\n",
      "Epoch 103 (0.18 sec): loss changed from 0.081 to 0.080\t\t0.082\n",
      "Epoch 104 (0.17 sec): loss changed from 0.084 to 0.082\t\t0.082\n",
      "Epoch 105 (0.15 sec): loss changed from 0.077 to 0.076\t\t0.082\n",
      "Epoch 106 (0.17 sec): loss changed from 0.080 to 0.079\t\t0.082\n",
      "Epoch 107 (0.18 sec): loss changed from 0.075 to 0.073\t\t0.081\n",
      "Epoch 108 (0.21 sec): loss changed from 0.083 to 0.081\t\t0.081\n",
      "Epoch 109 (0.23 sec): loss changed from 0.080 to 0.078\t\t0.081\n",
      "Epoch 110 (0.21 sec): loss changed from 0.081 to 0.080\t\t0.081\n",
      "Epoch 111 (0.19 sec): loss changed from 0.078 to 0.076\t\t0.081\n",
      "Epoch 112 (0.19 sec): loss changed from 0.079 to 0.077\t\t0.081\n",
      "Epoch 113 (0.19 sec): loss changed from 0.084 to 0.082\t\t0.081\n",
      "Epoch 114 (0.22 sec): loss changed from 0.078 to 0.076\t\t0.081\n",
      "Epoch 115 (0.23 sec): loss changed from 0.078 to 0.076\t\t0.081\n",
      "Epoch 116 (0.21 sec): loss changed from 0.078 to 0.076\t\t0.080\n",
      "Epoch 117 (0.20 sec): loss changed from 0.080 to 0.079\t\t0.080\n",
      "Epoch 118 (0.26 sec): loss changed from 0.082 to 0.081\t\t0.080\n",
      "Epoch 119 (0.17 sec): loss changed from 0.085 to 0.083\t\t0.080\n",
      "Epoch 120 (0.25 sec): loss changed from 0.079 to 0.078\t\t0.080\n",
      "Epoch 121 (0.20 sec): loss changed from 0.076 to 0.075\t\t0.080\n",
      "Epoch 122 (0.16 sec): loss changed from 0.082 to 0.081\t\t0.080\n",
      "Epoch 123 (0.18 sec): loss changed from 0.081 to 0.079\t\t0.080\n",
      "Epoch 124 (0.15 sec): loss changed from 0.084 to 0.082\t\t0.080\n",
      "Epoch 125 (0.24 sec): loss changed from 0.083 to 0.080\t\t0.080\n",
      "Epoch 126 (0.19 sec): loss changed from 0.083 to 0.081\t\t0.079\n",
      "Epoch 127 (0.19 sec): loss changed from 0.079 to 0.077\t\t0.079\n",
      "Epoch 128 (0.19 sec): loss changed from 0.077 to 0.076\t\t0.079\n",
      "Epoch 129 (0.17 sec): loss changed from 0.085 to 0.083\t\t0.079\n",
      "Epoch 130 (0.19 sec): loss changed from 0.078 to 0.076\t\t0.079\n",
      "Epoch 131 (0.19 sec): loss changed from 0.077 to 0.076\t\t0.079\n",
      "Epoch 132 (0.17 sec): loss changed from 0.081 to 0.079\t\t0.079\n",
      "Epoch 133 (0.17 sec): loss changed from 0.077 to 0.076\t\t0.079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134 (0.21 sec): loss changed from 0.082 to 0.080\t\t0.079\n",
      "Epoch 135 (0.24 sec): loss changed from 0.079 to 0.077\t\t0.079\n",
      "Epoch 136 (0.18 sec): loss changed from 0.080 to 0.078\t\t0.079\n",
      "Epoch 137 (0.16 sec): loss changed from 0.079 to 0.077\t\t0.079\n",
      "Epoch 138 (0.17 sec): loss changed from 0.076 to 0.075\t\t0.079\n",
      "Epoch 139 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.078\n",
      "Epoch 140 (0.17 sec): loss changed from 0.074 to 0.072\t\t0.079\n",
      "Epoch 141 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.078\n",
      "Epoch 142 (0.19 sec): loss changed from 0.079 to 0.077\t\t0.078\n",
      "Epoch 143 (0.15 sec): loss changed from 0.072 to 0.071\t\t0.078\n",
      "Epoch 144 (0.16 sec): loss changed from 0.079 to 0.077\t\t0.078\n",
      "Epoch 145 (0.15 sec): loss changed from 0.078 to 0.076\t\t0.078\n",
      "Epoch 146 (0.17 sec): loss changed from 0.074 to 0.072\t\t0.078\n",
      "Epoch 147 (0.17 sec): loss changed from 0.078 to 0.076\t\t0.078\n",
      "Epoch 148 (0.18 sec): loss changed from 0.081 to 0.079\t\t0.078\n",
      "Epoch 149 (0.16 sec): loss changed from 0.076 to 0.074\t\t0.078\n",
      "Epoch 150 (0.17 sec): loss changed from 0.077 to 0.075\t\t0.078\n",
      "Epoch 151 (0.15 sec): loss changed from 0.076 to 0.074\t\t0.078\n",
      "Epoch 152 (0.17 sec): loss changed from 0.084 to 0.082\t\t0.077\n",
      "Epoch 153 (0.23 sec): loss changed from 0.078 to 0.076\t\t0.077\n",
      "Epoch 154 (0.16 sec): loss changed from 0.078 to 0.076\t\t0.077\n",
      "Epoch 155 (0.17 sec): loss changed from 0.075 to 0.073\t\t0.077\n",
      "Epoch 156 (0.19 sec): loss changed from 0.078 to 0.075\t\t0.077\n",
      "Epoch 157 (0.16 sec): loss changed from 0.082 to 0.080\t\t0.077\n",
      "Epoch 158 (0.18 sec): loss changed from 0.076 to 0.074\t\t0.077\n",
      "Epoch 159 (0.17 sec): loss changed from 0.073 to 0.071\t\t0.077\n",
      "Epoch 160 (0.18 sec): loss changed from 0.072 to 0.071\t\t0.077\n",
      "Epoch 161 (0.16 sec): loss changed from 0.078 to 0.076\t\t0.077\n",
      "Epoch 162 (0.16 sec): loss changed from 0.078 to 0.076\t\t0.077\n",
      "Epoch 163 (0.15 sec): loss changed from 0.076 to 0.075\t\t0.077\n",
      "Epoch 164 (0.16 sec): loss changed from 0.077 to 0.075\t\t0.077\n",
      "Epoch 165 (0.15 sec): loss changed from 0.079 to 0.077\t\t0.076\n",
      "Epoch 166 (0.16 sec): loss changed from 0.074 to 0.073\t\t0.077\n",
      "Epoch 167 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.076\n",
      "Epoch 168 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.076\n",
      "Epoch 169 (0.15 sec): loss changed from 0.079 to 0.078\t\t0.076\n",
      "Epoch 170 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.076\n",
      "Epoch 171 (0.15 sec): loss changed from 0.078 to 0.076\t\t0.076\n",
      "Epoch 172 (0.16 sec): loss changed from 0.078 to 0.076\t\t0.076\n",
      "Epoch 173 (0.15 sec): loss changed from 0.074 to 0.073\t\t0.076\n",
      "Epoch 174 (0.16 sec): loss changed from 0.075 to 0.074\t\t0.076\n",
      "Epoch 175 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.076\n",
      "Epoch 176 (0.16 sec): loss changed from 0.080 to 0.078\t\t0.076\n",
      "Epoch 177 (0.15 sec): loss changed from 0.078 to 0.076\t\t0.076\n",
      "Epoch 178 (0.16 sec): loss changed from 0.077 to 0.076\t\t0.076\n",
      "Epoch 179 (0.18 sec): loss changed from 0.076 to 0.074\t\t0.076\n",
      "Epoch 180 (0.16 sec): loss changed from 0.075 to 0.073\t\t0.076\n",
      "Epoch 181 (0.17 sec): loss changed from 0.076 to 0.074\t\t0.076\n",
      "Epoch 182 (0.16 sec): loss changed from 0.075 to 0.074\t\t0.076\n",
      "Epoch 183 (0.23 sec): loss changed from 0.077 to 0.075\t\t0.075\n",
      "Epoch 184 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.076\n",
      "Epoch 185 (0.17 sec): loss changed from 0.081 to 0.079\t\t0.075\n",
      "Epoch 186 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.075\n",
      "Epoch 187 (0.17 sec): loss changed from 0.074 to 0.072\t\t0.075\n",
      "Epoch 188 (0.15 sec): loss changed from 0.077 to 0.075\t\t0.075\n",
      "Epoch 189 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.075\n",
      "Epoch 190 (0.15 sec): loss changed from 0.068 to 0.066\t\t0.075\n",
      "Epoch 191 (0.17 sec): loss changed from 0.069 to 0.068\t\t0.075\n",
      "Epoch 192 (0.15 sec): loss changed from 0.077 to 0.075\t\t0.075\n",
      "Epoch 193 (0.16 sec): loss changed from 0.074 to 0.073\t\t0.075\n",
      "Epoch 194 (0.15 sec): loss changed from 0.079 to 0.077\t\t0.075\n",
      "Epoch 195 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.075\n",
      "Epoch 196 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.075\n",
      "Epoch 197 (0.16 sec): loss changed from 0.074 to 0.073\t\t0.075\n",
      "Epoch 198 (0.15 sec): loss changed from 0.068 to 0.066\t\t0.075\n",
      "Epoch 199 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.075\n",
      "Epoch 200 (0.15 sec): loss changed from 0.070 to 0.068\t\t0.075\n",
      "Epoch 201 (0.17 sec): loss changed from 0.076 to 0.075\t\t0.075\n",
      "Epoch 202 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.075\n",
      "Epoch 203 (0.18 sec): loss changed from 0.072 to 0.070\t\t0.074\n",
      "Epoch 204 (0.19 sec): loss changed from 0.073 to 0.071\t\t0.074\n",
      "Epoch 205 (0.20 sec): loss changed from 0.074 to 0.072\t\t0.074\n",
      "Epoch 206 (0.20 sec): loss changed from 0.077 to 0.075\t\t0.074\n",
      "Epoch 207 (0.22 sec): loss changed from 0.076 to 0.074\t\t0.074\n",
      "Epoch 208 (0.20 sec): loss changed from 0.076 to 0.074\t\t0.074\n",
      "Epoch 209 (0.20 sec): loss changed from 0.078 to 0.077\t\t0.074\n",
      "Epoch 210 (0.21 sec): loss changed from 0.071 to 0.070\t\t0.074\n",
      "Epoch 211 (0.19 sec): loss changed from 0.073 to 0.072\t\t0.074\n",
      "Epoch 212 (0.16 sec): loss changed from 0.075 to 0.073\t\t0.074\n",
      "Epoch 213 (0.16 sec): loss changed from 0.075 to 0.073\t\t0.074\n",
      "Epoch 214 (0.15 sec): loss changed from 0.073 to 0.072\t\t0.074\n",
      "Epoch 215 (0.17 sec): loss changed from 0.074 to 0.073\t\t0.074\n",
      "Epoch 216 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.074\n",
      "Epoch 217 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.074\n",
      "Epoch 218 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.074\n",
      "Epoch 219 (0.16 sec): loss changed from 0.074 to 0.073\t\t0.074\n",
      "Epoch 220 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.074\n",
      "Epoch 221 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.074\n",
      "Epoch 222 (0.15 sec): loss changed from 0.080 to 0.078\t\t0.074\n",
      "Epoch 223 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.074\n",
      "Epoch 224 (0.15 sec): loss changed from 0.074 to 0.073\t\t0.074\n",
      "Epoch 225 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.074\n",
      "Epoch 226 (0.15 sec): loss changed from 0.076 to 0.074\t\t0.074\n",
      "Epoch 227 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.073\n",
      "Epoch 228 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.073\n",
      "Epoch 229 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.073\n",
      "Epoch 230 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.073\n",
      "Epoch 231 (0.17 sec): loss changed from 0.072 to 0.070\t\t0.073\n",
      "Epoch 232 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.073\n",
      "Epoch 233 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.073\n",
      "Epoch 234 (0.15 sec): loss changed from 0.074 to 0.073\t\t0.073\n",
      "Epoch 235 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.073\n",
      "Epoch 236 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.073\n",
      "Epoch 237 (0.17 sec): loss changed from 0.075 to 0.073\t\t0.073\n",
      "Epoch 238 (0.16 sec): loss changed from 0.077 to 0.075\t\t0.073\n",
      "Epoch 239 (0.16 sec): loss changed from 0.070 to 0.069\t\t0.073\n",
      "Epoch 240 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.073\n",
      "Epoch 241 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.073\n",
      "Epoch 242 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.073\n",
      "Epoch 243 (0.16 sec): loss changed from 0.078 to 0.076\t\t0.073\n",
      "Epoch 244 (0.15 sec): loss changed from 0.079 to 0.077\t\t0.073\n",
      "Epoch 245 (0.16 sec): loss changed from 0.069 to 0.068\t\t0.073\n",
      "Epoch 246 (0.15 sec): loss changed from 0.074 to 0.072\t\t0.073\n",
      "Epoch 247 (0.17 sec): loss changed from 0.070 to 0.069\t\t0.073\n",
      "Epoch 248 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.073\n",
      "Epoch 249 (0.17 sec): loss changed from 0.070 to 0.068\t\t0.073\n",
      "Epoch 250 (0.15 sec): loss changed from 0.064 to 0.062\t\t0.073\n",
      "Epoch 251 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.073\n",
      "Epoch 252 (0.15 sec): loss changed from 0.077 to 0.075\t\t0.072\n",
      "Epoch 253 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.072\n",
      "Epoch 254 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.072\n",
      "Epoch 255 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.072\n",
      "Epoch 256 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.072\n",
      "Epoch 257 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.072\n",
      "Epoch 258 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.072\n",
      "Epoch 259 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.072\n",
      "Epoch 260 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.072\n",
      "Epoch 261 (0.16 sec): loss changed from 0.076 to 0.074\t\t0.072\n",
      "Epoch 262 (0.15 sec): loss changed from 0.074 to 0.073\t\t0.072\n",
      "Epoch 263 (0.17 sec): loss changed from 0.072 to 0.070\t\t0.072\n",
      "Epoch 264 (0.15 sec): loss changed from 0.067 to 0.065\t\t0.072\n",
      "Epoch 265 (0.17 sec): loss changed from 0.072 to 0.071\t\t0.072\n",
      "Epoch 266 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.072\n",
      "Epoch 268 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.072\n",
      "Epoch 269 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.072\n",
      "Epoch 270 (0.15 sec): loss changed from 0.065 to 0.064\t\t0.072\n",
      "Epoch 271 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.072\n",
      "Epoch 272 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.072\n",
      "Epoch 273 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.072\n",
      "Epoch 274 (0.16 sec): loss changed from 0.073 to 0.072\t\t0.072\n",
      "Epoch 275 (0.16 sec): loss changed from 0.069 to 0.067\t\t0.072\n",
      "Epoch 276 (0.15 sec): loss changed from 0.071 to 0.070\t\t0.072\n",
      "Epoch 277 (0.16 sec): loss changed from 0.076 to 0.074\t\t0.072\n",
      "Epoch 278 (0.15 sec): loss changed from 0.077 to 0.075\t\t0.072\n",
      "Epoch 279 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.072\n",
      "Epoch 280 (0.15 sec): loss changed from 0.076 to 0.074\t\t0.072\n",
      "Epoch 281 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.072\n",
      "Epoch 282 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.072\n",
      "Epoch 283 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 284 (0.15 sec): loss changed from 0.069 to 0.068\t\t0.072\n",
      "Epoch 285 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.072\n",
      "Epoch 286 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.072\n",
      "Epoch 287 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.071\n",
      "Epoch 288 (0.15 sec): loss changed from 0.069 to 0.068\t\t0.071\n",
      "Epoch 289 (0.16 sec): loss changed from 0.072 to 0.071\t\t0.071\n",
      "Epoch 290 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.071\n",
      "Epoch 291 (0.16 sec): loss changed from 0.072 to 0.071\t\t0.071\n",
      "Epoch 292 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.071\n",
      "Epoch 293 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 294 (0.16 sec): loss changed from 0.069 to 0.068\t\t0.071\n",
      "Epoch 295 (0.17 sec): loss changed from 0.074 to 0.072\t\t0.071\n",
      "Epoch 296 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.071\n",
      "Epoch 297 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.071\n",
      "Epoch 298 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.071\n",
      "Epoch 299 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 300 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.071\n",
      "Epoch 301 (0.17 sec): loss changed from 0.073 to 0.071\t\t0.071\n",
      "Epoch 302 (0.17 sec): loss changed from 0.072 to 0.071\t\t0.071\n",
      "Epoch 303 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 304 (0.15 sec): loss changed from 0.067 to 0.066\t\t0.071\n",
      "Epoch 305 (0.16 sec): loss changed from 0.074 to 0.072\t\t0.071\n",
      "Epoch 306 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.071\n",
      "Epoch 307 (0.16 sec): loss changed from 0.069 to 0.067\t\t0.071\n",
      "Epoch 308 (0.18 sec): loss changed from 0.068 to 0.066\t\t0.071\n",
      "Epoch 309 (0.19 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 310 (0.20 sec): loss changed from 0.068 to 0.066\t\t0.071\n",
      "Epoch 311 (0.19 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 312 (0.19 sec): loss changed from 0.070 to 0.068\t\t0.071\n",
      "Epoch 313 (0.19 sec): loss changed from 0.069 to 0.068\t\t0.071\n",
      "Epoch 314 (0.19 sec): loss changed from 0.070 to 0.069\t\t0.071\n",
      "Epoch 315 (0.19 sec): loss changed from 0.067 to 0.065\t\t0.071\n",
      "Epoch 316 (0.19 sec): loss changed from 0.068 to 0.066\t\t0.071\n",
      "Epoch 317 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.071\n",
      "Epoch 318 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.071\n",
      "Epoch 319 (0.15 sec): loss changed from 0.075 to 0.073\t\t0.070\n",
      "Epoch 320 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.070\n",
      "Epoch 321 (0.15 sec): loss changed from 0.069 to 0.068\t\t0.071\n",
      "Epoch 322 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 323 (0.15 sec): loss changed from 0.067 to 0.065\t\t0.070\n",
      "Epoch 324 (0.16 sec): loss changed from 0.065 to 0.063\t\t0.070\n",
      "Epoch 325 (0.15 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 326 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.070\n",
      "Epoch 327 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 328 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.070\n",
      "Epoch 329 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.070\n",
      "Epoch 330 (0.16 sec): loss changed from 0.070 to 0.069\t\t0.070\n",
      "Epoch 331 (0.15 sec): loss changed from 0.070 to 0.069\t\t0.070\n",
      "Epoch 332 (0.19 sec): loss changed from 0.068 to 0.067\t\t0.070\n",
      "Epoch 333 (0.18 sec): loss changed from 0.068 to 0.066\t\t0.070\n",
      "Epoch 334 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 335 (0.18 sec): loss changed from 0.068 to 0.067\t\t0.070\n",
      "Epoch 336 (0.16 sec): loss changed from 0.069 to 0.068\t\t0.070\n",
      "Epoch 337 (0.16 sec): loss changed from 0.072 to 0.071\t\t0.070\n",
      "Epoch 338 (0.17 sec): loss changed from 0.067 to 0.066\t\t0.070\n",
      "Epoch 339 (0.16 sec): loss changed from 0.073 to 0.072\t\t0.070\n",
      "Epoch 340 (0.20 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 341 (0.23 sec): loss changed from 0.074 to 0.072\t\t0.070\n",
      "Epoch 342 (0.24 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 343 (0.17 sec): loss changed from 0.074 to 0.073\t\t0.070\n",
      "Epoch 344 (0.18 sec): loss changed from 0.073 to 0.071\t\t0.070\n",
      "Epoch 345 (0.17 sec): loss changed from 0.075 to 0.073\t\t0.070\n",
      "Epoch 346 (0.17 sec): loss changed from 0.067 to 0.065\t\t0.070\n",
      "Epoch 347 (0.18 sec): loss changed from 0.069 to 0.068\t\t0.070\n",
      "Epoch 348 (0.16 sec): loss changed from 0.072 to 0.071\t\t0.070\n",
      "Epoch 349 (0.18 sec): loss changed from 0.069 to 0.067\t\t0.070\n",
      "Epoch 350 (0.17 sec): loss changed from 0.070 to 0.069\t\t0.070\n",
      "Epoch 351 (0.18 sec): loss changed from 0.075 to 0.073\t\t0.070\n",
      "Epoch 352 (0.18 sec): loss changed from 0.069 to 0.068\t\t0.070\n",
      "Epoch 353 (0.17 sec): loss changed from 0.068 to 0.066\t\t0.070\n",
      "Epoch 354 (0.17 sec): loss changed from 0.069 to 0.067\t\t0.070\n",
      "Epoch 355 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 356 (0.16 sec): loss changed from 0.067 to 0.066\t\t0.070\n",
      "Epoch 357 (0.18 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 358 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.070\n",
      "Epoch 359 (0.17 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 360 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 361 (0.16 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 362 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 363 (0.16 sec): loss changed from 0.071 to 0.070\t\t0.070\n",
      "Epoch 364 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.070\n",
      "Epoch 365 (0.17 sec): loss changed from 0.067 to 0.066\t\t0.070\n",
      "Epoch 366 (0.15 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 367 (0.17 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 368 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 369 (0.16 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 370 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 371 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.069\n",
      "Epoch 372 (0.15 sec): loss changed from 0.069 to 0.068\t\t0.069\n",
      "Epoch 373 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.069\n",
      "Epoch 374 (0.15 sec): loss changed from 0.070 to 0.068\t\t0.069\n",
      "Epoch 375 (0.17 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 376 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.069\n",
      "Epoch 377 (0.17 sec): loss changed from 0.074 to 0.072\t\t0.069\n",
      "Epoch 378 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 379 (0.15 sec): loss changed from 0.072 to 0.070\t\t0.069\n",
      "Epoch 380 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.069\n",
      "Epoch 381 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 382 (0.17 sec): loss changed from 0.072 to 0.070\t\t0.069\n",
      "Epoch 383 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 384 (0.16 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 385 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 386 (0.17 sec): loss changed from 0.066 to 0.064\t\t0.069\n",
      "Epoch 387 (0.15 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 388 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.069\n",
      "Epoch 389 (0.15 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 390 (0.16 sec): loss changed from 0.065 to 0.064\t\t0.069\n",
      "Epoch 391 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.069\n",
      "Epoch 392 (0.19 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 393 (0.17 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 394 (0.18 sec): loss changed from 0.066 to 0.064\t\t0.069\n",
      "Epoch 395 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.069\n",
      "Epoch 396 (0.23 sec): loss changed from 0.070 to 0.068\t\t0.069\n",
      "Epoch 397 (0.18 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 398 (0.17 sec): loss changed from 0.063 to 0.062\t\t0.069\n",
      "Epoch 399 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400 (0.17 sec): loss changed from 0.066 to 0.065\t\t0.069\n",
      "Epoch 401 (0.15 sec): loss changed from 0.064 to 0.062\t\t0.069\n",
      "Epoch 402 (0.16 sec): loss changed from 0.064 to 0.062\t\t0.069\n",
      "Epoch 403 (0.18 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 404 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.069\n",
      "Epoch 405 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.069\n",
      "Epoch 406 (0.17 sec): loss changed from 0.068 to 0.066\t\t0.069\n",
      "Epoch 407 (0.24 sec): loss changed from 0.068 to 0.066\t\t0.069\n",
      "Epoch 408 (0.23 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 409 (0.25 sec): loss changed from 0.070 to 0.068\t\t0.069\n",
      "Epoch 410 (0.20 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 411 (0.19 sec): loss changed from 0.071 to 0.069\t\t0.069\n",
      "Epoch 412 (0.19 sec): loss changed from 0.069 to 0.067\t\t0.069\n",
      "Epoch 413 (0.19 sec): loss changed from 0.071 to 0.069\t\t0.069\n",
      "Epoch 414 (0.19 sec): loss changed from 0.065 to 0.063\t\t0.069\n",
      "Epoch 415 (0.19 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 416 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 417 (0.15 sec): loss changed from 0.073 to 0.071\t\t0.069\n",
      "Epoch 418 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.068\n",
      "Epoch 419 (0.15 sec): loss changed from 0.065 to 0.064\t\t0.068\n",
      "Epoch 420 (0.17 sec): loss changed from 0.067 to 0.065\t\t0.069\n",
      "Epoch 421 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 422 (0.16 sec): loss changed from 0.068 to 0.067\t\t0.068\n",
      "Epoch 423 (0.15 sec): loss changed from 0.066 to 0.065\t\t0.069\n",
      "Epoch 424 (0.16 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 425 (0.15 sec): loss changed from 0.070 to 0.068\t\t0.068\n",
      "Epoch 426 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.068\n",
      "Epoch 427 (0.15 sec): loss changed from 0.068 to 0.067\t\t0.068\n",
      "Epoch 428 (0.16 sec): loss changed from 0.073 to 0.071\t\t0.068\n",
      "Epoch 429 (0.15 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 430 (0.17 sec): loss changed from 0.072 to 0.070\t\t0.068\n",
      "Epoch 431 (0.15 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 432 (0.16 sec): loss changed from 0.070 to 0.069\t\t0.068\n",
      "Epoch 433 (0.15 sec): loss changed from 0.071 to 0.070\t\t0.068\n",
      "Epoch 434 (0.16 sec): loss changed from 0.065 to 0.064\t\t0.068\n",
      "Epoch 435 (0.15 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 436 (0.17 sec): loss changed from 0.068 to 0.067\t\t0.068\n",
      "Epoch 437 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 438 (0.16 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 439 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 440 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 441 (0.15 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 442 (0.16 sec): loss changed from 0.070 to 0.069\t\t0.068\n",
      "Epoch 443 (0.15 sec): loss changed from 0.065 to 0.063\t\t0.068\n",
      "Epoch 444 (0.16 sec): loss changed from 0.066 to 0.065\t\t0.068\n",
      "Epoch 445 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 446 (0.19 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 447 (0.18 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 448 (0.17 sec): loss changed from 0.061 to 0.060\t\t0.068\n",
      "Epoch 449 (0.17 sec): loss changed from 0.071 to 0.069\t\t0.068\n",
      "Epoch 450 (0.17 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 451 (0.23 sec): loss changed from 0.064 to 0.062\t\t0.068\n",
      "Epoch 452 (0.18 sec): loss changed from 0.064 to 0.062\t\t0.068\n",
      "Epoch 453 (0.18 sec): loss changed from 0.071 to 0.069\t\t0.068\n",
      "Epoch 454 (0.17 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 455 (0.18 sec): loss changed from 0.072 to 0.071\t\t0.068\n",
      "Epoch 456 (0.15 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 457 (0.18 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 458 (0.15 sec): loss changed from 0.070 to 0.068\t\t0.068\n",
      "Epoch 459 (0.17 sec): loss changed from 0.070 to 0.068\t\t0.068\n",
      "Epoch 460 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 461 (0.16 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 462 (0.15 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 463 (0.16 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 464 (0.15 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 465 (0.17 sec): loss changed from 0.067 to 0.065\t\t0.068\n",
      "Epoch 466 (0.15 sec): loss changed from 0.065 to 0.064\t\t0.068\n",
      "Epoch 467 (0.17 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 468 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 469 (0.16 sec): loss changed from 0.065 to 0.064\t\t0.068\n",
      "Epoch 470 (0.15 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 471 (0.17 sec): loss changed from 0.065 to 0.064\t\t0.068\n",
      "Epoch 472 (0.15 sec): loss changed from 0.064 to 0.063\t\t0.068\n",
      "Epoch 473 (0.16 sec): loss changed from 0.069 to 0.068\t\t0.068\n",
      "Epoch 474 (0.15 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 475 (0.16 sec): loss changed from 0.071 to 0.069\t\t0.068\n",
      "Epoch 476 (0.15 sec): loss changed from 0.069 to 0.068\t\t0.068\n",
      "Epoch 477 (0.20 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 478 (0.18 sec): loss changed from 0.067 to 0.066\t\t0.068\n",
      "Epoch 479 (0.16 sec): loss changed from 0.070 to 0.068\t\t0.067\n",
      "Epoch 480 (0.18 sec): loss changed from 0.065 to 0.064\t\t0.067\n",
      "Epoch 481 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.068\n",
      "Epoch 482 (0.23 sec): loss changed from 0.065 to 0.063\t\t0.068\n",
      "Epoch 483 (0.17 sec): loss changed from 0.069 to 0.067\t\t0.067\n",
      "Epoch 484 (0.19 sec): loss changed from 0.063 to 0.061\t\t0.068\n",
      "Epoch 485 (0.16 sec): loss changed from 0.066 to 0.064\t\t0.067\n",
      "Epoch 486 (0.16 sec): loss changed from 0.065 to 0.063\t\t0.067\n",
      "Epoch 487 (0.17 sec): loss changed from 0.065 to 0.063\t\t0.067\n",
      "Epoch 488 (0.16 sec): loss changed from 0.062 to 0.061\t\t0.067\n",
      "Epoch 489 (0.16 sec): loss changed from 0.071 to 0.070\t\t0.067\n",
      "Epoch 490 (0.16 sec): loss changed from 0.069 to 0.067\t\t0.068\n",
      "Epoch 491 (0.15 sec): loss changed from 0.070 to 0.068\t\t0.067\n",
      "Epoch 492 (0.17 sec): loss changed from 0.066 to 0.064\t\t0.067\n",
      "Epoch 493 (0.16 sec): loss changed from 0.068 to 0.066\t\t0.067\n",
      "Epoch 494 (0.18 sec): loss changed from 0.066 to 0.064\t\t0.068\n",
      "Epoch 495 (0.17 sec): loss changed from 0.065 to 0.063\t\t0.067\n",
      "Epoch 496 (0.16 sec): loss changed from 0.070 to 0.069\t\t0.067\n",
      "Epoch 497 (0.15 sec): loss changed from 0.063 to 0.062\t\t0.067\n",
      "Epoch 498 (0.16 sec): loss changed from 0.066 to 0.064\t\t0.067\n",
      "Epoch 499 (0.15 sec): loss changed from 0.068 to 0.066\t\t0.067\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 500\n",
    "num_step  = 10\n",
    "batch_size = 500\n",
    "\n",
    "\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_A', tf.get_default_graph())\n",
    "train_batch = {tfIn_PP: train_pp}\n",
    "valid_batch = {tfIn_PP: valid_pp}\n",
    "with tf.Session() as tfs:    \n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epoch):\n",
    "        mini_pp, mini_meta, mini_trgt = randomBatch((train_pp, train_meta, train_trgt), batch_size)\n",
    "        mini_batch = {tfIn_PP: mini_pp, tfIn_hrand: np.random.rand(mini_pp.shape[0], param_RBM_hid)}\n",
    "        \n",
    "        time0 = time.perf_counter()\n",
    "        loss0 = tfLoss.eval(feed_dict=mini_batch)\n",
    "        for j in range(num_step):\n",
    "            tfTrain.run(feed_dict=mini_batch)\n",
    "        loss1 = tfLoss.eval(feed_dict=mini_batch)\n",
    "        time1 = time.perf_counter()\n",
    "        \n",
    "        valid_loss = tfLoss.eval(feed_dict=valid_batch)\n",
    "        #valid_loss_str = tfLossSummary.eval(feed_dict=valid_batch)\n",
    "        #tffw.add_summary(valid_loss_str, i)\n",
    "        print('Epoch {0} ({3:1.2f} sec): loss changed from {1:1.3f} to {2:1.3f}\\t\\t{4:1.3f}'.format(i, loss0, loss1, time1-time0,valid_loss))\n",
    "    train_dbnf = tfFeatures.eval(feed_dict=train_batch)\n",
    "    valid_dbnf = tfFeatures.eval(feed_dict=valid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg1 = LogisticRegression().fit(train_dbnf, train_y)\n",
    "train_nn = logreg1.predict_proba(train_dbnf)[:,1]\n",
    "valid_nn = logreg1.predict_proba(valid_dbnf)[:,1]\n",
    "\n",
    "vsmpl = valid_sample.copy()\n",
    "vsmpl['nnp'] = valid_nn\n",
    "vsmpl['lrp'] = valid_p\n",
    "vsmpl = vsmpl[['accnt_id','trgt','prob','nnp','lrp', 'pp0','pp0t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83261771341241553"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.prob, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69988912528842384"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.nnp, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70034717911617017"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(np.array(vsmpl.trgt, dtype=np.float32), np.array(vsmpl.lrp, dtype=np.float32))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = {a:a+'-' for a in 'xyz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.41255005e-08,   2.24236214e-06,   9.99820173e-01, ...,\n",
       "          1.65283825e-06,   7.56276535e-08,   9.98151362e-01],\n",
       "       [  3.54402630e-07,   1.12323498e-04,   7.45301776e-08, ...,\n",
       "          3.61397746e-04,   7.63305507e-05,   5.18086134e-04],\n",
       "       [  6.03610943e-07,   4.99429552e-05,   6.28965964e-08, ...,\n",
       "          3.13086173e-04,   2.11163133e-04,   1.00003462e-03],\n",
       "       ..., \n",
       "       [  1.10576241e-06,   8.29852434e-05,   8.05648170e-08, ...,\n",
       "          3.61195911e-04,   5.51577832e-04,   1.13630970e-03],\n",
       "       [  6.62167281e-07,   6.95560593e-05,   1.22061323e-07, ...,\n",
       "          3.96315387e-04,   4.25364822e-04,   1.07475719e-03],\n",
       "       [  1.05501829e-06,   1.67262813e-04,   8.49903427e-06, ...,\n",
       "          1.10150264e-04,   8.94813274e-05,   4.08689346e-04]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dbnf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
