{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import modutils\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "w2v_src_file = '../DataSets/Quora/w2v_src_180115.pickle'\n",
    "w2v_model = '../Models-23Quora03-W2V/model-02.ckpt'\n",
    "w2v_size = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recode_max_dict(sentences, full_dict, dict_size):\n",
    "    last_ind = dict_size - 1\n",
    "    new_dict = full_dict[:last_ind]\n",
    "    new_num = sum([x[1] for x in full_dict[last_ind:]])\n",
    "    new_freq = sum([x[2] for x in full_dict[last_ind:]])\n",
    "    new_dict.append(('<UNK>', new_num, new_freq, 1))\n",
    "    \n",
    "    new_sentences = [[min(last_ind, z) for z in x] for x in sentences]\n",
    "    return (new_sentences, new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(w2v_src_file, 'rb') as f:\n",
    "    (full_dict, full_sentences) = pickle.load(f)\n",
    "    \n",
    "(w2v_src, w2v_dict) = recode_max_dict(full_sentences, full_dict, dict_size=w2v_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load state    \n",
    "mapper = {x[0]:i for (i,x) in enumerate(w2v_dict)}\n",
    "\n",
    "def word2idx(w):\n",
    "    if w in mapper:\n",
    "        return mapper[w]\n",
    "    else:\n",
    "        return mapper['<UNK>']\n",
    "    \n",
    "def idx2word(i):\n",
    "    if type(i) is list:\n",
    "        return [idx2word(x) for x in i]\n",
    "    if type(i) is np.ndarray:\n",
    "        return np.array([idx2word(x) for x in i])\n",
    "    if i >= len(w2v_dict):\n",
    "        return '<ERR>'\n",
    "    return w2v_dict[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete.\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = len(w2v_dict)\n",
    "EMBED_SIZE = 200\n",
    "NCE_NUM_SAMPLED = 100\n",
    "\n",
    "init_embeding = np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE)/np.sqrt(EMBED_SIZE)\n",
    "init_beta = np.random.multivariate_normal(np.zeros(EMBED_SIZE), np.identity(EMBED_SIZE), size=DICT_SIZE)/np.sqrt(EMBED_SIZE)\n",
    "init_intercept = np.zeros((DICT_SIZE,))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    tf_in_word = tf.placeholder(tf.int32, shape=(None, ), name='in_word')\n",
    "    tf_in_context = tf.placeholder(tf.int32, shape=(None, 1), name='in_context')\n",
    "    tf_in_regularization = tf.placeholder_with_default(0.1, shape=(), name='in_regularization')\n",
    "    \n",
    "with tf.name_scope('Embedding'):\n",
    "    tf_embedding = tf.Variable(init_embeding, dtype=tf.float32)\n",
    "    tf_embedded_word = tf.nn.embedding_lookup(tf_embedding, tf_in_word, name='out_embedding')\n",
    "    \n",
    "with tf.name_scope('Training'):\n",
    "    tf_nce_beta = tf.Variable(init_beta, dtype=tf.float32)\n",
    "    tf_nce_intercept = tf.Variable(init_intercept, dtype=tf.float32)\n",
    "    tf_nce_loss = tf.reduce_mean(\n",
    "                    tf.nn.nce_loss(weights=tf_nce_beta, biases=tf_nce_intercept,\n",
    "                                   labels=tf_in_context, inputs=tf_embedded_word,\n",
    "                                   num_sampled=NCE_NUM_SAMPLED, num_classes=DICT_SIZE))\n",
    "    #tf_reg_loss = tf.sqrt(tf.reduce_mean(tf.square(tf_embedding))) #bad loss\n",
    "    tf_reg_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.reduce_mean(tf_embedding, axis=0)))) #center of embedding is 0\n",
    "    tf_full_loss = tf_nce_loss + tf_in_regularization * tf_reg_loss\n",
    "    tf_train = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(tf_full_loss)\n",
    "    \n",
    "with tf.name_scope('Validation'):\n",
    "    tf_valid_dictionary = tf.constant(np.array(range(DICT_SIZE)))\n",
    "    tf_valid_embedding = tf.nn.embedding_lookup(tf_embedding, tf_valid_dictionary)\n",
    "    tf_valid_in_norm = tf_embedded_word / tf.sqrt(tf.reduce_sum(tf.square(tf_embedded_word), 1, keep_dims=True))\n",
    "    tf_valid_dic_norm = tf_valid_embedding / tf.sqrt(tf.reduce_sum(tf.square(tf_valid_embedding), 1, keep_dims=True))\n",
    "    tf_valid_similarity = tf.matmul(tf_valid_in_norm, tf_valid_dic_norm, transpose_b=True)\n",
    "    \n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_W2V', tf.get_default_graph())\n",
    "tffw.close()\n",
    "print('Graph creation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models-23Quora03-W2V/model-02.ckpt\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "tfsSaver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfsSaver.restore(tfs, save_path=w2v_model)\n",
    "    dic_embed = tf_valid_dic_norm.eval()\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(wrd, embed=dic_embed):\n",
    "    if type(wrd) is str:\n",
    "        return embed[word2idx(wrd)]\n",
    "    if type(wrd) is list:\n",
    "        return [word2vec(x) for x in wrd]\n",
    "    if type(wrd) is np.ndarray:\n",
    "        return [word2vec(x) for x in wrd]\n",
    "    return None\n",
    "\n",
    "def topNids(vec, embed=dic_embed):\n",
    "    dists = np.sqrt(np.sum(np.square(embed - vec), axis=1))\n",
    "    dord = np.argsort(dists)\n",
    "    return (dord, dists[dord], np.mean(dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_embed = dic_embed - dic_embed.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['soviet', 'european', 'mughal', 'poorest', 'pacific', 'higgs',\n",
       "        'advising', 'assad', 'territory', 'territories'],\n",
       "       dtype='<U11'),\n",
       " array([ 0.        ,  0.89992267,  1.01261008,  1.05298018,  1.05413282,\n",
       "         1.05812311,  1.07490003,  1.0761683 ,  1.07674336,  1.080948  ], dtype=float32),\n",
       " 1.4122971)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = word2vec('soviet', n2_embed)\n",
    "print(np.sqrt(np.sum(np.square(v))))\n",
    "res = topNids(v, n2_embed)\n",
    "idx2word(res[0][:10]), res[1][:10], res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2_embed = n_embed / np.sqrt(np.square(n_embed).sum(axis=1)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  0.99999994,  1.        ,  1.        ,  1.        ], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(n2_embed).sum(axis=1))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.37659877e-03,  -4.40228125e-03,  -2.23844755e-03,\n",
       "         4.76453773e-04,   1.52279297e-03,   3.10367695e-03,\n",
       "        -2.55712937e-03,   1.09814538e-03,  -2.45685340e-03,\n",
       "        -3.73511412e-03,   2.26351642e-03,  -2.82515213e-03,\n",
       "        -3.66856391e-03,  -2.72426102e-03,   1.96540123e-03,\n",
       "         5.08765597e-03,  -5.08268224e-03,   4.70175175e-03,\n",
       "        -3.55271739e-03,   4.73660091e-03,  -3.58514371e-03,\n",
       "        -4.53447457e-03,  -2.23767781e-03,   2.57108849e-03,\n",
       "         3.45585262e-03,   3.27310129e-03,  -1.96859823e-03,\n",
       "         6.29883260e-03,  -2.89325556e-03,   2.72309408e-03,\n",
       "         1.84798578e-03,  -2.38471874e-03,  -2.21907021e-03,\n",
       "        -1.90203602e-03,  -2.57443637e-03,   3.49301449e-03,\n",
       "        -3.74454027e-03,   5.36353746e-03,   4.13659308e-03,\n",
       "        -4.01344988e-03,   3.20694945e-03,  -3.79665382e-03,\n",
       "         3.02152196e-03,  -7.51617015e-04,   3.42757511e-03,\n",
       "         9.72645139e-05,  -4.43768129e-03,  -3.93606722e-03,\n",
       "         3.67681892e-03,   2.23966921e-03,   2.41478160e-03,\n",
       "        -8.99573555e-04,   5.59732597e-03,   1.52863178e-03,\n",
       "         2.42139492e-03,  -2.77981535e-03,  -3.29810497e-03,\n",
       "         3.05861398e-03,  -1.82798656e-03,  -1.15747716e-04,\n",
       "         1.86458055e-03,   2.26670085e-03,  -8.89383722e-03,\n",
       "        -1.49917998e-03,  -3.78760905e-03,   4.17716615e-03,\n",
       "        -2.15557031e-03,  -2.79417355e-03,  -4.01003752e-03,\n",
       "         3.77277588e-03,  -1.96214649e-03,  -3.28076142e-03,\n",
       "         2.18415889e-03,   3.46652837e-03,   3.25936638e-03,\n",
       "        -2.80930568e-03,  -1.85646687e-03,  -3.33557348e-03,\n",
       "         3.93945212e-03,  -3.81031493e-03,  -2.62861094e-03,\n",
       "         3.93380504e-03,  -3.63165443e-03,  -5.29781566e-04,\n",
       "         3.27689084e-03,   3.45808268e-03,   3.06895142e-03,\n",
       "         2.04198156e-03,  -1.08520722e-03,  -2.28210865e-03,\n",
       "        -4.36632289e-03,   2.56979419e-03,  -3.37066245e-03,\n",
       "        -2.43830541e-03,  -2.41766474e-03,  -2.18899129e-03,\n",
       "         5.20950044e-03,   2.53111683e-03,  -3.62723111e-03,\n",
       "         4.26996732e-03,  -1.70274160e-03,   4.60742321e-03,\n",
       "         3.99687840e-03,  -3.54109448e-03,   8.46728857e-04,\n",
       "        -3.83255887e-03,   2.75828713e-03,  -7.83616735e-04,\n",
       "         2.48443126e-03,  -2.06105667e-03,   8.96643440e-04,\n",
       "         4.08389978e-03,   2.47078761e-02,   2.57027731e-03,\n",
       "         4.36360016e-03,   3.94213293e-03,   2.67076446e-03,\n",
       "         3.15333577e-03,   4.01219400e-03,   2.93090008e-03,\n",
       "        -3.80727346e-03,   2.10002740e-03,  -2.62838416e-03,\n",
       "         1.17296539e-03,  -3.56129277e-03,  -4.42230841e-03,\n",
       "        -6.88449480e-04,  -2.95750820e-03,  -3.64029733e-03,\n",
       "         2.94289319e-03,  -3.42911319e-03,  -2.47968850e-03,\n",
       "        -3.79573612e-04,  -2.36301636e-03,   3.14172171e-03,\n",
       "         2.34533031e-03,  -2.99890060e-03,  -2.40453915e-03,\n",
       "         4.36603930e-03,   3.09162750e-03,  -3.02071800e-03,\n",
       "        -3.88895627e-03,  -1.43024093e-03,  -2.50519067e-03,\n",
       "        -1.60818791e-03,   3.57696018e-03,  -4.40555718e-03,\n",
       "        -1.49931549e-03,  -4.48596571e-03,   1.61834748e-03,\n",
       "         2.81321537e-03,   2.65482767e-03,  -4.21266444e-03,\n",
       "        -1.61782606e-03,   1.75566413e-03,  -4.74154670e-03,\n",
       "         2.87122559e-04,   3.23558459e-03,   1.67247897e-03,\n",
       "        -2.91963969e-03,  -2.07098830e-03,  -3.23785422e-03,\n",
       "         4.09189099e-03,   2.19699438e-03,  -2.44898023e-03,\n",
       "         2.32596090e-03,  -3.71260080e-03,  -1.81660592e-03,\n",
       "        -5.95734408e-03,  -2.10446259e-03,   4.32693865e-03,\n",
       "         2.44631246e-03,  -1.43965136e-03,   3.56242922e-03,\n",
       "        -3.59456078e-03,   4.58159251e-03,  -5.19725867e-03,\n",
       "        -2.30160705e-03,  -4.54744278e-03,   2.00872240e-03,\n",
       "        -2.59078364e-03,   1.22499128e-04,   2.92342901e-03,\n",
       "         3.76327604e-04,  -1.55925762e-03,  -3.09684174e-03,\n",
       "        -3.06237885e-03,  -3.40239634e-03,  -3.18630552e-03,\n",
       "        -1.51866593e-03,  -4.26316028e-03,  -2.26847525e-03,\n",
       "        -3.05157038e-03,  -3.58407910e-04,  -3.02769500e-03,\n",
       "        -2.39074952e-03,  -3.48933064e-03,   4.26550908e-03,\n",
       "         3.78778321e-03,  -2.74520344e-03], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2_embed.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
