{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_over_dirs(pth, fun, y):\n",
    "    for x in os.listdir(pth):\n",
    "        xf = pth + x\n",
    "        if os.path.isdir(xf):\n",
    "            y = iterate_over_dirs(xf + '/', fun, y)\n",
    "        elif os.path.isfile(xf):\n",
    "            y = fun(y, xf)\n",
    "    return y\n",
    "\n",
    "def sample_corpora(files, file_num=10):\n",
    "    ids = np.random.choice(len(files), size=file_num)\n",
    "    res = []\n",
    "    for i in ids:\n",
    "        with open(file=files[i], mode='rb') as f:\n",
    "            res += [f.read()]\n",
    "    return res\n",
    "\n",
    "def subsample_trunc_corpora(corpora, batch_num=100, batch_size=32):\n",
    "    data = np.random.choice(corpora, size=batch_num)\n",
    "    tmp = [(x, random.randint(0, len(x)-batch_size)) for x in data if len(x) >= batch_size]\n",
    "    return [x[i:(i+batch_size)] for (x,i) in tmp]\n",
    "    \n",
    "\n",
    "def gather_chars(files):\n",
    "    s = []\n",
    "    for fn in files:\n",
    "        with open(fn, 'rb') as f:\n",
    "            s = set(list(s) + [x for x in f.read()])\n",
    "    return list(s)\n",
    "\n",
    "def create_encoding(files, chars):\n",
    "    chars = gather_chars(files)\n",
    "    encoding = np.zeros((255, len(chars)), dtype=np.float32)\n",
    "    for i in range(len(chars)):\n",
    "        encoding[chars[i], i] = 1.0\n",
    "    return encoding\n",
    "\n",
    "def encode_file(file, encoding):\n",
    "    return np.array([encoding[int(i),:] for i in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = iterate_over_dirs('D:/Jupyter/Datasets/PaulGraham/', lambda y,x: y + [x], [])\n",
    "file_corpora = [x for x in all_files if os.path.splitext(x)[1] in ('.txt')]\n",
    "chars = gather_chars(file_corpora)\n",
    "encoding = create_encoding(file_corpora, chars)\n",
    "\n",
    "def next_batch(batch_num=50, batch_size=32, files_num=10):\n",
    "    texts = subsample_trunc_corpora(sample_corpora(all_files, files_num), batch_num, batch_size)\n",
    "    return np.array([encode_file(x, encoding) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_basic_rnn_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    x_one_hot = tf.one_hot(x, num_classes)\n",
    "    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(1, num_steps, x_one_hot)]\n",
    "\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytf]",
   "language": "python",
   "name": "conda-env-pytf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
