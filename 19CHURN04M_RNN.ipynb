{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time\n",
    "import sklearn, sklearn.metrics\n",
    "\n",
    "import modutils\n",
    "\n",
    "data_dir = '../DataSets/Churn/'\n",
    "logm_fmt = data_dir + 'user_logs/uldtm_{0}.csv'\n",
    "used_log = '201701'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src0 = pd.read_csv(logm_fmt.format(used_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['used', 'ln_tot', 'ln_sec', 'pct_low', 'pct_100', 'pct_unq', 'avg_sec_n']\n",
    "target = ['nxt_used']\n",
    "devX = src0[src0.date < 20170131][features].values.reshape(-1, 30, len(features))\n",
    "devY = src0.nxt_used[src0.date < 20170131].values.reshape(-1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (validX, validY), (testX, testY) = modutils.splitSample((devX, devY), pcts=[0.2, 0.2, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation complete\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 30\n",
    "SEQ_FEATURES = len(features)\n",
    "RNN_SIZE = [24, 12]\n",
    "HIDDEN_LAYER = 10\n",
    "\n",
    "RCell = lambda n: tf.nn.rnn_cell.GRUCell(num_units=n, activation=tf.nn.elu)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "rnnCell = tf.nn.rnn_cell.MultiRNNCell([RCell(s) for s in RNN_SIZE], state_is_tuple=True)\n",
    "\n",
    "with tf.name_scope(name='INPUT'):\n",
    "    tfi_x = tf.placeholder(shape=(None, SEQ_LENGTH, SEQ_FEATURES), dtype=tf.float32)\n",
    "    tfi_y = tf.placeholder(shape=(None, SEQ_LENGTH), dtype=tf.int32)\n",
    "    \n",
    "    tfX = tfi_x\n",
    "    tfY = tf.one_hot(tfi_y, 2, dtype=tf.float32)\n",
    "    tfActual = tf.cast(tfi_y, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(name='RNN'):\n",
    "    tfRNN_Hist, _ = tf.nn.dynamic_rnn(rnnCell, inputs=tfX, dtype=tf.float32, time_major=False)\n",
    "    \n",
    "    tfRNN_HistHid = tf.layers.dense(tfRNN_Hist, HIDDEN_LAYER, activation=tf.nn.elu)\n",
    "    tfRNN_HistRes = tf.layers.dense(tfRNN_HistHid, 2)\n",
    "\n",
    "with tf.name_scope(name='LOSS'):\n",
    "    tfLoss0 = tf.nn.softmax_cross_entropy_with_logits(labels=tfY[:,10:,:], logits=tfRNN_HistRes[:,10:,:])\n",
    "    tfLoss = tf.reduce_mean(tfLoss0)\n",
    "    tfTrain = tf.train.AdamOptimizer(1e-3).minimize(tfLoss)\n",
    "\n",
    "with tf.name_scope(name='OUTPUT'):\n",
    "    #tfPredicted = tf.argmax(tfRNN_HistRes, axis=2)\n",
    "    tfProbability = tf.nn.softmax(tfRNN_HistRes)[:,:,1]\n",
    "    tfLogOdds = tf.log(tfProbability / (1-tfProbability))\n",
    "\n",
    "tfsLoss = tf.summary.scalar('CrossEntropy', tfLoss)\n",
    "\n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/00_A', tf.get_default_graph())\n",
    "print('Graph creation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (86.0 sec): \t\tVL:0.448\t\tGINI:0.725ec\n",
      "Epoch 1 (86.7 sec): \t\tVL:0.446\t\tGINI:0.733ec\n",
      "Epoch 2 (86.1 sec): \t\tVL:0.445\t\tGINI:0.737ec\n",
      "Epoch 3 (86.3 sec): \t\tVL:0.445\t\tGINI:0.735ec\n",
      "Epoch 4 (86.1 sec): \t\tVL:0.446\t\tGINI:0.736ec\n",
      "Epoch 5 (86.2 sec): \t\tVL:0.444\t\tGINI:0.737ec\n",
      "Epoch 6 (87.6 sec): \t\tVL:0.444\t\tGINI:0.741ec\n",
      "Epoch 7 (88.1 sec): \t\tVL:0.444\t\tGINI:0.740ec\n",
      "Epoch 8 (88.1 sec): \t\tVL:0.444\t\tGINI:0.741ec\n",
      "Epoch 9 (88.5 sec): \t\tVL:0.443\t\tGINI:0.739ec\n",
      "Epoch 10 (89.4 sec): \t\tVL:0.443\t\tGINI:0.741c\n",
      "Epoch 11 (88.1 sec): \t\tVL:0.445\t\tGINI:0.740c\n",
      "Epoch 12 (88.1 sec): \t\tVL:0.442\t\tGINI:0.740c\n",
      "Epoch 13 (88.5 sec): \t\tVL:0.442\t\tGINI:0.741c\n",
      "Train-step 4/228: 0.419->0.419 in 0.33 sec\r"
     ]
    }
   ],
   "source": [
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "tffw = tf.summary.FileWriter('D:/Jupyter/Logs/19CHURN04-RNN-{0}'.format(dt_now), tf.get_default_graph())\n",
    "\n",
    "batch_size = 1000\n",
    "num_steps  = 3\n",
    "num_epochs = 100\n",
    "checkpoints = 500 #every %x% epochs save the model\n",
    "\n",
    "fmtstr = 'Epoch {0} ({1:1.3} sec): \\t\\tVL:{2:1.3f}\\t\\tGINI:{3:1.3f}'\n",
    "valid_batch = {tfi_x: validX, tfi_y: validY}\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        te0 = time.perf_counter()\n",
    "        counter0 = 0\n",
    "        num0 = trainY.shape[0] // batch_size\n",
    "        if num0 * batch_size < trainY.shape[0]:\n",
    "            num0 += 1\n",
    "        for (mini_x, mini_y) in modutils.shuffleBatches((trainX, trainY), batchSize=batch_size):\n",
    "            train_batch = {tfi_x:mini_x, tfi_y: mini_y}\n",
    "            \n",
    "            l0 = tfLoss.eval(feed_dict=train_batch)\n",
    "            t0 = time.perf_counter()\n",
    "            for j in range(num_steps):\n",
    "                tfTrain.run(feed_dict=train_batch)\n",
    "            t1 = time.perf_counter()\n",
    "            l1 = tfLoss.eval(feed_dict=train_batch)\n",
    "            counter0 += 1\n",
    "            print('Train-step {3}/{4}: {0:.3f}->{1:.3f} in {2:.2f} sec'.format(l0, l1, t1-t0, counter0, num0), end='\\r')\n",
    "    \n",
    "        te1 = time.perf_counter()\n",
    "        lv = tfLoss.eval(feed_dict=valid_batch)\n",
    "        valid_p = tfProbability.eval(feed_dict=valid_batch)[:,-1]\n",
    "        gn = sklearn.metrics.roc_auc_score(validY[:,-1], valid_p) * 2 -1\n",
    "        if i%checkpoints == 0 and i > 0:\n",
    "            p = tfsSaver.save(tfs, 'D:/Jupyter/mltest/Models-13RNN02/model-{0:02d}.ckpt'.format(i))\n",
    "            print('Model saved at checkpoint: {0}'.format(p))\n",
    "                             \n",
    "        print(fmtstr.format(i,te1-te0,lv,gn))\n",
    "    #valid_r = tfs.run(tfOutR, feed_dict=valid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
