{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sample_v0(length=-1):\n",
    "    seq_length = length\n",
    "    if seq_length == -1:\n",
    "        seq_length = np.random.poisson(16)\n",
    "    x = [np.random.choice([0,1,2],p=[0.4,0.4,0.2]) for i in range(seq_length)]\n",
    "    y = []\n",
    "    ps = []\n",
    "    for i in range(len(x)):\n",
    "        xscore = 0\n",
    "        yscore = 0\n",
    "        if x[i]==0:\n",
    "            xscore += 5\n",
    "        if x[i]==1:\n",
    "            yscore += 5\n",
    "        if i-3 >= 0 and x[i-3]==2:\n",
    "            xscore += 10\n",
    "            \n",
    "        p = np.exp(xscore) / (np.exp(xscore) + np.exp(yscore))\n",
    "        ps.append(p)\n",
    "        y.append(np.random.choice([0, 1], p=[p,1-p]))\n",
    "    return x,y\n",
    "\n",
    "def gen_sample_v1(length=-1):\n",
    "    seq_length = length\n",
    "    if seq_length == -1:\n",
    "        seq_length = np.random.poisson(16)\n",
    "    x = [np.random.choice([0,1,2],p=[0.4,0.4,0.2]) for i in range(seq_length)]\n",
    "    y = []\n",
    "    ps = []\n",
    "    xscore = 0\n",
    "    yscore = 0\n",
    "    for i in range(len(x)):        \n",
    "        if x[i]==0:\n",
    "            xscore += 3\n",
    "        if x[i]==1:\n",
    "            yscore += 3\n",
    "        if i-3 >= 0 and x[i-3]==2:\n",
    "            if x[i-1] == 1:\n",
    "                yscore = 0\n",
    "            if x[i-1] == 0:\n",
    "                xscore = 0\n",
    "        p = np.exp(xscore) / (np.exp(xscore) + np.exp(yscore))\n",
    "        ps.append(p)\n",
    "        y.append(np.random.choice([0, 1], p=[1-p, p]))\n",
    "    return x,y,ps\n",
    "\n",
    "\n",
    "def gen_sample_v2(length=-1, xp=-1, yp=-1):\n",
    "    seq_length = length\n",
    "    xpls, ypls = xp, yp\n",
    "    if seq_length == -1:\n",
    "        seq_length = np.random.poisson(16)\n",
    "    if xpls == -1:\n",
    "        xpls = np.exp(np.random.normal())\n",
    "    if ypls == -1:\n",
    "        ypls = np.exp(np.random.normal())\n",
    "    x = [np.random.choice([0,1,2],p=[0.4,0.4,0.2]) for i in range(seq_length)]\n",
    "    y = []\n",
    "    ps = []\n",
    "    xscore = 0\n",
    "    yscore = 0\n",
    "    for i in range(len(x)):        \n",
    "        if x[i]==0:\n",
    "            xscore += xpls\n",
    "        if x[i]==1:\n",
    "            yscore += ypls\n",
    "        if i-3 >= 0 and x[i-3]==2:\n",
    "            if x[i-1] == 1:\n",
    "                yscore = 0\n",
    "            if x[i-1] == 0:\n",
    "                xscore = 0\n",
    "        p = np.exp(xscore) / (np.exp(xscore) + np.exp(yscore))\n",
    "        ps.append(p)\n",
    "        y.append(np.random.choice([0, 1], p=[1-p, p]))\n",
    "    return x,y,ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, _ = zip(*[gen_sample_v1(32) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x26afbae9c18>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length = 16\n",
    "num_units = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_src_x = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_src_y = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_x = tf.one_hot(tf_src_x, 3)\n",
    "tf_y = tf.one_hot(tf_src_y, 2)\n",
    "\n",
    "tf_cell = tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "#tf_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(num_units=num_units, activation=tf.nn.relu) for _ in range(16)])\n",
    "\n",
    "tf_out, _ = tf.nn.dynamic_rnn(tf_cell, inputs=tf_x, dtype=tf.float32)\n",
    "tf_res = tf.layers.dense(tf_out, 2, use_bias=True)\n",
    "\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf_res, labels=tf_y))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y,axis=2), tf.argmax(tf_res,axis=2)), dtype=tf.float32))\n",
    "tf_prob = tf.nn.softmax(tf_res)\n",
    "tf_train = tf.train.AdamOptimizer(1e-2).minimize(tf_loss)\n",
    "\n",
    "tf.summary.FileWriter('D:/Jupyter/Logs/09A_SeqRNN_1', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train 0.686 -> 0.112, Validation: 0.1711123138666153\n",
      "Step 1: Train 0.147 -> 0.0876, Validation: 0.2341870218515396\n",
      "Step 2: Train 0.257 -> 0.102, Validation: 0.2326425164937973\n",
      "Step 3: Train 0.236 -> 0.104, Validation: 0.21845898032188416\n",
      "Step 4: Train 0.243 -> 0.121, Validation: 0.19225411117076874\n",
      "Step 5: Train 0.184 -> 0.104, Validation: 0.19973984360694885\n",
      "Step 6: Train 0.204 -> 0.11, Validation: 0.20690733194351196\n",
      "Step 7: Train 0.207 -> 0.109, Validation: 0.21080730855464935\n",
      "Step 8: Train 0.223 -> 0.119, Validation: 0.2004842907190323\n",
      "Step 9: Train 0.19 -> 0.101, Validation: 0.20857974886894226\n",
      "Step 10: Train 0.189 -> 0.103, Validation: 0.20632463693618774\n",
      "Step 11: Train 0.201 -> 0.114, Validation: 0.19410723447799683\n",
      "Step 12: Train 0.164 -> 0.112, Validation: 0.17720364034175873\n",
      "Step 13: Train 0.181 -> 0.117, Validation: 0.1877427101135254\n",
      "Step 14: Train 0.176 -> 0.115, Validation: 0.186659038066864\n",
      "Step 15: Train 0.197 -> 0.128, Validation: 0.18094901740550995\n",
      "Step 16: Train 0.174 -> 0.107, Validation: 0.1941824108362198\n",
      "Step 17: Train 0.2 -> 0.117, Validation: 0.18573610484600067\n",
      "Step 18: Train 0.197 -> 0.114, Validation: 0.1939586102962494\n",
      "Step 19: Train 0.181 -> 0.118, Validation: 0.17784304916858673\n",
      "Final Cross-Entropy Loss: 0.1778\n",
      "Final Accuracy: 0.9047\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "num_epochs = 20\n",
    "seq_length = 16\n",
    "batch_size = 200\n",
    "valid_size = 1000\n",
    "\n",
    "\n",
    "valid_x, valid_y, valid_p = zip(*[gen_sample_v0(seq_length) for i in range(valid_size)])\n",
    "valid_batch = {tf_src_x: valid_x, tf_src_y: valid_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_x, train_y, _ = zip(*[gen_sample_v0(seq_length) for i in range(batch_size)])\n",
    "        train_batch = {tf_src_x: train_x, tf_src_y: train_y}\n",
    "\n",
    "        start_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        for j in range(num_steps):\n",
    "            tf_train.run(feed_dict=train_batch)\n",
    "        \n",
    "        end_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        valid_loss = tf_loss.eval(feed_dict=valid_batch)\n",
    "        \n",
    "        print('Step {0}: Train {1:1.3} -> {2:1.3}, Validation: {3}'.format(i, start_loss, end_loss, valid_loss))\n",
    "    final_p, final_loss, final_accuracy = sess.run([tf_prob, tf_loss, tf_accuracy], feed_dict=valid_batch)\n",
    "print('Final Cross-Entropy Loss: {0:1.4}\\nFinal Accuracy: {1:1.4}'.format(final_loss, final_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for V-0\n",
    "### BasicRNNCell\n",
    "1) 1 unit $\\rightarrow$ Loss = $0.3230$, Accuracy = $0.8501$\n",
    "\n",
    "2) 2 units $\\rightarrow$ Loss = $0.2982$, Accuracy = $0.8465$\n",
    "\n",
    "3) 3 units $\\rightarrow$ Loss = $0.1743$, Accuracy = $0.9051$\n",
    "\n",
    "4) 4 units $\\rightarrow$ Loss = $0.1536$, Accuracy = $0.9103$ (Almost instant convergence)\n",
    "\n",
    "5) 5 units $\\rightarrow$ Loss = $0.1490$, Accuracy = $0.9089$ (Almost instant convergence)\n",
    "\n",
    "6) 10 units $\\rightarrow$ Loss = $0.1681$, Accuracy = $0.9084$ (Almost instant convergence & overtraining)\n",
    "\n",
    "7) 30 units $\\rightarrow$ Loss = $0.2426$, Accuracy = $0.9071$ (No convergence, overtraining on train sample)\n",
    "\n",
    "### BasicLSTMCell\n",
    "1) 1 unit $\\rightarrow$ Loss = $0.2911$, Accuracy = $0.8503$\n",
    "\n",
    "2) 2 units $\\rightarrow$ Loss = $0.2168$, Accuracy = $0.8944$ (Slow convergence)\n",
    "\n",
    "3) 3 units $\\rightarrow$ Loss = $0.1515$, Accuracy = $0.9087$\n",
    "\n",
    "4) 4 units $\\rightarrow$ Loss = $0.1680$, Accuracy = $0.9134$ \n",
    "\n",
    "5) 5 units $\\rightarrow$ Loss = $0.1732$, Accuracy = $0.9100$ \n",
    "\n",
    "6) 10 units $\\rightarrow$ Loss = $0.1855$, Accuracy = $0.9113$ (Overtraining)\n",
    "\n",
    "### GRUCell\n",
    "1) 1 unit $\\rightarrow$ Loss = $0.3166$, Accuracy = $0.8268$\n",
    "\n",
    "2) 2 units $\\rightarrow$ Loss = $0.2756$, Accuracy = $0.8586$\n",
    "\n",
    "3) 3 units $\\rightarrow$ Loss = $0.1854$, Accuracy = $0.9062$\n",
    "\n",
    "4) 10 units $\\rightarrow$ Loss = $0.1778$, Accuracy = $0.9047$ (Overtraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x26a8259b668>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length = 16\n",
    "num_units = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_src_x = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_src_y = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_x = tf.one_hot(tf_src_x, 3)\n",
    "tf_y = tf.one_hot(tf_src_y, 2)\n",
    "\n",
    "tf_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_units)\n",
    "#tf_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(num_units=num_units, activation=tf.nn.relu) for _ in range(16)])\n",
    "\n",
    "tf_out, _ = tf.nn.dynamic_rnn(tf_cell, inputs=tf_x, dtype=tf.float32)\n",
    "tf_res = tf.layers.dense(tf_out, 2, use_bias=True)\n",
    "\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf_res, labels=tf_y))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y,axis=2), tf.argmax(tf_res,axis=2)), dtype=tf.float32))\n",
    "tf_prob = tf.nn.softmax(tf_res)\n",
    "tf_train = tf.train.AdamOptimizer(1e-2).minimize(tf_loss)\n",
    "\n",
    "tf.summary.FileWriter('D:/Jupyter/Logs/09A_SeqRNN_2', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train 0.707 -> 0.213, Validation: 0.2992643117904663\n",
      "Step 1: Train 0.292 -> 0.169, Validation: 0.27766871452331543\n",
      "Step 2: Train 0.269 -> 0.167, Validation: 0.2666035294532776\n",
      "Step 3: Train 0.259 -> 0.173, Validation: 0.2639874815940857\n",
      "Step 4: Train 0.269 -> 0.167, Validation: 0.2515926957130432\n",
      "Final Cross-Entropy Loss: 0.2516\n",
      "Final Accuracy: 0.8885\n",
      "Maximum possible accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "num_epochs = 5\n",
    "seq_length = 16\n",
    "batch_size = 200\n",
    "valid_size = 1000\n",
    "\n",
    "\n",
    "valid_x, valid_y, valid_p = zip(*[gen_sample_v1(seq_length) for i in range(valid_size)])\n",
    "valid_batch = {tf_src_x: valid_x, tf_src_y: valid_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_x, train_y, _ = zip(*[gen_sample_v1(seq_length) for i in range(batch_size)])\n",
    "        train_batch = {tf_src_x: train_x, tf_src_y: train_y}\n",
    "\n",
    "        start_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        for j in range(num_steps):\n",
    "            tf_train.run(feed_dict=train_batch)\n",
    "        \n",
    "        end_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        valid_loss = tf_loss.eval(feed_dict=valid_batch)\n",
    "        \n",
    "        print('Step {0}: Train {1:1.3} -> {2:1.3}, Validation: {3}'.format(i, start_loss, end_loss, valid_loss))\n",
    "    final_p, final_loss, final_accuracy = sess.run([tf_prob, tf_loss, tf_accuracy], feed_dict=valid_batch)\n",
    "print('Final Cross-Entropy Loss: {0:1.4}\\nFinal Accuracy: {1:1.4}'.format(final_loss, final_accuracy))\n",
    "max_accuracy = np.mean(np.argmax(np.array([1-np.array(valid_p), np.array(valid_p)]), axis=0)==np.array(valid_y))\n",
    "print('Maximum possible accuracy: {0:1.4}'.format(max_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x26a8412dd68>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length = 16\n",
    "num_units = 16\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_src_x = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_src_y = tf.placeholder(shape=(None, src_length), dtype=tf.int32)\n",
    "tf_x = tf.one_hot(tf_src_x, 3)\n",
    "tf_y = tf.one_hot(tf_src_y, 2)\n",
    "\n",
    "tf_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_units)\n",
    "#tf_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(num_units=num_units, activation=tf.nn.relu) for _ in range(16)])\n",
    "\n",
    "tf_out, _ = tf.nn.dynamic_rnn(tf_cell, inputs=tf_x, dtype=tf.float32)\n",
    "tf_res = tf.layers.dense(tf_out, 2, use_bias=True)\n",
    "\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf_res, labels=tf_y))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y,axis=2), tf.argmax(tf_res,axis=2)), dtype=tf.float32))\n",
    "tf_prob = tf.nn.softmax(tf_res)\n",
    "tf_train = tf.train.AdamOptimizer(1e-2).minimize(tf_loss)\n",
    "\n",
    "tf.summary.FileWriter('D:/Jupyter/Logs/09A_SeqRNN_2', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train 0.689 -> 0.441, Validation: 0.752077579498291\n",
      "Step 1: Train 0.738 -> 0.411, Validation: 0.7948712110519409\n",
      "Step 2: Train 0.687 -> 0.39, Validation: 0.8525694608688354\n",
      "Step 3: Train 0.808 -> 0.394, Validation: 0.7713333964347839\n",
      "Step 4: Train 0.77 -> 0.377, Validation: 0.8468413352966309\n",
      "Final Cross-Entropy Loss: 0.8468\n",
      "Final Accuracy: 0.6666\n",
      "Maximum possible accuracy: 0.841\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "num_epochs = 5\n",
    "seq_length = 16\n",
    "batch_size = 200\n",
    "valid_size = 1000\n",
    "\n",
    "\n",
    "valid_x, valid_y, valid_p = zip(*[gen_sample_v2(seq_length) for i in range(valid_size)])\n",
    "valid_batch = {tf_src_x: valid_x, tf_src_y: valid_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_x, train_y, _ = zip(*[gen_sample_v2(seq_length) for i in range(batch_size)])\n",
    "        train_batch = {tf_src_x: train_x, tf_src_y: train_y}\n",
    "\n",
    "        start_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        for j in range(num_steps):\n",
    "            tf_train.run(feed_dict=train_batch)\n",
    "        \n",
    "        end_loss = tf_loss.eval(feed_dict=train_batch)\n",
    "        valid_loss = tf_loss.eval(feed_dict=valid_batch)\n",
    "        \n",
    "        print('Step {0}: Train {1:1.3} -> {2:1.3}, Validation: {3}'.format(i, start_loss, end_loss, valid_loss))\n",
    "    final_p, final_loss, final_accuracy = sess.run([tf_prob, tf_loss, tf_accuracy], feed_dict=valid_batch)\n",
    "print('Final Cross-Entropy Loss: {0:1.4}\\nFinal Accuracy: {1:1.4}'.format(final_loss, final_accuracy))\n",
    "max_accuracy = np.mean(np.argmax(np.array([1-np.array(valid_p), np.array(valid_p)]), axis=0)==np.array(valid_y))\n",
    "print('Maximum possible accuracy: {0:1.4}'.format(max_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
