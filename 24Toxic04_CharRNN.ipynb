{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import modutils\n",
    "import pickle\n",
    "import time, datetime\n",
    "import sklearn, sklearn.metrics, sklearn.decomposition\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_file = '../DataSets/Toxic/dev_train.csv'\n",
    "test_file = '../DataSets/Toxic/dev_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = pd.read_csv(train_file)\n",
    "src_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  207.,   437.,   896.,  1376.,  3508.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(x) for x in src_train.comment_text])\n",
    "np.percentile(lens, q=[50, 75, 90, 95, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(dict(collections.Counter([z for x in src_train.comment_text for z in x])).items(),\n",
    "               key=lambda x:x[1], reverse=True)\n",
    "chars_pct = np.cumsum([x[1] for x in chars]) / np.sum([x[1] for x in chars])\n",
    "chars_res = list(zip([x[0] for x in chars], chars_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_text(text, charmap, seq_len):\n",
    "    if type(text) is list:\n",
    "        return np.array([transform_text(x, charmap, seq_len) for x in text])\n",
    "    \n",
    "    tmp = [charmap[x] for x in text if x in charmap]\n",
    "    if len(tmp) >= seq_len:\n",
    "        return np.array(tmp[:seq_len])\n",
    "    return np.array(tmp + [0]*(seq_len - len(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_char_transform(chars_stats, seq_length, charset_size):\n",
    "    charmap = {x[0]:(i+1) for (i, x) in enumerate(chars_stats[:charset_size])}\n",
    "    return lambda x: transform_text(list(x), charmap, seq_length)\n",
    "\n",
    "def build_charrnn_graph(input_shape, rnn_arch, fc_arch):\n",
    "    RNNCell = lambda n: tf.nn.rnn_cell.GRUCell(num_units=n, activation=tf.nn.elu)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.name_scope('Input'):\n",
    "        tf_in_x = tf.placeholder(tf.int32, shape=(None, input_shape[0]))\n",
    "        tf_in_y = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        tf_x1hot = tf.one_hot(tf_in_x, input_shape[1])\n",
    "        tf_temp = tf_x1hot\n",
    "        \n",
    "    with tf.name_scope('RNN'):\n",
    "        rnnCell = tf.nn.rnn_cell.MultiRNNCell([RNNCell(s) for s in rnn_arch], state_is_tuple=True)\n",
    "    \n",
    "        tf_AllStates0, tf_FinState0 = tf.nn.dynamic_rnn(rnnCell, inputs=tf_x1hot, dtype=tf.float32, time_major=False)\n",
    "        tf_FinState = tf_FinState0[-1] #get latest layer in RNN\n",
    "        tf_AllStates = tf_AllStates0\n",
    "        \n",
    "    with tf.name_scope('SEQ-FC'):\n",
    "        tf_NextForecast = tf.layers.dense(tf_AllStates, input_shape[1])\n",
    "        tf_NextProb = tf.nn.softmax(tf_NextForecast)\n",
    "        tf_NextPredicted = tf.cast(tf.argmax(tf_NextProb, axis=2), dtype=tf.int32)\n",
    "\n",
    "    with tf.name_scope('FC'):\n",
    "        tf_temp = tf_FinState\n",
    "        for sz in fc_arch:\n",
    "            tf_temp = tf.layers.dense(tf_temp, sz, activation=tf.nn.elu)\n",
    "        tf_final = tf.layers.dense(tf_temp, 2)\n",
    "        tf_prob = tf.nn.softmax(tf_final)\n",
    "        tf_predicted = tf.cast(tf.argmax(tf_prob, axis=1), dtype=tf.int32)\n",
    "\n",
    "    with tf.name_scope('LOSS'):\n",
    "        tf_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_in_y, logits=tf_final))\n",
    "        tf_train = tf.train.AdamOptimizer(1e-3).minimize(tf_loss)\n",
    "        \n",
    "        tf_loss_seq = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_in_x[:,1:], logits=tf_NextForecast[:,:-1,:]))\n",
    "        tf_train_seq = tf.train.AdamOptimizer(1e-3).minimize(tf_loss_seq)\n",
    "        \n",
    "        tf_rocauc, tf_upd_rocuac = tf.metrics.auc(labels=tf_in_y, predictions=tf_prob[:,1], num_thresholds=10000)\n",
    "        tf_gini = tf_rocauc * 2 - 1\n",
    "        tf_accuracy, tf_upd_accuracy = tf.metrics.accuracy(labels=tf_in_y, predictions=tf_predicted)\n",
    "        tf_seq_accuracy, tf_upd_seq_accuracy = tf.metrics.accuracy(labels=tf_in_x[:,1:], predictions=tf_NextPredicted[:,:-1])\n",
    "        tf_update_metrics = tf.group(tf_upd_rocuac, tf_upd_accuracy, tf_upd_seq_accuracy)\n",
    "        \n",
    "        tfsummary_logloss = tf.summary.scalar('Log-Loss', tf_loss)\n",
    "        tfsummary_gini = tf.summary.scalar('1-Gini', 1-tf_gini)\n",
    "        tfsummary_accuracy = tf.summary.scalar('1-Accuracy', 1-tf_accuracy)\n",
    "        tfsummary = tf.summary.merge([tfsummary_logloss, tfsummary_gini, tfsummary_accuracy])\n",
    "\n",
    "    return {'in':{'data':tf_in_x, 'label':tf_in_y},\n",
    "            'out':{'logit':tf_final, 'prob':tf_prob},\n",
    "            'run':{'loss': tf_loss, 'seq-loss':tf_loss_seq, 'upd_metrics':tf_update_metrics,\n",
    "                   'gini':tf_gini, 'accuracy':tf_accuracy, 'seq-accuracy':tf_seq_accuracy,\n",
    "                   'train': tf_train, 'seq-train':tf_train_seq, 'summary':tfsummary}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created\n",
      "Preparation complete\n"
     ]
    }
   ],
   "source": [
    "transform_fun = prepare_char_transform(chars_res, 200, 50)\n",
    "graph_descr = build_charrnn_graph((200, 51), [40], [20])\n",
    "model_name = '24Toxic04CRNN_v1'\n",
    "\n",
    "tffw_graph = tf.summary.FileWriter('D:/Jupyter/Logs/Graph_{}'.format(model_name), tf.get_default_graph())\n",
    "model_ckpt_name = '../Models/{0}/model'.format(model_name)+'-{:02d}.ckpt'\n",
    "\n",
    "print('Graph created')\n",
    "\n",
    "batch_steps = 1\n",
    "batch_size  = 128\n",
    "calc_batch_size = 2048\n",
    "\n",
    "train_set = (src_train.comment_text.values, src_train.toxic.values)\n",
    "test_set = (src_test.comment_text.values, src_test.toxic.values)\n",
    "test_y = test_set[1]\n",
    "stat_set = test_set\n",
    "\n",
    "set2dict = lambda x: {graph_descr['in']['data']: transform_fun(x[0]), graph_descr['in']['label']: x[1]}\n",
    "\n",
    "stat_dict = set2dict(stat_set)\n",
    "print('Preparation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-00.ckpt\n",
      "Epoch 0: 0.243 in 163.46 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-01.ckpt\n",
      "Epoch 1: 0.214 in 162.82 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-02.ckpt\n",
      "Epoch 2: 0.205 in 168.62 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-03.ckpt\n",
      "Epoch 3: 0.203 in 158.80 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-04.ckpt\n",
      "Epoch 4: 0.204 in 153.92 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-05.ckpt\n",
      "Epoch 5: 0.191 in 159.60 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-06.ckpt\n",
      "Epoch 6: 0.186 in 153.86 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-07.ckpt\n",
      "Epoch 7: 0.184 in 158.61 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-08.ckpt\n",
      "Epoch 8: 0.180 in 160.82 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-09.ckpt\n",
      "Epoch 9: 0.181 in 162.13 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-10.ckpt\n",
      "Epoch 10: 0.167 in 161.40 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-11.ckpt\n",
      "Epoch 11: 0.163 in 165.91 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-12.ckpt\n",
      "Epoch 12: 0.161 in 164.28 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-13.ckpt\n",
      "Epoch 13: 0.157 in 163.76 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-14.ckpt\n",
      "Epoch 14: 0.163 in 162.35 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-15.ckpt\n",
      "Epoch 15: 0.154 in 164.92 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-16.ckpt\n",
      "Epoch 16: 0.151 in 159.79 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-17.ckpt\n",
      "Epoch 17: 0.152 in 163.57 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-18.ckpt\n",
      "Epoch 18: 0.161 in 162.23 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-19.ckpt\n",
      "Epoch 19: 0.157 in 153.97 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-20.ckpt\n",
      "Epoch 20: 0.148 in 156.63 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-21.ckpt\n",
      "Epoch 21: 0.147 in 153.85 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-22.ckpt\n",
      "Epoch 22: 0.152 in 153.07 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-23.ckpt\n",
      "Epoch 23: 0.147 in 152.89 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-24.ckpt\n",
      "Epoch 24: 0.145 in 153.15 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-25.ckpt\n",
      "Epoch 25: 0.151 in 154.60 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-26.ckpt\n",
      "Epoch 26: 0.145 in 162.02 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-27.ckpt\n",
      "Epoch 27: 0.149 in 163.57 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-28.ckpt\n",
      "Epoch 28: 0.148 in 164.08 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-29.ckpt\n",
      "Epoch 29: 0.149 in 163.57 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-30.ckpt\n",
      "Epoch 30: 0.153 in 160.82 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-31.ckpt\n",
      "Epoch 31: 0.148 in 158.25 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-32.ckpt\n",
      "Epoch 32: 0.162 in 162.93 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-33.ckpt\n",
      "Epoch 33: 0.151 in 160.00 sec\n",
      "Model saved at checkpoint: ../Models/24Toxic04CRNN_v1/model-34.ckpt\n",
      "Epoch 34: 0.157 in 168.53 sec\n",
      "24704/79336:\t0.039 -> 0.038\t0.20 sec\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-80eddfa34763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         modutils.runEpoch(tfs, train_set, batch_size, set2dict, graph_descr['run']['train'],\n\u001b[1;32m---> 15\u001b[1;33m                      op_loss=graph_descr['run']['loss'], verbatim=True)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#test_res = run_tf_calc(tfs, test_set, calc_batch_size, set2dict,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Jupyter\\mltest\\modutils.py\u001b[0m in \u001b[0;36mrunEpoch\u001b[1;34m(tfs, train_set, batch_size, set2feeddict, op_train, op_loss, batch_steps, verbatim)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[0mtl0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "tffw_valid = tf.summary.FileWriter('D:/Jupyter/Logs/Run_{0}-{1}-V'.format(model_name, dt_now), tf.get_default_graph())\n",
    "tfsSaver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    tfs.run(tf.local_variables_initializer())\n",
    "    \n",
    "    for n in range(num_epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        \n",
    "        modutils.runEpoch(tfs, train_set, batch_size, set2dict, graph_descr['run']['train'],\n",
    "                     op_loss=graph_descr['run']['loss'], verbatim=True)\n",
    "        \n",
    "        #test_res = run_tf_calc(tfs, test_set, calc_batch_size, set2dict,\n",
    "        #                       [graph_descr['run']['loss'], graph_descr['out']['prob']])\n",
    "        \n",
    "        #test_loss = np.sum([x[1] * x[2][0] for x in test_res]) / np.sum([x[1] for x in test_res])\n",
    "        #test_p = np.concatenate([x[2][1] for x in test_res])\n",
    "        #gini = sklearn.metrics.roc_auc_score(test_y, test_p[:,1])*2-1\n",
    "        #accur = sklearn.metrics.accuracy_score(test_y, 1*(test_p[:,1]>0.5))\n",
    "        \n",
    "        tfs.run(graph_descr['run']['upd_metrics'], stat_dict)\n",
    "        (loss, summary) = tfs.run([graph_descr['run']['loss'], graph_descr['run']['summary']], stat_dict)\n",
    "        tffw_valid.add_summary(summary, n)\n",
    "        t1 = time.perf_counter()\n",
    "        \n",
    "        p = tfsSaver.save(tfs, model_ckpt_name.format(n))\n",
    "        print('Model saved at checkpoint: {0}'.format(p))        \n",
    "        print('Epoch {0}: {1:.3f} in {2:.2f} sec'.format(n, loss, t1-t0))\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytf]",
   "language": "python",
   "name": "conda-env-pytf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
