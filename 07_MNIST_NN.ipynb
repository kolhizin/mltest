{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import struct\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "root_dir = \"D:/Jupyter/\";\n",
    "logs_dir = root_dir + \"Logs/\"\n",
    "data_dir = root_dir + 'Datasets/'\n",
    "\n",
    "def mnist_read_imgs(fname):\n",
    "    with open(fname, mode='rb') as f:\n",
    "        (_, img_num, img_xsize, img_ysize) = struct.unpack('>IIII',f.read(4 * 4))\n",
    "        data_img = np.fromfile(f, dtype=np.uint8).reshape(img_num, img_xsize, img_ysize)\n",
    "    return data_img\n",
    "\n",
    "def mnist_read_lbls(fname):\n",
    "    with open(data_dir + 'MNIST/train-labels.idx1-ubyte', mode='rb') as f:\n",
    "        (_, lab_num) = struct.unpack('>II', f.read(4 * 2))\n",
    "        data_lab = np.fromfile(f, dtype=np.uint8)\n",
    "    return data_lab\n",
    "\n",
    "def minibatch(X, y, num=1000):\n",
    "    inds = np.random.choice(range(X.shape[0]), size=num)\n",
    "    return X[inds], y[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_X = mnist_read_imgs(data_dir+'MNIST/train-images.idx3-ubyte')\n",
    "src_y = mnist_read_lbls(data_dir+'MNIST/train-labels.idx1-ubyte')\n",
    "\n",
    "random_seed = 42\n",
    "(dev_X, test_X, dev_y, test_y) = sklearn.model_selection.train_test_split(src_X, src_y, random_state=random_seed, test_size=0.2)\n",
    "(train_X, valid_X, train_y, valid_y) = sklearn.model_selection.train_test_split(dev_X, dev_y, random_state=random_seed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Neural Networks\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist1d_transform_imgs(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1] * x.shape[2]) / 255\n",
    "\n",
    "def mnist1d_transform_lbls(y):\n",
    "    return np.array([1.0*(y==i) for i in range(10)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train1d_X, valid1d_X, test1d_X) = (mnist1d_transform_imgs(x) for x in (train_X, valid_X, test_X))\n",
    "(train1d_y, valid1d_y, test1d_y) = (mnist1d_transform_lbls(y) for y in (train_y, valid_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-hidden layer network\n",
    "Current accuracy on validation is __92.5%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-LR-S-B5k'\n",
    "\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Output = tf.layers.dense(tf_Input, 10, use_bias=True, name='LogisticRegression')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Optimizer = tf.train.GradientDescentOptimizer(tf_LearningRate)\n",
    "tf_TrainStep = tf_Optimizer.minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 50:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn0_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn0_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn0_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 933,    0,    2,    1,    5,   11,    2,    0,    8,    1],\n",
       "       [   0, 1076,    2,    0,    1,    6,    1,    1,    9,    3],\n",
       "       [   8,   12,  818,   18,   10,    4,   12,   13,   20,    8],\n",
       "       [   2,    2,   23,  928,    2,   25,    2,    9,   26,    3],\n",
       "       [   1,    4,    7,    1,  887,    0,    8,    3,   11,   39],\n",
       "       [   5,    6,    7,   29,    5,  753,   15,    1,   19,    4],\n",
       "       [   4,    5,    5,    0,   11,   16,  903,    0,    4,    0],\n",
       "       [   1,    7,    7,    3,    5,    3,    1,  901,    0,   50],\n",
       "       [   4,   16,    7,   21,    1,   23,    5,    2,  816,   27],\n",
       "       [   5,    4,    0,    8,   26,    6,    0,   25,    3,  863]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92479166666666668"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(sklearn.metrics.confusion_matrix(valid_y, np.argmax(valid1d_nn0_prob, axis=1)))\n",
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn0_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-hidden layer network\n",
    "Current accuracy on validation is __97.3%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-H200-LR-S-B5k'\n",
    "\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Hidden = tf.layers.dense(tf_Input, 200, use_bias=True, activation=tf.nn.elu, name='Hidden-1')\n",
    "tf_Output = tf.layers.dense(tf_Hidden, 10, use_bias=True, name='SoftMax')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Optimizer = tf.train.GradientDescentOptimizer(tf_LearningRate)\n",
    "tf_TrainStep = tf_Optimizer.minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 50:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn1_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn1_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn1_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97281249999999997"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn1_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-hidden layer network\n",
    "With 300-300 combination, dropout and res-net hack arrived at __98.1%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-H300-H300-DROP-RES-LR-S-B5k-GD'\n",
    "\n",
    "tf_Training = tf.placeholder(shape=(), name='Training', dtype=tf.bool)\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Hidden1 = tf.layers.dense(tf_Input, 300, use_bias=True, activation=tf.nn.relu, name='Hidden-1')\n",
    "tf_Hidden2 = tf.layers.dense(tf.layers.dropout(tf_Hidden1, training=tf_Training),\n",
    "                             300, use_bias=True, activation=tf.nn.relu, name='Hidden-2')\n",
    "tf_Output = tf.layers.dense(tf.layers.dropout(tf.concat([tf_Hidden1, tf_Hidden2], axis=1), training=tf_Training),\n",
    "                            10, use_bias=True, name='SoftMax')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "#tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_Labels, logits=tf_Output))\n",
    "tf_TrainStep = tf.train.GradientDescentOptimizer(tf_LearningRate).minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Training: False, tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Training: False, tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Training: False, tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 25:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Training: True, tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn2_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn2_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn2_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98052083333333329"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn2_prob, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99843749999999998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.98062499999999997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.98024999999999995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn2_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn2_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn2_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99382812499999995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.97437499999999999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.97233333333333338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn1_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn1_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn1_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92942708333333335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92343750000000002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92133333333333334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn0_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn0_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn0_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2D Neural Networks\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist2d_transform_imgs(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1], x.shape[2], 1) / 255\n",
    "\n",
    "def mnist2d_transform_lbls(y):\n",
    "    return np.array([1.0*(y==i) for i in range(10)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train2d_X, valid2d_X, test2d_X) = (mnist2d_transform_imgs(x) for x in (train_X, valid_X, test_X))\n",
    "(train2d_y, valid2d_y, test2d_y) = (mnist2d_transform_lbls(y) for y in (train_y, valid_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Manual\n",
    "#### Desired architecture\n",
    "1) Convolution layer with __K1__ 3x3 filter\n",
    "\n",
    "2) Max Pooling layer 2x2 with stride => 28x28->14x14\n",
    "\n",
    "3) Convolution layer __K2__ 3x3 filter\n",
    "\n",
    "4) Max Pooling layer 2x2 with same padding and stride => 7x7\n",
    "\n",
    "5) 1 fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_layer3x3(name, x_input, num_out, activation=tf.nn.relu):\n",
    "    #x_input dimensions:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel\n",
    "    \n",
    "    #output dimension:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel (num_out)\n",
    "    with tf.name_scope(name=name):\n",
    "        tW0 = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[1,1,1,num_out]), dtype=tf.float32, name='Intercept')\n",
    "        tW = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[3,3,int(x_input.shape[3]),int(num_out)]), dtype=tf.float32, name='Weights')\n",
    "        tR = tW0 + tf.nn.conv2d(x_input, tW, strides=[1,1,1,1], padding='SAME')\n",
    "        return activation(tR)\n",
    "    \n",
    "def convolution_layer5x5(name, x_input, num_out, activation=tf.nn.relu):\n",
    "    #x_input dimensions:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel\n",
    "    \n",
    "    #output dimension:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel (num_out)\n",
    "    with tf.name_scope(name=name):\n",
    "        tW0 = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[1,1,1,num_out]), dtype=tf.float32, name='Intercept')\n",
    "        tW = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[5,5,int(x_input.shape[3]),int(num_out)]), dtype=tf.float32, name='Weights')\n",
    "        tR = tW0 + tf.nn.conv2d(x_input, tW, strides=[1,1,1,1], padding='SAME')\n",
    "        return activation(tR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-15C3-MP2-15C3-MP2-H1024-F10'\n",
    "\n",
    "tfTraining = tf.placeholder(shape=(),dtype=tf.bool) \n",
    "tfLR = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "tfInput = tf.placeholder(shape=(None,28,28,1),dtype=tf.float32)\n",
    "tfLabels = tf.placeholder(shape=(None,10),dtype=tf.float32)\n",
    "tfL1 = tf.nn.max_pool(convolution_layer5x5('L1-20C3', tfInput, 15), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfL2 = tf.nn.max_pool(convolution_layer5x5('L2-20C3', tfL1, 15), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfLF = tf.reshape(tfL2, shape=(-1, 49*15), name='FLAT')\n",
    "\n",
    "tfH = tf.layers.dense(tf.layers.dropout(tfLF, training=tfTraining), 1024, activation=tf.nn.elu)\n",
    "\n",
    "tfOut = tf.layers.dense(tfH, 10, use_bias=True, name='Output')\n",
    "tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfLabels, logits=tfOut, name='Loss'))\n",
    "tfOutProb = tf.nn.softmax(tfOut, name='OutputProbs')\n",
    "\n",
    "tfAccuracy = 1.0-tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tfOutProb, axis=1),tf.argmax(tfLabels, axis=1)), dtype=tf.float32))\n",
    "tfAccuracySummary = tf.summary.scalar('Accuracy', tfAccuracy)\n",
    "\n",
    "tfTrain = tf.train.AdamOptimizer(tfLR).minimize(tfLoss)\n",
    "tfInit = tf.global_variables_initializer()\n",
    "\n",
    "tffw = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss:  0.279722\n",
      "Epoch 1 training loss:  0.13167\n",
      "Epoch 2 training loss:  0.0723307\n",
      "Epoch 3 training loss:  0.0498464\n",
      "Epoch 4 training loss:  0.0374366\n",
      "Epoch 5 training loss:  0.0593303\n",
      "Epoch 6 training loss:  0.0515806\n",
      "Epoch 7 training loss:  0.0611168\n",
      "Epoch 8 training loss:  0.0369614\n",
      "Epoch 9 training loss:  0.0261763\n",
      "repeating run\n",
      "Epoch 10 training loss:  0.0132244\n",
      "repeating run\n",
      "Epoch 11 training loss:  0.0064265\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 12 training loss:  0.00552645\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 13 training loss:  0.00901855\n",
      "repeating run\n",
      "Epoch 14 training loss:  0.0150277\n",
      "Epoch 15 training loss:  0.0376857\n",
      "Epoch 16 training loss:  0.0125948\n",
      "repeating run\n",
      "Epoch 17 training loss:  0.0129953\n",
      "repeating run\n",
      "Epoch 18 training loss:  0.002734\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 19 training loss:  0.00405908\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 20 training loss:  0.00132252\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 21 training loss:  0.00159324\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 22 training loss:  0.000839358\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 23 training loss:  0.00121691\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 24 training loss:  0.00110951\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 25 training loss:  0.000865716\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 26 training loss:  0.00253023\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 27 training loss:  0.000928794\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 28 training loss:  0.000257867\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 29 training loss:  0.000400885\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 30 training loss:  0.000403195\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 31 training loss:  0.00060054\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 32 training loss:  0.00272429\n",
      "repeating run\n",
      "Epoch 33 training loss:  0.0062128\n",
      "repeating run\n",
      "Epoch 34 training loss:  0.00123088\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 35 training loss:  0.00170135\n",
      "repeating run\n",
      "Epoch 36 training loss:  0.00877293\n",
      "repeating run\n",
      "Epoch 37 training loss:  0.00613793\n",
      "repeating run\n",
      "Epoch 38 training loss:  0.000749991\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 39 training loss:  0.00272381\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 40 training loss:  0.00105973\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 41 training loss:  0.00529686\n",
      "Epoch 42 training loss:  0.00418318\n",
      "repeating run\n",
      "Epoch 43 training loss:  0.000669753\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 44 training loss:  0.0015344\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 45 training loss:  0.000232829\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 46 training loss:  9.91279e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 47 training loss:  0.00450471\n",
      "Epoch 48 training loss:  0.0055525\n",
      "Epoch 49 training loss:  0.00141158\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 50 training loss:  0.00862756\n",
      "repeating run\n",
      "Epoch 51 training loss:  0.00425774\n",
      "repeating run\n",
      "Epoch 52 training loss:  0.00764821\n",
      "repeating run\n",
      "Epoch 53 training loss:  0.00105992\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 54 training loss:  0.000753914\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 55 training loss:  0.000522873\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 56 training loss:  0.00237529\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 57 training loss:  0.000202239\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 58 training loss:  0.000169835\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 59 training loss:  0.000658487\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 60 training loss:  0.000284638\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 61 training loss:  6.42343e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 62 training loss:  8.28612e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 63 training loss:  0.000333478\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 64 training loss:  0.0027513\n",
      "Epoch 65 training loss:  0.000976655\n",
      "repeating run\n",
      "Epoch 66 training loss:  0.000925406\n",
      "Epoch 67 training loss:  0.00444897\n",
      "repeating run\n",
      "Epoch 68 training loss:  0.000645946\n",
      "repeating run\n",
      "Epoch 69 training loss:  0.000864419\n",
      "repeating run\n",
      "Epoch 70 training loss:  0.00700812\n",
      "repeating run\n",
      "Epoch 71 training loss:  0.000644343\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 72 training loss:  0.00724968\n",
      "Epoch 73 training loss:  0.0034654\n",
      "repeating run\n",
      "Epoch 74 training loss:  0.000904441\n",
      "Epoch 75 training loss:  0.00685151\n",
      "Epoch 76 training loss:  0.00527615\n",
      "Epoch 77 training loss:  0.00164832\n",
      "Epoch 78 training loss:  0.00480879\n",
      "repeating run\n",
      "Epoch 79 training loss:  0.000100146\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 80 training loss:  0.000182551\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 81 training loss:  0.00679913\n",
      "Epoch 82 training loss:  0.00493924\n",
      "repeating run\n",
      "Epoch 83 training loss:  8.38901e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 84 training loss:  0.000294114\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 85 training loss:  0.000134877\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 86 training loss:  0.000256701\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 87 training loss:  0.000250491\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 88 training loss:  0.000548403\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 89 training loss:  0.00332235\n",
      "Epoch 90 training loss:  0.00133233\n",
      "repeating run\n",
      "Epoch 91 training loss:  0.000453545\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 92 training loss:  0.000183839\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 93 training loss:  0.0023739\n",
      "repeating run\n",
      "Epoch 94 training loss:  0.00251734\n",
      "Epoch 95 training loss:  0.000320847\n",
      "repeating run\n",
      "Epoch 96 training loss:  0.00122597\n",
      "repeating run\n",
      "Epoch 97 training loss:  0.00406562\n",
      "repeating run\n",
      "Epoch 98 training loss:  9.74014e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 99 training loss:  0.00125124\n",
      "repeating run\n",
      "Epoch 100 training loss:  0.00534148\n",
      "repeating run\n",
      "Epoch 101 training loss:  0.000146721\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 102 training loss:  5.16418e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 103 training loss:  7.53768e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 104 training loss:  6.65333e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 105 training loss:  2.04706e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 106 training loss:  1.64043e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 107 training loss:  0.000284196\n",
      "repeating run\n",
      "Epoch 108 training loss:  0.000109608\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 109 training loss:  1.10079e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 110 training loss:  4.54896e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 111 training loss:  7.18097e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 112 training loss:  7.15687e-05\n",
      "repeating run\n",
      "Epoch 113 training loss:  0.000172854\n",
      "repeating run\n",
      "Epoch 114 training loss:  8.10822e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 115 training loss:  9.27233e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 116 training loss:  0.000259904\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 117 training loss:  1.73387e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 118 training loss:  2.61618e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 119 training loss:  0.00214972\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 120 training loss:  1.46533e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 121 training loss:  0.000139679\n",
      "Epoch 122 training loss:  0.000680333\n",
      "repeating run\n",
      "Epoch 123 training loss:  9.22713e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 124 training loss:  0.00134568\n",
      "repeating run\n",
      "Epoch 125 training loss:  0.000612042\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 126 training loss:  1.78653e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 127 training loss:  3.41206e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 128 training loss:  0.000615059\n",
      "repeating run\n",
      "Epoch 129 training loss:  7.87025e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 130 training loss:  0.000231184\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 131 training loss:  0.000191405\n",
      "repeating run\n",
      "Epoch 132 training loss:  0.000131586\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 133 training loss:  9.2605e-05\n",
      "repeating run\n",
      "Epoch 134 training loss:  7.69875e-05\n",
      "repeating run\n",
      "Epoch 135 training loss:  0.00926572\n",
      "Epoch 136 training loss:  0.00311617\n",
      "repeating run\n",
      "Epoch 137 training loss:  0.000330644\n",
      "repeating run\n",
      "Epoch 138 training loss:  0.0046904\n",
      "Epoch 139 training loss:  0.000516696\n",
      "repeating run\n",
      "Epoch 140 training loss:  0.00297232\n",
      "Epoch 141 training loss:  0.0051837\n",
      "Epoch 142 training loss:  0.000597735\n",
      "repeating run\n",
      "Epoch 143 training loss:  0.000289384\n",
      "repeating run\n",
      "Epoch 144 training loss:  0.00174356\n",
      "Epoch 145 training loss:  0.000896032\n",
      "Epoch 146 training loss:  0.000638051\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 147 training loss:  0.00240073\n",
      "Epoch 148 training loss:  0.00599061\n",
      "repeating run\n",
      "Epoch 149 training loss:  0.00709609\n",
      "Epoch 150 training loss:  0.00162516\n",
      "repeating run\n",
      "Epoch 151 training loss:  6.8147e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 152 training loss:  0.000130977\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 153 training loss:  0.00010309\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 154 training loss:  0.000135565\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 155 training loss:  2.15951e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 156 training loss:  2.63126e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 157 training loss:  0.000301698\n",
      "repeating run\n",
      "Epoch 158 training loss:  9.19058e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 159 training loss:  2.0026e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 160 training loss:  3.15393e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 161 training loss:  0.00106233\n",
      "Epoch 162 training loss:  0.00146844\n",
      "Epoch 163 training loss:  0.00035303\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 164 training loss:  8.38177e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 165 training loss:  7.54812e-05\n",
      "repeating run\n",
      "Epoch 166 training loss:  0.00674482\n",
      "Epoch 167 training loss:  0.00173368\n",
      "repeating run\n",
      "Epoch 168 training loss:  0.000736499\n",
      "Epoch 169 training loss:  0.015121\n",
      "repeating run\n",
      "Epoch 170 training loss:  0.0006958\n",
      "Epoch 171 training loss:  0.0019444\n",
      "Epoch 172 training loss:  6.92303e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 173 training loss:  6.25213e-05\n",
      "repeating run\n",
      "Epoch 174 training loss:  1.11642e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 175 training loss:  1.15885e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 176 training loss:  3.07429e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 177 training loss:  0.000639658\n",
      "repeating run\n",
      "Epoch 178 training loss:  0.00143085\n",
      "Epoch 179 training loss:  0.000743399\n",
      "Epoch 180 training loss:  0.00514728\n",
      "repeating run\n",
      "Epoch 181 training loss:  0.000112205\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 182 training loss:  2.82522e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 183 training loss:  0.00504246\n",
      "repeating run\n",
      "Epoch 184 training loss:  0.000789972\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 185 training loss:  2.1486e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 186 training loss:  0.000699127\n",
      "repeating run\n",
      "Epoch 187 training loss:  8.08301e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 188 training loss:  2.38973e-05\n",
      "Epoch 189 training loss:  0.00677812\n",
      "Epoch 190 training loss:  0.000715623\n",
      "Epoch 191 training loss:  0.00695939\n",
      "Epoch 192 training loss:  0.000506626\n",
      "repeating run\n",
      "Epoch 193 training loss:  1.68116e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 194 training loss:  6.04046e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 195 training loss:  9.3408e-05\n",
      "Epoch 196 training loss:  5.29056e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 197 training loss:  0.00591168\n",
      "Epoch 198 training loss:  0.00814176\n",
      "Epoch 199 training loss:  9.56664e-05\n",
      "repeating run\n",
      "Epoch 200 training loss:  2.16583e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 201 training loss:  0.000122481\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 202 training loss:  0.00245188\n",
      "Epoch 203 training loss:  0.013286\n",
      "Epoch 204 training loss:  0.000112662\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 205 training loss:  7.4702e-05\n",
      "Epoch 206 training loss:  0.00701174\n",
      "Epoch 207 training loss:  0.010738\n",
      "Epoch 208 training loss:  0.00903289\n",
      "Epoch 209 training loss:  9.7116e-05\n",
      "Epoch 210 training loss:  0.00193337\n",
      "Epoch 211 training loss:  0.00308907\n",
      "Epoch 212 training loss:  0.000133292\n",
      "repeating run\n",
      "Epoch 213 training loss:  5.57243e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 214 training loss:  0.000204349\n",
      "Epoch 215 training loss:  5.57594e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 216 training loss:  2.12604e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 217 training loss:  1.15808e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 218 training loss:  0.00043746\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 219 training loss:  5.8115e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 220 training loss:  3.93053e-05\n",
      "Epoch 221 training loss:  0.00316758\n",
      "Epoch 222 training loss:  0.00273272\n",
      "repeating run\n",
      "Epoch 223 training loss:  0.000430576\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 224 training loss:  1.04376e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 225 training loss:  3.91951e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 226 training loss:  8.05577e-05\n",
      "repeating run\n",
      "Epoch 227 training loss:  3.24573e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 228 training loss:  0.000108203\n",
      "Epoch 229 training loss:  5.05589e-05\n",
      "repeating run\n",
      "Epoch 230 training loss:  0.000114123\n",
      "repeating run\n",
      "Epoch 231 training loss:  1.60131e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 232 training loss:  1.195e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 233 training loss:  1.70291e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 234 training loss:  3.04376e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 235 training loss:  1.3862e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 236 training loss:  1.11249e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 237 training loss:  3.61104e-05\n",
      "repeating run\n",
      "Epoch 238 training loss:  0.00134064\n",
      "repeating run\n",
      "Epoch 239 training loss:  0.00011227\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 240 training loss:  8.36141e-05\n",
      "Epoch 241 training loss:  1.08014e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 242 training loss:  1.44151e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 243 training loss:  0.000149966\n",
      "repeating run\n",
      "Epoch 244 training loss:  1.31214e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 245 training loss:  3.99815e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 246 training loss:  7.57881e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 247 training loss:  0.00323886\n",
      "Epoch 248 training loss:  0.00184494\n",
      "Epoch 249 training loss:  0.000216579\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 250 training loss:  7.1287e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 251 training loss:  4.08773e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 252 training loss:  0.000432067\n",
      "repeating run\n",
      "Epoch 253 training loss:  0.00145138\n",
      "repeating run\n",
      "Epoch 254 training loss:  5.49695e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 255 training loss:  2.55998e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 256 training loss:  0.00183837\n",
      "Epoch 257 training loss:  0.0022255\n",
      "Epoch 258 training loss:  0.00361758\n",
      "Epoch 259 training loss:  0.000133757\n",
      "repeating run\n",
      "Epoch 260 training loss:  0.00043943\n",
      "Epoch 261 training loss:  0.000309935\n",
      "repeating run\n",
      "Epoch 262 training loss:  0.0213719\n",
      "Epoch 263 training loss:  0.000226095\n",
      "Epoch 264 training loss:  3.2783e-05\n",
      "Epoch 265 training loss:  0.00190477\n",
      "Epoch 266 training loss:  0.00396455\n",
      "Epoch 267 training loss:  0.00140416\n",
      "Epoch 268 training loss:  0.000195161\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 269 training loss:  0.00416873\n",
      "Epoch 270 training loss:  0.0080665\n",
      "Epoch 271 training loss:  0.0151449\n",
      "Epoch 272 training loss:  8.8398e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 273 training loss:  0.000141699\n",
      "Epoch 274 training loss:  0.00754492\n",
      "Epoch 275 training loss:  0.0027805\n",
      "Epoch 276 training loss:  0.000215535\n",
      "Epoch 277 training loss:  0.0166181\n",
      "Epoch 278 training loss:  0.00453846\n",
      "Epoch 279 training loss:  0.000503155\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 280 training loss:  0.000183819\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 281 training loss:  5.66971e-05\n",
      "repeating run\n",
      "Epoch 282 training loss:  0.00542565\n",
      "Epoch 283 training loss:  0.000106943\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 284 training loss:  5.17043e-06\n",
      "Epoch 285 training loss:  3.96751e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 286 training loss:  1.57838e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 287 training loss:  3.77338e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 288 training loss:  0.000223481\n",
      "repeating run\n",
      "Epoch 289 training loss:  0.00152123\n",
      "repeating run\n",
      "Epoch 290 training loss:  8.11845e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 291 training loss:  6.91162e-05\n",
      "repeating run\n",
      "Epoch 292 training loss:  3.41862e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 293 training loss:  4.60642e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 294 training loss:  1.44822e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 295 training loss:  9.47945e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 296 training loss:  3.93704e-05\n",
      "Epoch 297 training loss:  0.00615967\n",
      "Epoch 298 training loss:  0.00121095\n",
      "repeating run\n",
      "Epoch 299 training loss:  5.60845e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 300 training loss:  1.50469e-05\n",
      "repeating run\n",
      "Epoch 301 training loss:  0.000412833\n",
      "repeating run\n",
      "Epoch 302 training loss:  1.09603e-05\n",
      "Epoch 303 training loss:  0.00759086\n",
      "Epoch 304 training loss:  0.00241968\n",
      "Epoch 305 training loss:  0.000375753\n",
      "Epoch 306 training loss:  3.02306e-06\n",
      "repeating run\n",
      "Epoch 307 training loss:  0.000156333\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 308 training loss:  0.000668913\n",
      "Epoch 309 training loss:  0.000177543\n",
      "Epoch 310 training loss:  0.000886515\n",
      "Epoch 311 training loss:  0.00572696\n",
      "repeating run\n",
      "Epoch 312 training loss:  4.39818e-06\n",
      "repeating run\n",
      "Epoch 313 training loss:  1.34742e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 314 training loss:  1.3207e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 315 training loss:  0.000628062\n",
      "repeating run\n",
      "Epoch 316 training loss:  0.000547695\n",
      "repeating run\n",
      "Epoch 317 training loss:  4.3462e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 318 training loss:  1.20154e-05\n",
      "repeating run\n",
      "Epoch 319 training loss:  0.000807051\n",
      "repeating run\n",
      "Epoch 320 training loss:  2.02459e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 321 training loss:  5.79361e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 322 training loss:  1.7604e-06\n",
      "repeating run\n",
      "Epoch 323 training loss:  0.000206125\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 324 training loss:  0.00024535\n",
      "Epoch 325 training loss:  0.00617954\n",
      "Epoch 326 training loss:  0.000451506\n",
      "Epoch 327 training loss:  1.76257e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 328 training loss:  8.81987e-07\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 329 training loss:  0.00378146\n",
      "Epoch 330 training loss:  1.40289e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 331 training loss:  1.39647e-05\n",
      "Epoch 332 training loss:  0.000395162\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 333 training loss:  1.53811e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 334 training loss:  1.18824e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 335 training loss:  5.8814e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 336 training loss:  8.34086e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 337 training loss:  1.02857e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeating run\n",
      "Epoch 338 training loss:  9.60919e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 339 training loss:  7.51377e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 340 training loss:  6.37693e-05\n",
      "repeating run\n",
      "Epoch 341 training loss:  0.00257118\n",
      "Epoch 342 training loss:  2.24443e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 343 training loss:  3.85975e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 344 training loss:  4.46598e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 345 training loss:  0.00119156\n",
      "repeating run\n",
      "Epoch 346 training loss:  6.02944e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 347 training loss:  9.02278e-06\n",
      "Epoch 348 training loss:  6.45506e-05\n",
      "Epoch 349 training loss:  0.00141103\n",
      "Epoch 350 training loss:  0.000389726\n",
      "Epoch 351 training loss:  5.08872e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 352 training loss:  5.70697e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 353 training loss:  1.42972e-05\n",
      "Epoch 354 training loss:  0.000103787\n",
      "Epoch 355 training loss:  0.000687071\n",
      "repeating run\n",
      "Epoch 356 training loss:  2.04414e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 357 training loss:  0.00461688\n",
      "Epoch 358 training loss:  0.00858072\n",
      "Epoch 359 training loss:  8.97316e-05\n",
      "repeating run\n",
      "Epoch 360 training loss:  0.000195852\n",
      "repeating run\n",
      "Epoch 361 training loss:  0.000358696\n",
      "Epoch 362 training loss:  0.00279563\n",
      "Epoch 363 training loss:  0.00727887\n",
      "Epoch 364 training loss:  0.00169773\n",
      "Epoch 365 training loss:  2.31934e-05\n",
      "Epoch 366 training loss:  0.000440753\n",
      "repeating run\n",
      "Epoch 367 training loss:  0.000245322\n",
      "Epoch 368 training loss:  0.00247961\n",
      "Epoch 369 training loss:  0.00167692\n",
      "Epoch 370 training loss:  1.35515e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 371 training loss:  2.23569e-06\n",
      "repeating run\n",
      "Epoch 372 training loss:  0.00286683\n",
      "Epoch 373 training loss:  6.82328e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 374 training loss:  4.41918e-06\n",
      "Epoch 375 training loss:  1.50868e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 376 training loss:  1.42941e-05\n",
      "Epoch 377 training loss:  6.44071e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 378 training loss:  1.03514e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 379 training loss:  2.70211e-05\n",
      "Epoch 380 training loss:  0.00107539\n",
      "Epoch 381 training loss:  0.000164464\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 382 training loss:  0.00845402\n",
      "repeating run\n",
      "Epoch 383 training loss:  2.00608e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 384 training loss:  1.94595e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 385 training loss:  4.86909e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 386 training loss:  0.00281666\n",
      "Epoch 387 training loss:  0.00019392\n",
      "Epoch 388 training loss:  0.000436112\n",
      "Epoch 389 training loss:  0.00144909\n",
      "Epoch 390 training loss:  0.00027235\n",
      "Epoch 391 training loss:  0.000230709\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 392 training loss:  3.38554e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 393 training loss:  1.23977e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 394 training loss:  9.0599e-09\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 395 training loss:  1.03828e-05\n",
      "repeating run\n",
      "Epoch 396 training loss:  1.45854e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 397 training loss:  4.01677e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 398 training loss:  1.54903e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 399 training loss:  0.001256\n",
      "Epoch 400 training loss:  2.51095e-06\n",
      "Epoch 401 training loss:  3.94848e-06\n",
      "repeating run\n",
      "Epoch 402 training loss:  6.03222e-06\n",
      "Epoch 403 training loss:  0.000790207\n",
      "Epoch 404 training loss:  0.00309293\n",
      "Epoch 405 training loss:  0.000415856\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 406 training loss:  0.00972634\n",
      "Epoch 407 training loss:  0.000332038\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 408 training loss:  6.83704e-05\n",
      "Epoch 409 training loss:  0.000123331\n",
      "Epoch 410 training loss:  0.000358162\n",
      "repeating run\n",
      "Epoch 411 training loss:  0.00291484\n",
      "Epoch 412 training loss:  0.00661795\n",
      "Epoch 413 training loss:  0.000771656\n",
      "Epoch 414 training loss:  2.65849e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 415 training loss:  4.13666e-05\n",
      "Epoch 416 training loss:  0.00637829\n",
      "Epoch 417 training loss:  0.000578849\n",
      "Epoch 418 training loss:  0.0131137\n",
      "Epoch 419 training loss:  1.87503e-05\n",
      "repeating run\n",
      "Epoch 420 training loss:  1.66636e-05\n",
      "repeating run\n",
      "Epoch 421 training loss:  6.15737e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 422 training loss:  0.000619963\n",
      "repeating run\n",
      "Epoch 423 training loss:  2.38418e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 424 training loss:  1.04904e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 425 training loss:  2.07427e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 426 training loss:  0.000155606\n",
      "repeating run\n",
      "Epoch 427 training loss:  0.000171429\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 428 training loss:  3.14711e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 429 training loss:  1.80236e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 430 training loss:  1.62073e-06\n",
      "repeating run\n",
      "Epoch 431 training loss:  0.000288063\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 432 training loss:  3.56588e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 433 training loss:  1.04604e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 434 training loss:  2.51175e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 435 training loss:  1.11676e-05\n",
      "Epoch 436 training loss:  0.000166915\n",
      "Epoch 437 training loss:  0.0418942\n",
      "Epoch 438 training loss:  2.26252e-07\n",
      "repeating run\n",
      "Epoch 439 training loss:  1.28394e-05\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 440 training loss:  4.19616e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 441 training loss:  1.76914e-06\n",
      "repeating run\n",
      "Epoch 442 training loss:  7.22024e-06\n",
      "repeating run\n",
      "Epoch 443 training loss:  6.11763e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 444 training loss:  0.000427443\n",
      "Epoch 445 training loss:  0.00010966\n",
      "Epoch 446 training loss:  0.0133354\n",
      "Epoch 447 training loss:  4.00794e-05\n",
      "Epoch 448 training loss:  3.44981e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 449 training loss:  4.43455e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 450 training loss:  9.77515e-09\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451 training loss:  2.00271e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 452 training loss:  3.01656e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 453 training loss:  2.16661e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 454 training loss:  0.000101181\n",
      "Epoch 455 training loss:  0.0039562\n",
      "Epoch 456 training loss:  6.91468e-05\n",
      "repeating run\n",
      "Epoch 457 training loss:  3.61653e-07\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 458 training loss:  3.12547e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 459 training loss:  0.00450841\n",
      "Epoch 460 training loss:  0.00279027\n",
      "Epoch 461 training loss:  7.63765e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 462 training loss:  0.000154488\n",
      "Epoch 463 training loss:  8.68117e-05\n",
      "repeating run\n",
      "Epoch 464 training loss:  3.92753e-06\n",
      "Epoch 465 training loss:  0.000429849\n",
      "Epoch 466 training loss:  7.20665e-05\n",
      "Epoch 467 training loss:  0.00159983\n",
      "Epoch 468 training loss:  0.0070535\n",
      "Epoch 469 training loss:  0.000371089\n",
      "Epoch 470 training loss:  0.0125874\n",
      "Epoch 471 training loss:  0.000107186\n",
      "repeating run\n",
      "Epoch 472 training loss:  4.17941e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 473 training loss:  3.27096e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 474 training loss:  1.3113e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 475 training loss:  1.28469e-06\n",
      "Epoch 476 training loss:  0.00111513\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 477 training loss:  0.000684361\n",
      "repeating run\n",
      "Epoch 478 training loss:  2.11004e-05\n",
      "repeating run\n",
      "Epoch 479 training loss:  1.42172e-05\n",
      "Epoch 480 training loss:  0.00382416\n",
      "Epoch 481 training loss:  0.00025208\n",
      "Epoch 482 training loss:  3.23501e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 483 training loss:  1.6069e-07\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 484 training loss:  1.00136e-08\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 485 training loss:  7.97172e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 486 training loss:  8.64966e-06\n",
      "Epoch 487 training loss:  7.59924e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 488 training loss:  1.35599e-06\n",
      "Epoch 489 training loss:  8.53443e-07\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 490 training loss:  0.0290381\n",
      "Epoch 491 training loss:  0.000248139\n",
      "Epoch 492 training loss:  0.000270284\n",
      "Epoch 493 training loss:  2.11445e-06\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 494 training loss:  0.0132807\n",
      "Epoch 495 training loss:  9.08448e-05\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 496 training loss:  4.17914e-07\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 497 training loss:  3.05852e-05\n",
      "Epoch 498 training loss:  3.24151e-06\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 499 training loss:  3.28962e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 30\n",
    "num_epochs = 500\n",
    "batch_size = 500\n",
    "\n",
    "vX, vy = minibatch(valid2d_X, valid2d_y, num=5000)\n",
    "valid_batch = {tfTraining: False, tfLR: 0.0, tfLabels: vy, tfInput: vX}\n",
    "prev_loss = 1e20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tfInit.run()\n",
    "    for i in range(num_epochs):\n",
    "        tX, ty = minibatch(train2d_X, train2d_y, num=batch_size)\n",
    "        train_batch = {tfTraining: True, tfLR: learning_rate, tfLabels: ty, tfInput: tX}\n",
    "        for j in range(num_steps):\n",
    "            tfTrain.run(feed_dict=train_batch)\n",
    "        while tfLoss.eval(feed_dict=train_batch) > 1.05 * prev_loss:\n",
    "            print('repeating run')\n",
    "            for j in range(num_steps):\n",
    "                tfTrain.run(feed_dict=train_batch)\n",
    "        prev_loss = tfLoss.eval(feed_dict=train_batch)\n",
    "        accstr = tfAccuracySummary.eval(feed_dict=valid_batch)\n",
    "        \n",
    "        print('Epoch {0} training loss: '.format(i), prev_loss)\n",
    "        tffw.add_summary(accstr, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-15C3-MP2-15C3-MP2-H100-H100-F10'\n",
    "\n",
    "tfTraining = tf.placeholder(shape=(),dtype=tf.bool) \n",
    "tfLR = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "tfInput = tf.placeholder(shape=(None,28,28,1),dtype=tf.float32)\n",
    "tfLabels = tf.placeholder(shape=(None,10),dtype=tf.float32)\n",
    "\n",
    "tfL1 = tf.nn.max_pool(tf.layers.conv2d(tfInput, 15, [3,3], strides=[1,1], padding='SAME'), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfL2 = tf.nn.max_pool(tf.layers.conv2d(tfL1, 15, [3,3], strides=[1,1], padding='SAME'), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfLF = tf.reshape(tfL2, shape=(-1, 49*15), name='FLAT')\n",
    "\n",
    "tfH1 = tf.layers.dense(tfLF, 100, use_bias=True, name='Hidden1', activation=tf.nn.relu)\n",
    "tfH2 = tf.layers.dense(tfH1, 100, use_bias=True, name='Hidden2', activation=tf.nn.relu)\n",
    "tfOut = tf.layers.dense(tfH2, 10, use_bias=True, name='Output')\n",
    "tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfLabels, logits=tfOut, name='Loss'))\n",
    "tfOutProb = tf.nn.softmax(tfOut, name='OutputProbs')\n",
    "\n",
    "tfAccuracy = 1.0-tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tfOutProb, axis=1),tf.argmax(tfLabels, axis=1)), dtype=tf.float32))\n",
    "tfAccuracySummary = tf.summary.scalar('Accuracy', tfAccuracy)\n",
    "\n",
    "tfTrain = tf.train.GradientDescentOptimizer(tfLR).minimize(tfLoss)\n",
    "tfInit = tf.global_variables_initializer()\n",
    "\n",
    "tffw = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(train_X[:10,10:13,10:13] * np.array([[1,0,0],[0,1,0],[0,0,1]]),axis=1),axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:  [ 1.0196079   1.10588241  0.          0.          0.95686275  1.03137255\n",
      "  1.24705887  2.22745085  0.          1.36078429]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-TEST'\n",
    "\n",
    "tfA = tf.Variable(train_X[:10,10:13,10:13],dtype=tf.float32)\n",
    "tfW = tf.Variable(np.array([[1,0,0],[0,1,0],[0,0,1]],dtype=np.float32).reshape(3,3))\n",
    "#tfR = tf.reduce_sum(tfA * tfW, axis=[1,2])\n",
    "tfR2 = tf.tensordot(tfA, tfW, axes=[[1,2],[0,1]])\n",
    "\n",
    "tfI = tf.global_variables_initializer()\n",
    "tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as ses:\n",
    "    tfI.run()\n",
    "    print('R: ', tfR2.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98703956,  0.53436008,  0.59091353, -0.7829164 ],\n",
       "       [-0.21208172,  0.38659534, -0.95584803,  0.13732337]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp0 = np.random.uniform(-1.0,1.0,[2,4])\n",
    "tmp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1 = np.array([0,0,1,0]).reshape(1,4)\n",
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98703956,  0.53436008,  1.59091353, -0.7829164 ],\n",
       "       [-0.21208172,  0.38659534,  0.04415197,  0.13732337]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp0+tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
