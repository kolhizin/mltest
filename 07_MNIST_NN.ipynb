{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import struct\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "root_dir = \"D:/Jupyter/\";\n",
    "logs_dir = root_dir + \"Logs/\"\n",
    "data_dir = root_dir + 'Datasets/'\n",
    "\n",
    "def mnist_read_imgs(fname):\n",
    "    with open(fname, mode='rb') as f:\n",
    "        (_, img_num, img_xsize, img_ysize) = struct.unpack('>IIII',f.read(4 * 4))\n",
    "        data_img = np.fromfile(f, dtype=np.uint8).reshape(img_num, img_xsize, img_ysize)\n",
    "    return data_img\n",
    "\n",
    "def mnist_read_lbls(fname):\n",
    "    with open(data_dir + 'MNIST/train-labels.idx1-ubyte', mode='rb') as f:\n",
    "        (_, lab_num) = struct.unpack('>II', f.read(4 * 2))\n",
    "        data_lab = np.fromfile(f, dtype=np.uint8)\n",
    "    return data_lab\n",
    "\n",
    "def minibatch(X, y, num=1000):\n",
    "    inds = np.random.choice(range(X.shape[0]), size=num)\n",
    "    return X[inds], y[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_X = mnist_read_imgs(data_dir+'MNIST/train-images.idx3-ubyte')\n",
    "src_y = mnist_read_lbls(data_dir+'MNIST/train-labels.idx1-ubyte')\n",
    "\n",
    "random_seed = 42\n",
    "(dev_X, test_X, dev_y, test_y) = sklearn.model_selection.train_test_split(src_X, src_y, random_state=random_seed, test_size=0.2)\n",
    "(train_X, valid_X, train_y, valid_y) = sklearn.model_selection.train_test_split(dev_X, dev_y, random_state=random_seed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Neural Networks\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist1d_transform_imgs(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1] * x.shape[2]) / 255\n",
    "\n",
    "def mnist1d_transform_lbls(y):\n",
    "    return np.array([1.0*(y==i) for i in range(10)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train1d_X, valid1d_X, test1d_X) = (mnist1d_transform_imgs(x) for x in (train_X, valid_X, test_X))\n",
    "(train1d_y, valid1d_y, test1d_y) = (mnist1d_transform_lbls(y) for y in (train_y, valid_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-hidden layer network\n",
    "Current accuracy on validation is __92.5%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-LR-S-B5k'\n",
    "\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Output = tf.layers.dense(tf_Input, 10, use_bias=True, name='LogisticRegression')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Optimizer = tf.train.GradientDescentOptimizer(tf_LearningRate)\n",
    "tf_TrainStep = tf_Optimizer.minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 50:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn0_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn0_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn0_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 933,    0,    2,    1,    5,   11,    2,    0,    8,    1],\n",
       "       [   0, 1076,    2,    0,    1,    6,    1,    1,    9,    3],\n",
       "       [   8,   12,  818,   18,   10,    4,   12,   13,   20,    8],\n",
       "       [   2,    2,   23,  928,    2,   25,    2,    9,   26,    3],\n",
       "       [   1,    4,    7,    1,  887,    0,    8,    3,   11,   39],\n",
       "       [   5,    6,    7,   29,    5,  753,   15,    1,   19,    4],\n",
       "       [   4,    5,    5,    0,   11,   16,  903,    0,    4,    0],\n",
       "       [   1,    7,    7,    3,    5,    3,    1,  901,    0,   50],\n",
       "       [   4,   16,    7,   21,    1,   23,    5,    2,  816,   27],\n",
       "       [   5,    4,    0,    8,   26,    6,    0,   25,    3,  863]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92479166666666668"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(sklearn.metrics.confusion_matrix(valid_y, np.argmax(valid1d_nn0_prob, axis=1)))\n",
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn0_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-hidden layer network\n",
    "Current accuracy on validation is __97.3%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-H200-LR-S-B5k'\n",
    "\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Hidden = tf.layers.dense(tf_Input, 200, use_bias=True, activation=tf.nn.elu, name='Hidden-1')\n",
    "tf_Output = tf.layers.dense(tf_Hidden, 10, use_bias=True, name='SoftMax')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Optimizer = tf.train.GradientDescentOptimizer(tf_LearningRate)\n",
    "tf_TrainStep = tf_Optimizer.minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 50:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn1_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn1_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn1_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97281249999999997"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn1_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-hidden layer network\n",
    "With 300-300 combination, dropout and res-net hack arrived at __98.1%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-H300-H300-DROP-RES-LR-S-B5k-GD'\n",
    "\n",
    "tf_Training = tf.placeholder(shape=(), name='Training', dtype=tf.bool)\n",
    "tf_LearningRate = tf.placeholder(shape=(), name='LearningRate', dtype=tf.float32)\n",
    "tf_Input = tf.placeholder(shape=(None, 784), name='Input', dtype=tf.float32)\n",
    "tf_Labels = tf.placeholder(shape=(None, 10), name='Labels', dtype=tf.float32)\n",
    "\n",
    "tf_Hidden1 = tf.layers.dense(tf_Input, 300, use_bias=True, activation=tf.nn.relu, name='Hidden-1')\n",
    "tf_Hidden2 = tf.layers.dense(tf.layers.dropout(tf_Hidden1, training=tf_Training),\n",
    "                             300, use_bias=True, activation=tf.nn.relu, name='Hidden-2')\n",
    "tf_Output = tf.layers.dense(tf.layers.dropout(tf.concat([tf_Hidden1, tf_Hidden2], axis=1), training=tf_Training),\n",
    "                            10, use_bias=True, name='SoftMax')\n",
    "\n",
    "tf_OutProb = tf.nn.softmax(tf_Output)\n",
    "\n",
    "#tf_Error = -tf.reduce_mean(tf.reduce_sum(tf_Labels * tf.log(tf_OutProb), reduction_indices=1))\n",
    "tf_Error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_Labels, logits=tf_Output))\n",
    "tf_TrainStep = tf.train.GradientDescentOptimizer(tf_LearningRate).minimize(tf_Error)\n",
    "\n",
    "tf_Initialize = tf.global_variables_initializer()\n",
    "\n",
    "tf_ErrorSummary = tf.summary.scalar('Error', tf_Error)\n",
    "tf_FW = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_steps = 100\n",
    "batch_size = 5000\n",
    "learning_rate = 0.5\n",
    "fulltrain_batch = {tf_Training: False, tf_Input: train1d_X, tf_Labels:train1d_y}\n",
    "validation_batch = {tf_Training: False, tf_Input: valid1d_X, tf_Labels: valid1d_y}\n",
    "test_batch = {tf_Training: False, tf_Input: test1d_X, tf_Labels: test1d_y}\n",
    "with tf.Session() as sess:\n",
    "    tf_Initialize.run()\n",
    "    for i in range(num_epochs):\n",
    "        if i > 10:\n",
    "            learning_rate = 0.2\n",
    "        if i > 25:\n",
    "            learning_rate = 0.1\n",
    "        tX, ty = minibatch(train1d_X, train1d_y, num=batch_size)\n",
    "        batch = {tf_Training: True, tf_Input: tX, tf_Labels:ty, tf_LearningRate: learning_rate}\n",
    "        for j in range(num_steps):\n",
    "            tf_TrainStep.run(feed_dict=batch)\n",
    "        \n",
    "        sumstr = tf_ErrorSummary.eval(feed_dict=validation_batch)\n",
    "        tf_FW.add_summary(sumstr, i)\n",
    "    train1d_nn2_prob = tf_OutProb.eval(feed_dict=fulltrain_batch)\n",
    "    valid1d_nn2_prob = tf_OutProb.eval(feed_dict=validation_batch)\n",
    "    test1d_nn2_prob = tf_OutProb.eval(feed_dict=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98052083333333329"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn2_prob, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99843749999999998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.98062499999999997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.98024999999999995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn2_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn2_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn2_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99382812499999995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.97437499999999999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.97233333333333338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn1_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn1_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn1_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92942708333333335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92343750000000002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92133333333333334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sklearn.metrics.accuracy_score(train_y, np.argmax(train1d_nn0_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(valid_y, np.argmax(valid1d_nn0_prob, axis=1)))\n",
    "display(sklearn.metrics.accuracy_score(test_y, np.argmax(test1d_nn0_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2D Neural Networks\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist2d_transform_imgs(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1], x.shape[2], 1) / 255\n",
    "\n",
    "def mnist2d_transform_lbls(y):\n",
    "    return np.array([1.0*(y==i) for i in range(10)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train2d_X, valid2d_X, test2d_X) = (mnist2d_transform_imgs(x) for x in (train_X, valid_X, test_X))\n",
    "(train2d_y, valid2d_y, test2d_y) = (mnist2d_transform_lbls(y) for y in (train_y, valid_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Manual\n",
    "#### Desired architecture\n",
    "1) Convolution layer with __K1__ 3x3 filter\n",
    "\n",
    "2) Max Pooling layer 2x2 with stride => 28x28->14x14\n",
    "\n",
    "3) Convolution layer __K2__ 3x3 filter\n",
    "\n",
    "4) Max Pooling layer 2x2 with same padding and stride => 7x7\n",
    "\n",
    "5) 1 fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer3x3(name, x_input, num_out, activation=tf.nn.relu):\n",
    "    #x_input dimensions:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel\n",
    "    \n",
    "    #output dimension:\n",
    "    #0    minibatch\n",
    "    #1,2  row & col \n",
    "    #3    channel (num_out)\n",
    "    with tf.name_scope(name=name):\n",
    "        tW0 = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[1,1,1,num_out]), dtype=tf.float32, name='Intercept')\n",
    "        tW = tf.Variable(tf.truncated_normal(stddev=0.1,shape=[3,3,int(x_input.shape[3]),int(num_out)]), dtype=tf.float32, name='Weights')\n",
    "        tR = tW0 + tf.nn.conv2d(x_input, tW, strides=[1,1,1,1], padding='SAME')\n",
    "        return activation(tR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-15C3-MP2-15C3-MP2-F10'\n",
    "\n",
    "tfTraining = tf.placeholder(shape=(),dtype=tf.bool) \n",
    "tfLR = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "tfInput = tf.placeholder(shape=(None,28,28,1),dtype=tf.float32)\n",
    "tfLabels = tf.placeholder(shape=(None,10),dtype=tf.float32)\n",
    "tfL1 = tf.nn.max_pool(convolution_layer3x3('L1-15C3', tfInput, 15), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfL2 = tf.nn.max_pool(convolution_layer3x3('L2-15C3', tfL1, 15), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfLF = tf.reshape(tfL2, shape=(-1, 49*15), name='FLAT')\n",
    "\n",
    "tfOut = tf.layers.dense(tfLF, 10, use_bias=True, name='Output')\n",
    "tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfLabels, logits=tfOut, name='Loss'))\n",
    "tfOutProb = tf.nn.softmax(tfOut, name='OutputProbs')\n",
    "\n",
    "tfAccuracy = 1.0-tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tfOutProb, axis=1),tf.argmax(tfLabels, axis=1)), dtype=tf.float32))\n",
    "tfAccuracySummary = tf.summary.scalar('Accuracy', tfAccuracy)\n",
    "\n",
    "tfTrain = tf.train.GradientDescentOptimizer(tfLR).minimize(tfLoss)\n",
    "tfInit = tf.global_variables_initializer()\n",
    "\n",
    "tffw = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss:  2.24203\n",
      "Epoch 1 training loss:  2.20602\n",
      "Epoch 2 training loss:  2.07541\n",
      "Epoch 3 training loss:  1.95152\n",
      "Epoch 4 training loss:  1.72314\n",
      "Epoch 5 training loss:  1.3309\n",
      "Epoch 6 training loss:  0.980136\n",
      "Epoch 7 training loss:  0.707363\n",
      "Epoch 8 training loss:  0.541347\n",
      "Epoch 9 training loss:  0.41554\n",
      "Epoch 10 training loss:  0.40434\n",
      "Epoch 11 training loss:  0.311344\n",
      "repeating run\n",
      "Epoch 12 training loss:  0.308851\n",
      "Epoch 13 training loss:  0.247814\n",
      "Epoch 14 training loss:  0.204581\n",
      "Epoch 15 training loss:  0.182909\n",
      "Epoch 16 training loss:  0.170959\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 17 training loss:  0.186893\n",
      "Epoch 18 training loss:  0.107808\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 19 training loss:  0.109062\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 20 training loss:  0.0886431\n",
      "repeating run\n",
      "Epoch 21 training loss:  0.0879674\n",
      "Epoch 22 training loss:  0.0809701\n",
      "repeating run\n",
      "Epoch 23 training loss:  0.0613584\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 24 training loss:  0.0523004\n",
      "repeating run\n",
      "Epoch 25 training loss:  0.0469877\n",
      "repeating run\n",
      "Epoch 26 training loss:  0.0420247\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 27 training loss:  0.0451654\n",
      "repeating run\n",
      "Epoch 28 training loss:  0.0492515\n",
      "Epoch 29 training loss:  0.0539111\n",
      "repeating run\n",
      "Epoch 30 training loss:  0.0366107\n",
      "repeating run\n",
      "Epoch 31 training loss:  0.0341116\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 32 training loss:  0.0308181\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 33 training loss:  0.0255155\n",
      "repeating run\n",
      "Epoch 34 training loss:  0.0205628\n",
      "repeating run\n",
      "Epoch 35 training loss:  0.0200061\n",
      "repeating run\n",
      "Epoch 36 training loss:  0.0169138\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 37 training loss:  0.0151356\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 38 training loss:  0.0158769\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 39 training loss:  0.017366\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 40 training loss:  0.0167499\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 41 training loss:  0.0140768\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 42 training loss:  0.0131846\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 43 training loss:  0.0129607\n",
      "repeating run\n",
      "Epoch 44 training loss:  0.0121979\n",
      "repeating run\n",
      "Epoch 45 training loss:  0.012768\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 46 training loss:  0.0131625\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 47 training loss:  0.0126214\n",
      "repeating run\n",
      "Epoch 48 training loss:  0.0116632\n",
      "repeating run\n",
      "Epoch 49 training loss:  0.0105046\n",
      "repeating run\n",
      "Epoch 50 training loss:  0.0113576\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 51 training loss:  0.0108513\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 52 training loss:  0.0105385\n",
      "repeating run\n",
      "Epoch 53 training loss:  0.00988814\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 54 training loss:  0.00931417\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 55 training loss:  0.00952966\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 56 training loss:  0.00840569\n",
      "repeating run\n",
      "Epoch 57 training loss:  0.00739206\n",
      "repeating run\n",
      "repeating run\n",
      "repeating run\n",
      "Epoch 58 training loss:  0.00724681\n",
      "repeating run\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-ccc466d312c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mtfTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maccstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfAccuracySummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {0} training loss: '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3926\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3927\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3928\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_steps = 30\n",
    "num_epochs = 500\n",
    "batch_size = 200\n",
    "\n",
    "valid_batch = {tfTraining: False, tfLR: 0.0, tfLabels: valid2d_y, tfInput: valid2d_X}\n",
    "prev_loss = 1e20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tfInit.run()\n",
    "    for i in range(num_epochs):\n",
    "        tX, ty = minibatch(train2d_X, train2d_y, num=batch_size)\n",
    "        train_batch = {tfTraining: True, tfLR: learning_rate, tfLabels: ty, tfInput: tX}\n",
    "        for j in range(num_steps):\n",
    "            tfTrain.run(feed_dict=train_batch)\n",
    "        while tfLoss.eval(feed_dict=train_batch) > 1.10 * prev_loss:\n",
    "            print('repeating run')\n",
    "            for j in range(num_steps):\n",
    "                tfTrain.run(feed_dict=train_batch)\n",
    "        prev_loss = tfLoss.eval(feed_dict=train_batch)\n",
    "        accstr = tfAccuracySummary.eval(feed_dict=valid_batch)\n",
    "        \n",
    "        print('Epoch {0} training loss: '.format(i), prev_loss)\n",
    "        tffw.add_summary(accstr, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-15C3-MP2-15C3-MP2-H100-H100-F10'\n",
    "\n",
    "tfTraining = tf.placeholder(shape=(),dtype=tf.bool) \n",
    "tfLR = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "tfInput = tf.placeholder(shape=(None,28,28,1),dtype=tf.float32)\n",
    "tfLabels = tf.placeholder(shape=(None,10),dtype=tf.float32)\n",
    "\n",
    "tfL1 = tf.nn.max_pool(tf.layers.conv2d(tfInput, 15, [3,3], strides=[1,1], padding='SAME'), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfL2 = tf.nn.max_pool(tf.layers.conv2d(tfL1, 15, [3,3], strides=[1,1], padding='SAME'), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "tfLF = tf.reshape(tfL2, shape=(-1, 49*15), name='FLAT')\n",
    "\n",
    "tfH1 = tf.layers.dense(tfLF, 100, use_bias=True, name='Hidden1', activation=tf.nn.relu)\n",
    "tfH2 = tf.layers.dense(tfH1, 100, use_bias=True, name='Hidden2', activation=tf.nn.relu)\n",
    "tfOut = tf.layers.dense(tfH2, 10, use_bias=True, name='Output')\n",
    "tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfLabels, logits=tfOut, name='Loss'))\n",
    "tfOutProb = tf.nn.softmax(tfOut, name='OutputProbs')\n",
    "\n",
    "tfAccuracy = 1.0-tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tfOutProb, axis=1),tf.argmax(tfLabels, axis=1)), dtype=tf.float32))\n",
    "tfAccuracySummary = tf.summary.scalar('Accuracy', tfAccuracy)\n",
    "\n",
    "tfTrain = tf.train.GradientDescentOptimizer(tfLR).minimize(tfLoss)\n",
    "tfInit = tf.global_variables_initializer()\n",
    "\n",
    "tffw = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(train_X[:10,10:13,10:13] * np.array([[1,0,0],[0,1,0],[0,0,1]]),axis=1),axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:  [ 1.0196079   1.10588241  0.          0.          0.95686275  1.03137255\n",
      "  1.24705887  2.22745085  0.          1.36078429]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now + '-TEST'\n",
    "\n",
    "tfA = tf.Variable(train_X[:10,10:13,10:13],dtype=tf.float32)\n",
    "tfW = tf.Variable(np.array([[1,0,0],[0,1,0],[0,0,1]],dtype=np.float32).reshape(3,3))\n",
    "#tfR = tf.reduce_sum(tfA * tfW, axis=[1,2])\n",
    "tfR2 = tf.tensordot(tfA, tfW, axes=[[1,2],[0,1]])\n",
    "\n",
    "tfI = tf.global_variables_initializer()\n",
    "tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as ses:\n",
    "    tfI.run()\n",
    "    print('R: ', tfR2.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98703956,  0.53436008,  0.59091353, -0.7829164 ],\n",
       "       [-0.21208172,  0.38659534, -0.95584803,  0.13732337]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp0 = np.random.uniform(-1.0,1.0,[2,4])\n",
    "tmp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1 = np.array([0,0,1,0]).reshape(1,4)\n",
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98703956,  0.53436008,  1.59091353, -0.7829164 ],\n",
       "       [-0.21208172,  0.38659534,  0.04415197,  0.13732337]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp0+tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
