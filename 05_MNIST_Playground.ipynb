{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import struct\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "root_dir = \"D:/Jupyter/\";\n",
    "logs_dir = root_dir + \"Logs/\"\n",
    "data_dir = root_dir + 'Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_read_imgs(fname):\n",
    "    with open(fname, mode='rb') as f:\n",
    "        (_, img_num, img_xsize, img_ysize) = struct.unpack('>IIII',f.read(4 * 4))\n",
    "        data_img = np.fromfile(f, dtype=np.uint8).reshape(img_num, img_xsize, img_ysize)\n",
    "    return data_img\n",
    "\n",
    "def mnist_read_lbls(fname):\n",
    "    with open(data_dir + 'MNIST/train-labels.idx1-ubyte', mode='rb') as f:\n",
    "        (_, lab_num) = struct.unpack('>II', f.read(4 * 2))\n",
    "        data_lab = np.fromfile(f, dtype=np.uint8)\n",
    "    return data_lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_X = mnist_read_imgs(data_dir+'MNIST/train-images.idx3-ubyte')\n",
    "src_y = mnist_read_lbls(data_dir+'MNIST/train-labels.idx1-ubyte')\n",
    "\n",
    "random_seed = 42\n",
    "(dev_X, test_X, dev_y, test_y) = sklearn.model_selection.train_test_split(src_X, src_y, random_state=random_seed, test_size=0.2)\n",
    "(train_X, valid_X, train_y, valid_y) = sklearn.model_selection.train_test_split(dev_X, dev_y, random_state=random_seed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist1d_transform_imgs(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1] * x.shape[2]) / 255\n",
    "\n",
    "def mnist_transform_lbls(y):\n",
    "    return[1.0*(y==i) for i in range(10)]\n",
    "\n",
    "\n",
    "def mnist_transform_prob(yp):\n",
    "    return [(np.asarray(range(10))[(x >= np.max(x))][0], np.max(x)/np.sum(x)) for x in yp]\n",
    "\n",
    "def mnist_confmat(y, yf):\n",
    "    ty = np.asarray(y)\n",
    "    tyf = np.asarray(yf)[:,0].reshape(ty.shape)\n",
    "    return np.asarray([[np.sum(1*(ty==act)*(tyf==est)) for est in range(10)] for act in range(10)])\n",
    "\n",
    "def confmat_accuracy(confmat):\n",
    "    return np.sum(np.diag(confmat)) / np.sum(confmat)\n",
    "\n",
    "def confmat_pctact(confmat):\n",
    "    return confmat / np.sum(confmat, axis=0)\n",
    "\n",
    "def confmat_accuracy_lbl(confmat):\n",
    "    return np.diag(confmat) / np.sum(confmat, axis=0)\n",
    "\n",
    "def mnist_eval_prob(y, yp):\n",
    "    yf = mnist_transform_prob(yp)\n",
    "    cf = mnist_confmat(y, yf)\n",
    "    return confmat_accuracy(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1d_X = mnist1d_transform_imgs(train_X)\n",
    "train1d_y = mnist_transform_lbls(train_y)\n",
    "\n",
    "valid1d_X = mnist1d_transform_imgs(valid_X)\n",
    "valid1d_y = mnist_transform_lbls(valid_y)\n",
    "test1d_X = mnist1d_transform_imgs(test_X)\n",
    "test1d_y = mnist_transform_lbls(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted all models:\t\t 40.954590970224714\n",
      " + calculated probabilities:\t 41.822692847633206\n"
     ]
    }
   ],
   "source": [
    "#scikit-learn logistic regression\n",
    "start = time.perf_counter()\n",
    "sklrs = []\n",
    "for i in range(10):\n",
    "    sklr = linear_model.LogisticRegression()\n",
    "    sklr.fit(train1d_X, train1d_y[i])\n",
    "    sklrs = sklrs + [sklr]\n",
    "print('Fitted all models:\\t\\t', time.perf_counter() - start)\n",
    "sklr_train_yp = np.asarray([x.predict_proba(train1d_X)[:,1] for x in sklrs]).transpose()\n",
    "sklr_valid_yp = np.asarray([x.predict_proba(valid1d_X)[:,1] for x in sklrs]).transpose()\n",
    "print(' + calculated probabilities:\\t', time.perf_counter() - start)\n",
    "#~40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_eval_LogisticRegression(**kargs):\n",
    "    sklrs = []\n",
    "    t0 = time.perf_counter()\n",
    "    for i in range(10):\n",
    "        sklr = linear_model.LogisticRegression(**kargs)\n",
    "        sklr.fit(train1d_X, train1d_y[i])\n",
    "        sklrs = sklrs + [sklr]\n",
    "    t_fit = time.perf_counter() - t0\n",
    "    #Calculate\n",
    "    sklr_train_yp = np.asarray([x.predict_proba(train1d_X)[:,1] for x in sklrs]).transpose()\n",
    "    sklr_valid_yp = np.asarray([x.predict_proba(valid1d_X)[:,1] for x in sklrs]).transpose()\n",
    "    t_calc = time.perf_counter() - (t0+t_fit)\n",
    "    #Evaluate\n",
    "    train_res = mnist_eval_prob(train_y, sklr_train_yp)\n",
    "    valid_res = mnist_eval_prob(valid_y, sklr_valid_yp)\n",
    "    t_eval = time.perf_counter() - (t0+t_fit+t_calc)\n",
    "    return ((train_res, valid_res), (t_fit, t_calc, t_eval), sklrs)\n",
    "\n",
    "def mnist_eval_LinearSVM(**kargs):\n",
    "    sklrs = []\n",
    "    t0 = time.perf_counter()\n",
    "    for i in range(10):\n",
    "        sklr = svm.LinearSVC(**kargs)\n",
    "        sklr.fit(train1d_X, train1d_y[i])\n",
    "        sklrs = sklrs + [sklr]\n",
    "    t_fit = time.perf_counter() - t0\n",
    "    #Calculate\n",
    "    sklr_train_yp = np.asarray([x.predict(train1d_X) for x in sklrs]).transpose()\n",
    "    sklr_valid_yp = np.asarray([x.predict(valid1d_X) for x in sklrs]).transpose()\n",
    "    t_calc = time.perf_counter() - (t0+t_fit)\n",
    "    #Evaluate\n",
    "    train_res = mnist_eval_prob(train_y, sklr_train_yp)\n",
    "    valid_res = mnist_eval_prob(val_y, sklr_valid_yp)\n",
    "    t_eval = time.perf_counter() - (t0+t_fit+t_calc)\n",
    "    return ((train_res, valid_res), (t_fit, t_calc, t_eval), sklrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing (fit / calc / eval):\t 41.31127215362413 0.8263044103732113 1.0737019329884276\n",
      "Accuracy (train / valid):\t 0.929244791667 0.91875\n"
     ]
    }
   ],
   "source": [
    "#default logistic regression\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression()\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing (fit / calc / eval):\t 22.407737832281782 0.7833809462317731 1.0519142952725815\n",
      "Accuracy (train / valid):\t 0.925104166667 0.920520833333\n",
      "Timing (fit / calc / eval):\t 25.986422583542662 0.793956220297332 1.1127934838677902\n",
      "Accuracy (train / valid):\t 0.926927083333 0.920104166667\n",
      "Timing (fit / calc / eval):\t 27.45085978850875 0.8062506182886864 1.0600432254123007\n",
      "Accuracy (train / valid):\t 0.92859375 0.9203125\n"
     ]
    }
   ],
   "source": [
    "#L1-regularization Logistic regression\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l1',C=0.4,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l1',C=0.6,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l1',C=0.8,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing (fit / calc / eval):\t 31.344842363925636 0.7797372946060932 1.0490813089345465\n",
      "Accuracy (train / valid):\t 0.926354166667 0.920208333333\n",
      "Timing (fit / calc / eval):\t 33.28618599161382 0.7987356340054248 1.0540680143276404\n",
      "Accuracy (train / valid):\t 0.92796875 0.919791666667\n",
      "Timing (fit / calc / eval):\t 38.65521351798452 0.7909099009903002 1.064505093938351\n",
      "Accuracy (train / valid):\t 0.928828125 0.918854166667\n"
     ]
    }
   ],
   "source": [
    "#L2-regularization Logistic regression\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l2',C=0.4,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l2',C=0.6,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LogisticRegression(penalty='l2',C=0.8,n_jobs=4,max_iter=500)\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing (fit / calc / eval):\t 53.083806630464096 0.830865467320109 1.1599853800835263\n",
      "Accuracy (train / valid):\t 0.872708333333 0.858229166667\n"
     ]
    }
   ],
   "source": [
    "#default linear-SVM regression\n",
    "((train_acc, valid_acc), (dt_fit, dt_calc, dt_eval), _) = mnist_eval_LinearSVM()\n",
    "print('Timing (fit / calc / eval):\\t', dt_fit, dt_calc, dt_eval)\n",
    "print('Accuracy (train / valid):\\t', train_acc, valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Speed benchmarking: SkLearn vs NumPy vs TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Will clasify digit-9\n",
    "bm_train_X = train1d_X\n",
    "bm_train_y = train1d_y[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "sklr = linear_model.LogisticRegression().fit(bm_train_X, bm_train_y)\n",
    "sklr_time = time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6302586656729545"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklr_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Loss & Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss = sum(yi * ln(p(zi)) + (1 - yi) * ln(1 - p(zi)))\n",
    "     = sum(yi * ln(p(zi) / (1 - p(zi)) + ln(1 - p(zi)) )\n",
    "\n",
    "p(x) = 1 / (1 + exp(-x))\n",
    "=> 1 - p(x) = 1 / (1 + exp(x))\n",
    "=> p(x) / 1 - p(x) = 1 / exp(-x) = exp(x)\n",
    "\n",
    "#### Loss = sum(yi * zi - ln(1 + exp(zi)))\n",
    "\n",
    "dLoss / dzi = yi - 1/(1 + exp(zi)) * exp(zi)\n",
    "            = yi - p(zi)\n",
    "            \n",
    "zi = w0 + w * x\n",
    "dzi / dw0 = 1\n",
    "dzi / dw  = xi\n",
    "\n",
    "#### dLoss / dw0 = mean(yi - p(zi))\n",
    "#### dLoss / dw  = mean((yi - p(zi)) * xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_calc_grad(X, y, w0, w):\n",
    "    Z = w0 + np.matmul(X, w)\n",
    "    dp = y.reshape(Z.shape) - 1 / (1 + np.exp(-Z))\n",
    "    dw0 = np.mean(dp)\n",
    "    dw = np.mean(dp * X, axis=0).reshape(w.shape)\n",
    "    return (dw0, dw)\n",
    "\n",
    "def logreg_make_step(X, y, w0, w, learning_rate=1):\n",
    "    Z = w0 + np.matmul(X, w)\n",
    "    dp = y.reshape(Z.shape) - 1 / (1 + np.exp(-Z))\n",
    "    dw0 = np.mean(dp)\n",
    "    dw = np.mean(dp * X, axis=0).reshape(w.shape)\n",
    "    nw0 = w0 + learning_rate * dw0\n",
    "    nw = w + learning_rate * dw\n",
    "    return (nw0, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 0\n",
    "w = np.zeros((bm_train_X.shape[1], 1))\n",
    "num_steps = 100\n",
    "start = time.perf_counter()\n",
    "for i in range(num_steps):\n",
    "    (w0, w) = logreg_make_step(bm_train_X, bm_train_y, w0, w)\n",
    "numpylr_time = time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw0: 0.00636958773088\n",
      "dw (mean/max): 0.000236895165698 0.00140417549946\n",
      "time: 15.517000586762151\n"
     ]
    }
   ],
   "source": [
    "numpylr_Z = w0 + np.matmul(bm_train_X, w)\n",
    "numpylr_p = 1 / (1 + np.exp(-numpylr_Z))\n",
    "(numpylr_dw0, numpylr_dw) = logreg_calc_grad(bm_train_X, bm_train_y, w0, w)\n",
    "print('dw0:', abs(numpylr_dw0))\n",
    "print('dw (mean/max):', np.mean(np.abs(numpylr_dw)), np.max(np.abs(numpylr_dw)))\n",
    "print('time:', numpylr_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow All-Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x2e1c55b27f0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now\n",
    "\n",
    "tf_w0 = tf.Variable(0.0, name='w0')\n",
    "tf_w = tf.Variable(np.zeros(shape=(bm_train_X.shape[1], 1)), name='w', dtype=tf.float32)\n",
    "tf_lr = tf.placeholder(name='learning_rate', dtype=tf.float32, shape=())\n",
    "tf_y = tf.placeholder(name='y', dtype=tf.float32, shape=(None,1))\n",
    "tf_X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, 28*28))\n",
    "tf_Z = tf_w0 + tf.matmul(tf_X, tf_w)\n",
    "tf_p = 1 / (1 + tf.exp(-tf_Z))\n",
    "tf_dp = tf_y - tf_p\n",
    "tf_dw0 = tf.reduce_mean(tf_dp)\n",
    "tf_dw = tf.reshape(tf.reduce_mean(tf.multiply(tf_dp, tf_X), axis=0), tf_w.shape)\n",
    "tf_train_w0 = tf.assign(tf_w0, tf_w0 + tf_lr * tf_dw0)\n",
    "tf_train_w = tf.assign(tf_w, tf_w + tf_lr * tf_dw)\n",
    "tf_trainop = tf.group(tf_train_w, tf_train_w0)\n",
    "tf_start = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=100\n",
    "batch = {tf_X: bm_train_X, tf_y: bm_train_y.reshape(bm_train_X.shape[0],1), tf_lr:1.0}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf_start)\n",
    "    start = time.perf_counter()\n",
    "    for i in range(num_steps):\n",
    "        sess.run(tf_trainop, feed_dict=batch)\n",
    "    tfman_time = time.perf_counter() - start\n",
    "    tfman_w0 = tf_w0.eval()\n",
    "    tfman_w = tf_w.eval()\n",
    "    tfman_p = tf_p.eval(feed_dict=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x2e1c58faba8>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now\n",
    "\n",
    "tf_w0 = tf.Variable(0.0, name='w0')\n",
    "tf_w = tf.Variable(np.zeros(shape=(bm_train_X.shape[1], 1)), name='w', dtype=tf.float32)\n",
    "tf_lr = tf.placeholder(name='learning_rate', dtype=tf.float32, shape=())\n",
    "tf_y = tf.placeholder(name='y', dtype=tf.float32, shape=(None,1))\n",
    "tf_X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, 28*28))\n",
    "tf_Z = tf_w0 + tf.matmul(tf_X, tf_w)\n",
    "tf_p = 1 / (1 + tf.exp(-tf_Z))\n",
    "tf_loss = -tf.reduce_mean(tf_y * tf_Z - tf.log(1 + tf.exp(tf_Z)))\n",
    "tf_opt = tf.train.GradientDescentOptimizer(1.0)\n",
    "tf_trainop = tf_opt.minimize(tf_loss)\n",
    "tf_start = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps=100\n",
    "batch = {tf_X: bm_train_X, tf_y: bm_train_y.reshape(bm_train_X.shape[0],1), tf_lr:1.0}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf_start)\n",
    "    start = time.perf_counter()\n",
    "    for i in range(num_steps):\n",
    "        sess.run(tf_trainop, feed_dict=batch)\n",
    "    tfgd_time = time.perf_counter() - start\n",
    "    tfgd_w0 = tf_w0.eval()\n",
    "    tfgd_w = tf_w.eval()\n",
    "    tfgd_p = tf_p.eval(feed_dict=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.517000586762151, 67.120305629207, 13.184399634803412)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpylr_time, tfman_time, tfgd_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equivalent of Logistic Regression\n",
    "tf.reset_default_graph()\n",
    "dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "log_dir = root_dir + 'Logs/' + dt_now\n",
    "\n",
    "tf_lr = tf.placeholder(name='learning_rate', dtype=tf.float32, shape=())\n",
    "tf_y = tf.placeholder(name='y', dtype=tf.float32, shape=(None, 10))\n",
    "tf_X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, 28*28))\n",
    "\n",
    "tf_OUT = tf.layers.dense(tf_X, 10, use_bias=True) #Linear Activation\n",
    "tf_OUT_PROB = tf.nn.softmax(tf_OUT)\n",
    "\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_OUT))\n",
    "\n",
    "tf_optimizer = tf.train.GradientDescentOptimizer(learning_rate=tf_lr)\n",
    "\n",
    "tf_init = tf.global_variables_initializer()\n",
    "tf_train = tf_optimizer.minimize(tf_loss)\n",
    "\n",
    "tffw = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "tf_err_summary = tf.summary.scalar('ERROR', tf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.879148\n",
      "1 0.588482\n",
      "2 0.455498\n",
      "3 0.39903\n",
      "4 0.381073\n",
      "5 0.369185\n",
      "6 0.359958\n",
      "7 0.352458\n",
      "8 0.346203\n",
      "9 0.340889\n",
      "10 0.336308\n",
      "11 0.332312\n",
      "12 0.328791\n",
      "13 0.325663\n",
      "14 0.322862\n",
      "15 0.320339\n",
      "16 0.318052\n",
      "17 0.31597\n",
      "18 0.314064\n",
      "19 0.312314\n",
      "20 0.3107\n",
      "21 0.309206\n",
      "22 0.30782\n",
      "23 0.30653\n",
      "24 0.305326\n",
      "25 0.304199\n",
      "26 0.303143\n",
      "27 0.302151\n",
      "28 0.301217\n",
      "29 0.300335\n",
      "30 0.299503\n",
      "31 0.298715\n",
      "32 0.297969\n",
      "33 0.29726\n",
      "34 0.296587\n",
      "35 0.295946\n",
      "36 0.295336\n",
      "37 0.294754\n",
      "38 0.294198\n",
      "39 0.293667\n",
      "40 0.293159\n",
      "41 0.292673\n",
      "42 0.292207\n",
      "43 0.29176\n",
      "44 0.29133\n",
      "45 0.290918\n",
      "46 0.290522\n",
      "47 0.290141\n",
      "48 0.289774\n",
      "49 0.28942\n",
      "50 0.28908\n",
      "51 0.288751\n",
      "52 0.288434\n",
      "53 0.288129\n",
      "54 0.287833\n",
      "55 0.287548\n",
      "56 0.287272\n",
      "57 0.287005\n",
      "58 0.286747\n",
      "59 0.286497\n",
      "60 0.286255\n",
      "61 0.286021\n",
      "62 0.285794\n",
      "63 0.285574\n",
      "64 0.28536\n",
      "65 0.285153\n",
      "66 0.284952\n",
      "67 0.284757\n",
      "68 0.284568\n",
      "69 0.284384\n",
      "70 0.284205\n",
      "71 0.284032\n",
      "72 0.283863\n",
      "73 0.283699\n",
      "74 0.28354\n",
      "75 0.283384\n",
      "76 0.283234\n",
      "77 0.283087\n",
      "78 0.282944\n",
      "79 0.282805\n",
      "80 0.282669\n",
      "81 0.282538\n",
      "82 0.282409\n",
      "83 0.282284\n",
      "84 0.282162\n",
      "85 0.282044\n",
      "86 0.281928\n",
      "87 0.281815\n",
      "88 0.281705\n",
      "89 0.281598\n",
      "90 0.281493\n",
      "91 0.281391\n",
      "92 0.281292\n",
      "93 0.281195\n",
      "94 0.2811\n",
      "95 0.281008\n",
      "96 0.280918\n",
      "97 0.28083\n",
      "98 0.280744\n",
      "99 0.280661\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"X:0\", shape=(?, 784), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    941\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[1;32m--> 942\u001b[1;33m                                                     allow_operation=False)\n\u001b[0m\u001b[0;32m    943\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2584\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2662\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2663\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2664\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"X:0\", shape=(?, 784), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-287-8ee062b802ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtffw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_OUT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_OUT_PROB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3926\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3927\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3928\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    943\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[1;32m--> 945\u001b[1;33m                             + e.args[0])\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"X:0\", shape=(?, 784), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "epoch_size=10\n",
    "num_epochs=100\n",
    "train_batch = {tf_X: train1d_X, tf_y: np.asarray(train1d_y).transpose(), tf_lr:1.0}\n",
    "valid_batch = {tf_X: valid1d_X, tf_y: np.asarray(valid1d_y).transpose()}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf_init)\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(epoch_size):\n",
    "            sess.run(tf_train, feed_dict=train_batch)\n",
    "        sum_str = tf_err_summary.eval(feed_dict=valid_batch)\n",
    "        valid_loss = tf_loss.eval(feed_dict=valid_batch)\n",
    "        tffw.add_summary(sum_str, i)\n",
    "        print(i, valid_loss)\n",
    "    z = tf_OUT.eval(feed_dict=batch)\n",
    "    prob = tf_OUT_PROB.eval(feed_dict=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.68059257e-02,   6.15276862e-04,   1.85183878e-03, ...,\n",
       "          3.08779487e-03,   1.38314292e-01,   5.08469017e-03],\n",
       "       [  5.55270761e-02,   3.01036541e-03,   3.30095482e-03, ...,\n",
       "          2.74824561e-03,   2.61425972e-01,   1.15526728e-02],\n",
       "       [  2.83087866e-04,   8.45496178e-01,   1.47710973e-02, ...,\n",
       "          1.29105533e-02,   7.19248131e-02,   1.08698476e-03],\n",
       "       ..., \n",
       "       [  4.51952452e-03,   6.13750935e-01,   2.74646860e-02, ...,\n",
       "          1.77786071e-02,   1.64972827e-01,   1.09976474e-02],\n",
       "       [  1.14167552e-03,   2.31584981e-02,   3.56666297e-02, ...,\n",
       "          5.86982677e-03,   2.36413196e-01,   1.01279598e-02],\n",
       "       [  1.66436777e-01,   3.52604105e-03,   5.59435971e-03, ...,\n",
       "          1.24523835e-02,   1.01005249e-01,   2.72773160e-03]], dtype=float32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72562499999999996"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_eval_prob(train_y, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38400, 10)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
