{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, os.path\n",
    "\n",
    "data_dir = '../DataSets/FaceNet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015-2016 Carnegie Mellon University\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Module for dlib-based alignment.\"\"\"\n",
    "\n",
    "# NOTE: This file has been copied from the openface project.\n",
    "#  https://github.com/cmusatyalab/openface/blob/master/openface/align_dlib.py\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "INV_TEMPLATE = np.float32([\n",
    "    (-0.04099179660567834, -0.008425234314031194, 2.575498465013183),\n",
    "    (0.04062510634554352, -0.009678089746831375, -1.2534351452524177),\n",
    "    (0.0003666902601348179, 0.01810332406086298, -0.32206331976076663)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "\n",
    "class AlignDlib:\n",
    "    \"\"\"\n",
    "    Use `dlib's landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n",
    "\n",
    "    The alignment preprocess faces for input into a neural network.\n",
    "    Faces are resized to the same size (such as 96x96) and transformed\n",
    "    to make landmarks (such as the eyes and nose) appear at the same\n",
    "    location on every image.\n",
    "\n",
    "    Normalized landmarks:\n",
    "\n",
    "    .. image:: ../images/dlib-landmark-mean.png\n",
    "    \"\"\"\n",
    "\n",
    "    #: Landmark indices corresponding to the inner eyes and bottom lip.\n",
    "    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
    "\n",
    "    #: Landmark indices corresponding to the outer eyes and nose.\n",
    "    OUTER_EYES_AND_NOSE = [36, 45, 33]\n",
    "\n",
    "    def __init__(self, facePredictor):\n",
    "        \"\"\"\n",
    "        Instantiate an 'AlignDlib' object.\n",
    "\n",
    "        :param facePredictor: The path to dlib's\n",
    "        :type facePredictor: str\n",
    "        \"\"\"\n",
    "        assert facePredictor is not None\n",
    "\n",
    "        # pylint: disable=no-member\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(facePredictor)\n",
    "\n",
    "    def getAllFaceBoundingBoxes(self, rgbImg):\n",
    "        \"\"\"\n",
    "        Find all face bounding boxes in an image.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :return: All face bounding boxes in an image.\n",
    "        :rtype: dlib.rectangles\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        try:\n",
    "            return self.detector(rgbImg, 1)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            print(\"Warning: {}\".format(e))\n",
    "            # In rare cases, exceptions are thrown.\n",
    "            return []\n",
    "\n",
    "    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n",
    "        \"\"\"\n",
    "        Find the largest face bounding box in an image.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :return: The largest face bounding box in an image, or None.\n",
    "        :rtype: dlib.rectangle\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        faces = self.getAllFaceBoundingBoxes(rgbImg)\n",
    "        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n",
    "            return max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def findLandmarks(self, rgbImg, bb):\n",
    "        \"\"\"\n",
    "        Find the landmarks of a face.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to find landmarks for.\n",
    "        :type bb: dlib.rectangle\n",
    "        :return: Detected landmark locations.\n",
    "        :rtype: list of (x,y) tuples\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "        assert bb is not None\n",
    "\n",
    "        points = self.predictor(rgbImg, bb)\n",
    "        # return list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        return [(p.x, p.y) for p in points.parts()]\n",
    "\n",
    "    # pylint: disable=dangerous-default-value\n",
    "    def align(self, imgDim, rgbImg, bb=None,\n",
    "              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n",
    "              skipMulti=False, scale=1.0):\n",
    "        r\"\"\"align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n",
    "\n",
    "        Transform and align a face in an image.\n",
    "\n",
    "        :param imgDim: The edge length in pixels of the square the image is resized to.\n",
    "        :type imgDim: int\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to align. \\\n",
    "                   Defaults to the largest face.\n",
    "        :type bb: dlib.rectangle\n",
    "        :param landmarks: Detected landmark locations. \\\n",
    "                          Landmarks found on `bb` if not provided.\n",
    "        :type landmarks: list of (x,y) tuples\n",
    "        :param landmarkIndices: The indices to transform to.\n",
    "        :type landmarkIndices: list of ints\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :param scale: Scale image before cropping to the size given by imgDim.\n",
    "        :type scale: float\n",
    "        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n",
    "        :rtype: numpy.ndarray\n",
    "        \"\"\"\n",
    "        assert imgDim is not None\n",
    "        assert rgbImg is not None\n",
    "        assert landmarkIndices is not None\n",
    "\n",
    "        if bb is None:\n",
    "            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n",
    "            if bb is None:\n",
    "                return\n",
    "\n",
    "        if landmarks is None:\n",
    "            landmarks = self.findLandmarks(rgbImg, bb)\n",
    "\n",
    "        npLandmarks = np.float32(landmarks)\n",
    "        npLandmarkIndices = np.array(landmarkIndices)\n",
    "\n",
    "        # pylint: disable=maybe-no-member\n",
    "        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n",
    "                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices] * scale + imgDim * (1 - scale) / 2)\n",
    "        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n",
    "\n",
    "        return thumbnail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(input_path, output_path, crop_dim):\n",
    "    \"\"\"\n",
    "    Detect face, align and crop :param input_path. Write output to :param output_path\n",
    "    :param input_path: Path to input image\n",
    "    :param output_path: Path to write processed image\n",
    "    :param crop_dim: dimensions to crop image to\n",
    "    \"\"\"\n",
    "    image = _process_image(input_path, crop_dim)\n",
    "    if image is not None:\n",
    "        cv2.imwrite(output_path, image)\n",
    "        return output_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _process_image(filename, crop_dim):\n",
    "    image = None\n",
    "    aligned_image = None\n",
    "\n",
    "    image = _buffer_image(filename)\n",
    "\n",
    "    if image is not None:\n",
    "        aligned_image = _align_image(image, crop_dim)\n",
    "    else:\n",
    "        raise IOError('Error buffering image: {}'.format(filename))\n",
    "\n",
    "    return aligned_image\n",
    "\n",
    "\n",
    "def _buffer_image(filename):\n",
    "    image = cv2.imread(filename, )\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _align_image(image, crop_dim):\n",
    "    bb = align_dlib.getLargestFaceBoundingBox(image)\n",
    "    print(bb)\n",
    "    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n",
    "    if aligned is not None:\n",
    "        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
    "    return aligned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "align_dlib = AlignDlib(data_dir + 'shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "img_path = data_dir + 'images/'\n",
    "proc_path = data_dir + 'proc_images/'\n",
    "\n",
    "img_name = '20171222_163636.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(869, 2147) (2019, 3297)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../DataSets/FaceNet/proc_images/20171222_163636.jpg'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_image(img_path + img_name, proc_path + img_name, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in os.listdir(img_path) if os.path.isfile(img_path+f) and f.split('.')[-1].lower() in ('jpg', 'jpeg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(429, 2117) (1809, 3497)]\n",
      "[(741, 2019) (1891, 3169)]\n",
      "[(889, 1810) (2270, 3190)]\n",
      "[(741, 1891) (1891, 3042)]\n",
      "[(736, 1810) (2116, 3190)]\n",
      "[(869, 2147) (2019, 3297)]\n",
      "[(429, 2883) (1809, 4264)]\n",
      "[(996, 3042) (2147, 4192)]\n",
      "[(617, 2321) (1575, 3280)]\n",
      "[(357, 1508) (1508, 2658)]\n",
      "Processed 10/10 images..\r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for fn in images:\n",
    "    i += 1\n",
    "    preprocess_image(img_path + fn, proc_path + fn, 160)\n",
    "    print('Processed {}/{} images..'.format(i, len(images)), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
